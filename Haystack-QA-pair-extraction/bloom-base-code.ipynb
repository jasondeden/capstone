{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b47ea5-eb49-474c-aa84-3daa26aeba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers@main\n",
      "  Cloning https://github.com/huggingface/transformers (to revision main) to /tmp/pip-req-build-dm1tx85g\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-dm1tx85g\n",
      "  Resolved https://github.com/huggingface/transformers to commit 74b3eb3dea9c4a321e95d7f13d0d852d9e1e10a1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (1.21.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (4.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (0.10.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.24.0.dev0) (2022.9.13)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.24.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.24.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.24.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.24.0.dev0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.24.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.24.0.dev0) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffac34a-7b48-479a-b819-7a62e576772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9cfe73d-1493-43e8-8d47-838de87ccba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986483b8-c7c1-4645-8f43-e7ee549409d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir QA_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13005ba3-e1e0-4ea6-9379-7a29e3dc72c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/QA_tune\n"
     ]
    }
   ],
   "source": [
    "cd QA_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2d0090-17e9-404a-98c0-77e9bc1a558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 112247, done.\u001b[K\n",
      "remote: Total 112247 (delta 0), reused 0 (delta 0), pack-reused 112247\u001b[K\n",
      "Receiving objects: 100% (112247/112247), 105.67 MiB | 32.73 MiB/s, done.\n",
      "Resolving deltas: 100% (83385/83385), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7c28b9-42ef-4eb0-bf82-fba682f5a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering\n"
     ]
    }
   ],
   "source": [
    "cd transformers/examples/pytorch/question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119ab00d-27e0-43b4-9efd-d154fd1e0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.9/441.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "Successfully installed datasets-2.6.1 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 xxhash-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226be59f-6652-41a7-9c17-c404ed2d8bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.11.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.8.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c31d46e-8cdb-419d-a621-ed3af6dcd77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md               run_qa_beam_search_no_trainer.py  trainer_seq2seq_qa.py\n",
      "requirements.txt        \u001b[0m\u001b[01;32mrun_qa_no_trainer.py\u001b[0m*             utils_qa.py\n",
      "\u001b[01;32mrun_qa.py\u001b[0m*              run_seq2seq_qa.py\n",
      "\u001b[01;32mrun_qa_beam_search.py\u001b[0m*  trainer_qa.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4441a701-a50c-4204-906c-8f86b7bc78d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/22/2022 17:14:46 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "10/22/2022 17:14:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/tmp/debug_bloom_squad/runs/Oct22_17-14-45_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/tmp/debug_bloom_squad/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tmp/debug_bloom_squad/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/squad_v2.py not found in cache or force_download set to True, downloading to /home/jupyter/.cache/huggingface/datasets/downloads/tmpvb2sjprk\n",
      "Downloading builder script: 100%|██████████| 5.28k/5.28k [00:00<00:00, 5.11MB/s]\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/squad_v2.py in cache at /home/jupyter/.cache/huggingface/datasets/downloads/e6f4860cc1c35cc48453e3ef385c5381c8a2453a71f4c8f115027046973b7adf.fc0184c3b7bb06ab8ab00fd19da286638a944c4ce8fbd5d62de27c39469e05a4.py\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - creating metadata file for /home/jupyter/.cache/huggingface/datasets/downloads/e6f4860cc1c35cc48453e3ef385c5381c8a2453a71f4c8f115027046973b7adf.fc0184c3b7bb06ab8ab00fd19da286638a944c4ce8fbd5d62de27c39469e05a4.py\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/dataset_infos.json not found in cache or force_download set to True, downloading to /home/jupyter/.cache/huggingface/datasets/downloads/tmpqjkotg9i\n",
      "Downloading metadata: 100%|████████████████| 2.40k/2.40k [00:00<00:00, 2.85MB/s]\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/dataset_infos.json in cache at /home/jupyter/.cache/huggingface/datasets/downloads/090eb8ef450f094e58a2fadd20eca7e0307da70f996d0090fdfb4d50b844da0c.311a7a0cc567f92fd461689d7aabb8bbd8c594e327ae5ec31c7166354ce7b709\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - creating metadata file for /home/jupyter/.cache/huggingface/datasets/downloads/090eb8ef450f094e58a2fadd20eca7e0307da70f996d0090fdfb4d50b844da0c.311a7a0cc567f92fd461689d7aabb8bbd8c594e327ae5ec31c7166354ce7b709\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/squad_v2/resolve/main/README.md not found in cache or force_download set to True, downloading to /home/jupyter/.cache/huggingface/datasets/downloads/tmpmiiuy_8x\n",
      "Downloading readme: 100%|██████████████████| 7.52k/7.52k [00:00<00:00, 6.88MB/s]\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/squad_v2/resolve/main/README.md in cache at /home/jupyter/.cache/huggingface/datasets/downloads/161a8eec71065c834b63dcd80453ac01a13f2fde78a03e36700f898eb4102917.e3fcb47228f7d9d61d03ae98db6c92b35ec5955b681edf1f8078cf3de9eb3366\n",
      "10/22/2022 17:14:47 - INFO - datasets.utils.file_utils - creating metadata file for /home/jupyter/.cache/huggingface/datasets/downloads/161a8eec71065c834b63dcd80453ac01a13f2fde78a03e36700f898eb4102917.e3fcb47228f7d9d61d03ae98db6c92b35ec5955b681edf1f8078cf3de9eb3366\n",
      "10/22/2022 17:14:47 - INFO - datasets.builder - No config specified, defaulting to the single config: squad_v2/squad_v2\n",
      "10/22/2022 17:14:47 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n",
      "10/22/2022 17:14:47 - INFO - datasets.builder - Generating dataset squad_v2 (/home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n",
      "Downloading and preparing dataset squad_v2/squad_v2 (download: 44.34 MiB, generated: 122.41 MiB, post-processed: Unknown size, total: 166.75 MiB) to /home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n",
      "10/22/2022 17:14:47 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]10/22/2022 17:14:48 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json not found in cache or force_download set to True, downloading to /home/jupyter/.cache/huggingface/datasets/downloads/tmp5j_cwnt2\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/9.55M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  88%|█████████████████▌  | 8.37M/9.55M [00:00<00:00, 83.7MB/s]\u001b[A\n",
      "Downloading data: 18.9MB [00:00, 96.4MB/s]                                      \u001b[A\n",
      "Downloading data: 29.8MB [00:00, 102MB/s] \u001b[A\n",
      "Downloading data: 42.1MB [00:00, 101MB/s]\u001b[A\n",
      "10/22/2022 17:14:49 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json in cache at /home/jupyter/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n",
      "10/22/2022 17:14:49 - INFO - datasets.utils.file_utils - creating metadata file for /home/jupyter/.cache/huggingface/datasets/downloads/3ed6d8ee2e4a05a76f0e3c9aeac435b2a46efb6ade713d80cc81dfdc94a83183\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:01<00:01,  1.41s/it]10/22/2022 17:14:49 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json not found in cache or force_download set to True, downloading to /home/jupyter/.cache/huggingface/datasets/downloads/tmppnzn34gg\n",
      "\n",
      "Downloading data: 4.37MB [00:00, 75.4MB/s]                                      \u001b[A\n",
      "10/22/2022 17:14:49 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json in cache at /home/jupyter/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n",
      "10/22/2022 17:14:49 - INFO - datasets.utils.file_utils - creating metadata file for /home/jupyter/.cache/huggingface/datasets/downloads/5ac41b6f6bec6809c84d3591916085ec80858a34b7827db3ac4f46dca6e7bf32\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:01<00:00,  1.20it/s]\n",
      "10/22/2022 17:14:49 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "10/22/2022 17:14:49 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1994.44it/s]\n",
      "10/22/2022 17:14:49 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "10/22/2022 17:14:49 - INFO - datasets.builder - Generating train split\n",
      "10/22/2022 17:14:59 - INFO - datasets.builder - Generating validation split     \n",
      "10/22/2022 17:15:00 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad_v2 downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 253.04it/s]\n",
      "Downloading: 100%|██████████████████████████████| 688/688 [00:00<00:00, 623kB/s]\n",
      "[INFO|configuration_utils.py:653] 2022-10-22 17:15:00,771 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-22 17:15:00,775 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "Downloading: 100%|██████████████████████████████| 222/222 [00:00<00:00, 215kB/s]\n",
      "Downloading: 100%|█████████████████████████| 14.5M/14.5M [00:00<00:00, 54.4MB/s]\n",
      "Downloading: 100%|███████████████████████████| 85.0/85.0 [00:00<00:00, 81.4kB/s]\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 17:15:02,523 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 17:15:02,523 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 17:15:02,524 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 17:15:02,524 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "Downloading: 100%|█████████████████████████| 1.12G/1.12G [00:17<00:00, 63.4MB/s]\n",
      "[INFO|modeling_utils.py:2156] 2022-10-22 17:15:21,414 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-10-22 17:15:26,029 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-10-22 17:15:26,030 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|               | 0/131 [00:00<?, ?ba/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 681, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 463, in main\n",
      "    desc=\"Running tokenizer on train dataset\",\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2590, in map\n",
      "    desc=desc,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 584, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 551, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/fingerprint.py\", line 480, in wrapper\n",
      "    out = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2972, in _map_single\n",
      "    offset=offset,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2852, in apply_function_on_filtered_inputs\n",
      "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2532, in decorated\n",
      "    result = f(decorated_item, *args, **kwargs)\n",
      "  File \"run_qa.py\", line 404, in prepare_train_features\n",
      "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
      "ValueError: None is not in list\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name squad_v2 \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /tmp/debug_bloom_squad/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de27e48-ac6a-495c-af7f-0b5f362a7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f6d82a3-ef96-47fd-bc11-c96b889c899d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9549b4b-dbda-4c5c-bb79-96488d347053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/22/2022 18:17:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "10/22/2022 18:17:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/tmp/debug_bloom_squad/runs/Oct22_18-17-31_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/tmp/debug_bloom_squad/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/tmp/debug_bloom_squad/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "10/22/2022 18:17:32 - INFO - datasets.builder - No config specified, defaulting to the single config: squad_v2/squad_v2\n",
      "10/22/2022 18:17:32 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/squad_v2/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n",
      "10/22/2022 18:17:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "10/22/2022 18:17:32 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n",
      "10/22/2022 18:17:32 - WARNING - datasets.builder - Found cached dataset squad_v2 (/home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d)\n",
      "10/22/2022 18:17:32 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 474.28it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-10-22 18:17:33,083 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-22 18:17:33,115 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 18:17:33,260 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 18:17:33,261 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 18:17:33,261 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-10-22 18:17:33,261 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-10-22 18:17:33,979 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-10-22 18:17:38,507 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-10-22 18:17:38,507 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "10/22/2022 18:17:38 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d/cache-d8328915a14a3a89.arrow\n",
      "[INFO|trainer.py:557] 2022-10-22 18:17:42,089 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-10-22 18:17:42,117 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-22 18:17:42,117 >>   Num examples = 131854\n",
      "[INFO|trainer.py:1609] 2022-10-22 18:17:42,117 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1610] 2022-10-22 18:17:42,118 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-10-22 18:17:42,118 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-10-22 18:17:42,118 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-22 18:17:42,118 >>   Total optimization steps = 43952\n",
      "  0%|                                                 | 0/43952 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 8.2605, 'learning_rate': 2.9665544230069168e-05, 'epoch': 0.02}        \n",
      "  1%|▍                                    | 500/43952 [02:16<3:08:44,  3.84it/s][INFO|trainer.py:2671] 2022-10-22 18:19:58,864 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:19:58,865 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:20:03,521 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:20:03,522 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:20:03,522 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.2073, 'learning_rate': 2.9324262832180562e-05, 'epoch': 0.05}        \n",
      "  2%|▊                                   | 1000/43952 [05:05<3:07:50,  3.81it/s][INFO|trainer.py:2671] 2022-10-22 18:22:47,566 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:22:47,567 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:22:52,345 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:22:52,345 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:22:52,346 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.0271, 'learning_rate': 2.8982981434291954e-05, 'epoch': 0.07}        \n",
      "  3%|█▏                                  | 1500/43952 [08:02<3:09:47,  3.73it/s][INFO|trainer.py:2671] 2022-10-22 18:25:44,491 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:25:44,492 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:25:49,957 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:25:49,958 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:25:49,958 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.8021, 'learning_rate': 2.864170003640335e-05, 'epoch': 0.09}         \n",
      "  5%|█▋                                  | 2000/43952 [11:01<3:11:51,  3.64it/s][INFO|trainer.py:2671] 2022-10-22 18:28:43,546 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:28:43,547 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:28:48,856 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:28:48,856 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:28:48,857 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.7889, 'learning_rate': 2.8300418638514747e-05, 'epoch': 0.11}        \n",
      "  6%|██                                  | 2500/43952 [13:57<3:10:58,  3.62it/s][INFO|trainer.py:2671] 2022-10-22 18:31:39,418 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:31:39,419 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:31:44,784 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:31:44,784 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:31:44,785 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.9639, 'learning_rate': 2.795913724062614e-05, 'epoch': 0.14}         \n",
      "  7%|██▍                                 | 3000/43952 [16:56<3:08:28,  3.62it/s][INFO|trainer.py:2671] 2022-10-22 18:34:38,344 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:34:38,345 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:34:44,021 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:34:44,223 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:34:44,224 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.7584, 'learning_rate': 2.7617855842737534e-05, 'epoch': 0.16}        \n",
      "  8%|██▊                                 | 3500/43952 [19:48<2:55:55,  3.83it/s][INFO|trainer.py:2671] 2022-10-22 18:37:30,947 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:37:30,948 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:37:36,219 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:37:36,220 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:37:36,221 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.9555, 'learning_rate': 2.7276574444848925e-05, 'epoch': 0.18}        \n",
      "  9%|███▎                                | 4000/43952 [22:44<2:57:53,  3.74it/s][INFO|trainer.py:2671] 2022-10-22 18:40:26,359 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 18:40:26,360 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 18:40:31,957 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 18:40:31,958 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 18:40:31,958 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.8476, 'learning_rate': 2.3524526756461594e-05, 'epoch': 0.43}        \n",
      " 22%|███████▊                            | 9500/43952 [55:17<2:33:04,  3.75it/s][INFO|trainer.py:2671] 2022-10-22 19:12:59,369 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-9500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:12:59,370 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:13:05,091 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:13:05,091 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:13:05,092 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-9500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.8787, 'learning_rate': 2.147752093192574e-05, 'epoch': 0.57}         \n",
      " 28%|█████████▍                       | 12500/43952 [1:12:57<2:21:50,  3.70it/s][INFO|trainer.py:2671] 2022-10-22 19:30:40,051 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-12500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:30:40,052 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:30:45,558 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:30:45,559 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:30:45,559 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-12500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.9497, 'learning_rate': 2.113692209683291e-05, 'epoch': 0.59}         \n",
      " 30%|█████████▊                       | 13000/43952 [1:15:54<2:15:19,  3.81it/s][INFO|trainer.py:2671] 2022-10-22 19:33:36,519 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-13000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:33:36,520 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:33:42,173 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:33:42,174 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:33:42,174 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.8454, 'learning_rate': 2.0795640698944304e-05, 'epoch': 0.61}        \n",
      " 31%|██████████▏                      | 13500/43952 [1:18:50<2:18:04,  3.68it/s][INFO|trainer.py:2671] 2022-10-22 19:36:32,569 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-13500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:36:32,570 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:36:38,193 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:36:38,193 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:36:38,194 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-13500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.824, 'learning_rate': 2.0454359301055696e-05, 'epoch': 0.64}         \n",
      " 32%|██████████▌                      | 14000/43952 [1:21:47<2:17:21,  3.63it/s][INFO|trainer.py:2671] 2022-10-22 19:39:29,141 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-14000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:39:29,142 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:39:34,662 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:39:34,663 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:39:34,663 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.9265, 'learning_rate': 2.011307790316709e-05, 'epoch': 0.66}         \n",
      " 33%|██████████▉                      | 14500/43952 [1:24:43<2:09:23,  3.79it/s][INFO|trainer.py:2671] 2022-10-22 19:42:25,557 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-14500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:42:25,558 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:42:31,187 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:42:31,188 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:42:31,188 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-14500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.9985, 'learning_rate': 1.977179650527849e-05, 'epoch': 0.68}         \n",
      " 34%|███████████▎                     | 15000/43952 [1:27:40<2:07:39,  3.78it/s][INFO|trainer.py:2671] 2022-10-22 19:45:22,260 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-15000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:45:22,261 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:45:27,782 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:45:27,783 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:45:27,783 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.7808, 'learning_rate': 1.943051510738988e-05, 'epoch': 0.71}         \n",
      " 35%|███████████▋                     | 15500/43952 [1:30:36<2:06:42,  3.74it/s][INFO|trainer.py:2671] 2022-10-22 19:48:18,895 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-15500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:48:18,896 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:48:24,371 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:48:24,372 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:48:24,372 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-15500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.0853, 'learning_rate': 1.9089233709501275e-05, 'epoch': 0.73}        \n",
      " 36%|████████████                     | 16000/43952 [1:33:33<2:06:21,  3.69it/s][INFO|trainer.py:2671] 2022-10-22 19:51:15,449 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-16000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 19:51:15,449 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 19:51:20,992 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 19:51:20,993 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 19:51:20,993 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-16000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4298, 'learning_rate': 1.4995904623225337e-05, 'epoch': 1.0}         \n",
      " 50%|████████████████▌                | 22000/43952 [2:08:38<1:35:11,  3.84it/s][INFO|trainer.py:2671] 2022-10-22 20:26:20,625 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-22000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 20:26:20,627 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:26:26,122 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:26:26,123 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:26:26,123 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5444, 'learning_rate': 1.465462322533673e-05, 'epoch': 1.02}         \n",
      " 51%|████████████████▉                | 22500/43952 [2:11:32<1:38:07,  3.64it/s][INFO|trainer.py:2671] 2022-10-22 20:29:15,028 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-22500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 20:29:15,029 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:29:20,453 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:29:20,454 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:29:20,454 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-22500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1415, 'learning_rate': 1.4313341827448125e-05, 'epoch': 1.05}        \n",
      " 52%|█████████████████▎               | 23000/43952 [2:14:26<1:35:35,  3.65it/s][INFO|trainer.py:2671] 2022-10-22 20:32:08,144 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-23000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 20:32:08,145 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:32:13,642 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:32:13,643 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:32:13,643 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1462, 'learning_rate': 1.397206042955952e-05, 'epoch': 1.07}         \n",
      " 53%|█████████████████▋               | 23500/43952 [2:17:20<1:31:16,  3.73it/s][INFO|trainer.py:2671] 2022-10-22 20:35:02,825 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-23500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 20:35:02,826 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:35:08,445 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:35:08,446 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:35:08,446 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-23500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0745, 'learning_rate': 1.3630779031670913e-05, 'epoch': 1.09}        \n",
      " 55%|██████████████████               | 24000/43952 [2:20:14<1:31:45,  3.62it/s][INFO|trainer.py:2671] 2022-10-22 20:37:56,509 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-24000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 20:37:56,509 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:38:01,985 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:38:01,986 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:38:01,986 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-24000/special_tokens_map.json\n",
      "{'loss': 3.1032, 'learning_rate': 1.1584455769930834e-05, 'epoch': 1.23}        \n",
      " 61%|████████████████████▎            | 27000/43952 [2:37:36<1:17:29,  3.65it/s][INFO|trainer.py:2671] 2022-10-22 20:55:18,706 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-27000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 20:55:18,707 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:55:24,317 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:55:24,318 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:55:24,318 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27000/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 20:58:17,593 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 20:58:17,594 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 20:58:17,594 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-27500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.033, 'learning_rate': 1.0901892974153622e-05, 'epoch': 1.27}         \n",
      " 64%|█████████████████████            | 28000/43952 [2:43:23<1:11:57,  3.69it/s][INFO|trainer.py:2671] 2022-10-22 21:01:05,477 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-28000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:01:05,478 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:01:10,993 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:01:10,994 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:01:10,994 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0212, 'learning_rate': 1.0560611576265017e-05, 'epoch': 1.3}         \n",
      " 65%|█████████████████████▍           | 28500/43952 [2:46:17<1:07:28,  3.82it/s][INFO|trainer.py:2671] 2022-10-22 21:03:59,425 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-28500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:03:59,425 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:04:05,076 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:04:05,077 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:04:05,078 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-28500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.9057, 'learning_rate': 1.0220012741172187e-05, 'epoch': 1.32}        \n",
      " 66%|█████████████████████▊           | 29000/43952 [2:49:11<1:06:03,  3.77it/s][INFO|trainer.py:2671] 2022-10-22 21:06:53,824 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-29000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:06:53,825 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:06:59,385 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:06:59,385 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:06:59,385 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1331, 'learning_rate': 9.878731343283582e-06, 'epoch': 1.34}         \n",
      " 67%|██████████████████████▏          | 29500/43952 [2:52:04<1:03:59,  3.76it/s][INFO|trainer.py:2671] 2022-10-22 21:09:46,819 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-29500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:09:46,820 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:09:52,354 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:09:52,355 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:09:52,355 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-29500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.9195, 'learning_rate': 7.491126683654897e-06, 'epoch': 1.5}          \n",
      " 75%|██████████████████████████▎        | 33000/43952 [3:12:16<47:43,  3.82it/s][INFO|trainer.py:2671] 2022-10-22 21:29:59,017 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-33000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:29:59,018 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:30:04,605 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:30:04,606 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:30:04,606 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.3734, 'learning_rate': 7.149845285766291e-06, 'epoch': 1.52}         \n",
      " 76%|██████████████████████████▋        | 33500/43952 [3:15:09<46:21,  3.76it/s][INFO|trainer.py:2671] 2022-10-22 21:32:51,798 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-33500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:32:51,799 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:32:57,325 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:32:57,326 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:32:57,326 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-33500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.2046, 'learning_rate': 6.808563887877685e-06, 'epoch': 1.55}         \n",
      " 77%|███████████████████████████        | 34000/43952 [3:18:03<45:18,  3.66it/s][INFO|trainer.py:2671] 2022-10-22 21:35:45,140 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-34000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:35:45,141 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:35:50,673 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:35:50,673 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:35:50,674 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1923, 'learning_rate': 6.467282489989079e-06, 'epoch': 1.57}         \n",
      " 78%|███████████████████████████▍       | 34500/43952 [3:20:56<41:59,  3.75it/s][INFO|trainer.py:2671] 2022-10-22 21:38:38,187 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-34500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:38:38,189 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:38:43,708 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:38:43,709 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:38:43,709 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-34500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0387, 'learning_rate': 6.126683654896251e-06, 'epoch': 1.59}         \n",
      " 80%|███████████████████████████▊       | 35000/43952 [3:23:49<41:39,  3.58it/s][INFO|trainer.py:2671] 2022-10-22 21:41:31,233 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-35000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:41:31,234 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:41:36,754 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:41:36,755 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:41:36,755 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0879, 'learning_rate': 5.785402257007645e-06, 'epoch': 1.62}         \n",
      " 81%|████████████████████████████▎      | 35500/43952 [3:26:41<39:19,  3.58it/s][INFO|trainer.py:2671] 2022-10-22 21:44:23,722 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-35500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:44:23,724 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:44:29,246 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:44:29,247 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:44:29,247 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-35500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0943, 'learning_rate': 5.444120859119039e-06, 'epoch': 1.64}         \n",
      " 82%|████████████████████████████▋      | 36000/43952 [3:29:34<34:34,  3.83it/s][INFO|trainer.py:2671] 2022-10-22 21:47:16,512 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-36000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:47:16,514 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-36000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:47:22,030 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-36000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:47:22,031 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-36000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:47:22,031 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-36000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0139, 'learning_rate': 4.420959228248999e-06, 'epoch': 1.71}         \n",
      " 85%|█████████████████████████████▊     | 37500/43952 [3:38:11<28:31,  3.77it/s][INFO|trainer.py:2671] 2022-10-22 21:55:54,074 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-37500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:55:54,076 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-37500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:55:59,662 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-37500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:55:59,663 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-37500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:55:59,663 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-37500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1161, 'learning_rate': 4.079677830360393e-06, 'epoch': 1.73}         \n",
      " 86%|██████████████████████████████▎    | 38000/43952 [3:41:04<26:15,  3.78it/s][INFO|trainer.py:2671] 2022-10-22 21:58:46,297 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-38000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 21:58:46,298 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 21:58:51,825 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 21:58:51,826 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 21:58:51,827 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.2882, 'learning_rate': 3.7383964324717874e-06, 'epoch': 1.75}        \n",
      " 88%|██████████████████████████████▋    | 38500/43952 [3:43:57<26:20,  3.45it/s][INFO|trainer.py:2671] 2022-10-22 22:01:39,865 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-38500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:01:39,867 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:01:45,408 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:01:45,409 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:01:45,409 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-38500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0338, 'learning_rate': 3.3977975973789587e-06, 'epoch': 1.77}        \n",
      " 89%|███████████████████████████████    | 39000/43952 [3:46:50<21:46,  3.79it/s][INFO|trainer.py:2671] 2022-10-22 22:04:32,579 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-39000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:04:32,581 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:04:38,079 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:04:38,079 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:04:38,080 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.876, 'learning_rate': 3.056516199490353e-06, 'epoch': 1.8}           \n",
      " 90%|███████████████████████████████▍   | 39500/43952 [3:49:43<19:41,  3.77it/s][INFO|trainer.py:2671] 2022-10-22 22:07:25,709 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-39500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:07:25,711 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:07:31,330 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:07:31,331 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:07:31,331 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-39500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0888, 'learning_rate': 2.7152348016017477e-06, 'epoch': 1.82}        \n",
      " 91%|███████████████████████████████▊   | 40000/43952 [3:52:36<17:33,  3.75it/s][INFO|trainer.py:2671] 2022-10-22 22:10:18,593 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-40000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:10:18,594 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:10:24,061 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:10:24,062 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:10:24,062 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1206, 'learning_rate': 2.3739534037131413e-06, 'epoch': 1.84}        \n",
      " 92%|████████████████████████████████▎  | 40500/43952 [3:55:29<16:04,  3.58it/s][INFO|trainer.py:2671] 2022-10-22 22:13:11,515 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-40500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:13:11,517 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:13:17,033 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:13:17,034 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:13:17,034 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-40500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0954, 'learning_rate': 2.0326720058245358e-06, 'epoch': 1.87}        \n",
      " 93%|████████████████████████████████▋  | 41000/43952 [3:58:22<13:01,  3.78it/s][INFO|trainer.py:2671] 2022-10-22 22:16:04,486 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-41000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:16:04,488 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:16:09,962 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:16:09,963 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:16:09,964 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4009, 'learning_rate': 1.6920731707317074e-06, 'epoch': 1.89}        \n",
      " 94%|█████████████████████████████████  | 41500/43952 [4:01:14<10:57,  3.73it/s][INFO|trainer.py:2671] 2022-10-22 22:18:57,075 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-41500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:18:57,076 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:19:02,617 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:19:02,617 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:19:02,618 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-41500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1574, 'learning_rate': 1.3507917728431015e-06, 'epoch': 1.91}        \n",
      " 96%|█████████████████████████████████▍ | 42000/43952 [4:04:06<08:38,  3.76it/s][INFO|trainer.py:2671] 2022-10-22 22:21:48,499 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-42000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:21:48,500 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:21:53,751 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:21:53,752 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:21:53,752 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8075, 'learning_rate': 1.009510374954496e-06, 'epoch': 1.93}         \n",
      " 97%|█████████████████████████████████▊ | 42500/43952 [4:06:58<06:27,  3.74it/s][INFO|trainer.py:2671] 2022-10-22 22:24:40,935 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-42500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:24:40,936 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:24:46,429 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:24:46,430 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:24:46,430 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-42500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1462, 'learning_rate': 6.682289770658902e-07, 'epoch': 1.96}         \n",
      " 98%|██████████████████████████████████▏| 43000/43952 [4:09:50<04:12,  3.77it/s][INFO|trainer.py:2671] 2022-10-22 22:27:33,099 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-43000\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:27:33,101 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:27:38,598 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:27:38,598 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:27:38,599 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0345, 'learning_rate': 3.2763014197306156e-07, 'epoch': 1.98}        \n",
      " 99%|██████████████████████████████████▋| 43500/43952 [4:12:43<01:59,  3.79it/s][INFO|trainer.py:2671] 2022-10-22 22:30:25,786 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/checkpoint-43500\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:30:25,787 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:30:31,329 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:30:31,330 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:30:31,330 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/checkpoint-43500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████| 43952/43952 [4:15:23<00:00,  3.98it/s][INFO|trainer.py:1852] 2022-10-22 22:33:05,658 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 15323.5402, 'train_samples_per_second': 17.209, 'train_steps_per_second': 2.868, 'train_loss': 3.5304283500368654, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████| 43952/43952 [4:15:23<00:00,  2.87it/s]\n",
      "[INFO|trainer.py:2671] 2022-10-22 22:33:05,661 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/\n",
      "[INFO|configuration_utils.py:447] 2022-10-22 22:33:05,662 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-22 22:33:11,183 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:33:11,183 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:33:11,184 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     3.5304\n",
      "  train_runtime            = 4:15:23.54\n",
      "  train_samples            =     131854\n",
      "  train_samples_per_second =     17.209\n",
      "  train_steps_per_second   =      2.868\n",
      "[INFO|modelcard.py:444] 2022-10-22 22:33:13,465 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'config': 'squad_v2', 'split': 'train', 'args': 'squad_v2'}}\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name squad_v2 \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/tmp/debug_bloom_squad/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b268463-c6a9-419e-93bd-699f7f742ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
