{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8618a396-4f21-4995-a8d7-b083e55d0020",
   "metadata": {},
   "source": [
    "### Install transformers and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b47ea5-eb49-474c-aa84-3daa26aeba13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/transformers@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffac34a-7b48-479a-b819-7a62e576772a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf714a-54e4-4f3b-9064-2f95c8950bc6",
   "metadata": {},
   "source": [
    "### Set up Cloud Storage as place to store checkpoints and avoid disk space issues\n",
    "### Note: steps must be run from terminal, not Jupyter Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd041a5d-60f0-4d81-a880-09a08f9de885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://cloud.google.com/blog/topics/developers-practitioners/cloud-storage-file-system-vertex-ai-workbench-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f959ae-cda2-4dbe-ae86-459d9526c3ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!gcsfuse --implicit-dirs --rename-dir-limit=100 --disable-http2 --max-conns-per-host=100 $MY_BUCKET \"/home/jupyter/gcs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279db701-3471-4fde-b6f6-6e112592125b",
   "metadata": {},
   "source": [
    "### Collect JSON file of QA pairs generated using Haystack and formatted to match Squad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5f51ba-87d9-4604-996a-43eeb2331e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://haystack_question_test/2022_11_12/jupyter/haystack/json_qa_pairs.json...\n",
      "- [1 files][  9.2 MiB/  9.2 MiB]                                                \n",
      "Operation completed over 1 objects/9.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://haystack_question_test/2022_11_12/jupyter/haystack/json_qa_pairs.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccead8b-d0d7-4769-aba7-9c4a347581da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c5ad65-a889-4d88-ac12-fceb4e087663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_qa_pairs.json', encoding=\"utf-8\") as f:\n",
    "            cdcdata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e584dd-264f-4e7f-9fe0-1e87805953ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100000000000000000000001'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdcdata['id']['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b784170-6bb7-4699-9ade-0d7bcef9e140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10156"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cdcdata['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5513a6c-f3bf-4061-a1a8-0ee7465889c1",
   "metadata": {},
   "source": [
    "### Using SQuAD data loader (modified for our data format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc35b6c9-d8a8-4e55-9ccb-5d3665e2d6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    def generate_examples(filepath):\n",
    "        \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n",
    "        #logger.info(\"generating examples from = %s\", filepath)\n",
    "        key = 0\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            cdcdata = json.load(f)\n",
    "            for x in range(0,len(cdcdata['id'])):\n",
    "                curr = str(x)\n",
    "                title = cdcdata['title'][curr]\n",
    "                context = cdcdata['context'][curr]\n",
    "                answer_starts = cdcdata['answer_start'][curr]\n",
    "                answers = cdcdata['answer_text'][curr]\n",
    "                question = cdcdata['question'][curr]\n",
    "                ids = cdcdata['id'][curr]\n",
    "\n",
    "                yield key, {\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": question,\n",
    "                    \"id\": ids,\n",
    "                    \"answers\": {\n",
    "                        \"answer_start\": answer_starts,\n",
    "                        \"text\": answers,\n",
    "                    },\n",
    "                }\n",
    "                key += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88945aaf-88a4-4e1e-82ad-7ae6fc6fccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdctest = generate_examples('json_qa_pairs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3370500-4121-475f-95d9-3fa3b506b06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, {'title': 'f059e215ee14a89be75d577ec5ad4eb2', 'context': 'cdc.gov/coronavirus\\nWhat to Expect after Getting a COVID-19 Vaccine\\nThe COVID-19 shot may cause side effects in some people. Side effects should go away in a few days. On the arm where you got the shot:\\n• Pain\\n• Redness\\n• Swelling\\nIf you are sore where you got the shot:\\n• Apply a clean, cool, wet washcloth over the area\\n• Use or move your arm gently\\nIf you have a fever:\\n• Drink a lot of water\\n• Get plenty of rest\\n• Dress lightly\\nIf you have pain, headache, or fever, ask a healthcare provider (or facility\\nstaff) if you can have medicine.', 'question': 'What can happen to some people after getting a COVID-19 Vaccine?', 'id': '100000000000000000000001', 'answers': {'answer_start': 96, 'text': 'side effects'}})\n"
     ]
    }
   ],
   "source": [
    "print(next(cdctest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00244c9-b315-4841-b1f4-18bbb7c749b6",
   "metadata": {},
   "source": [
    "### This appears to match the format for SQuAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91572f72-0a01-4c15-a725-c9f1d0922cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ece1653-57f9-4fed-95f1-7e1c1e0b8977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#testing123 = datasets.load_dataset(\"cdctest\") ##Does not work without a local script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13086139-f7b6-415e-92e1-53f70031d634",
   "metadata": {},
   "source": [
    "### Next to create the loading script, modified from squad.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e73cac01-257c-4ff6-ab40-7e31d5dc49d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cdc_test/plain_text to /home/jupyter/.cache/huggingface/datasets/cdc_test/plain_text/1.0.0/c93b7d49b570ab9ee2be29b6ed1ca87b47821f3ae95cf486f90660d66c33bd71...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 4466.78it/s]\n",
      "\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 714.53it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7397/2551724809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jupyter/cdc_test.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m         \u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m     )\n\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m                             \u001b[0mverify_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                             \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m                             \u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m                         )\n\u001b[1;32m    820\u001b[0m                     \u001b[0;31m# Sync info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_download_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_splits_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         super()._download_and_prepare(\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_duplicate_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_splits_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m         )\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m                 \u001b[0;31m# Prepare split will record examples associated to the split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m                 raise OSError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, max_shard_size)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msplit_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0mwriter_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetWriter\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"parquet\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_generate_examples\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0mencoded\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \"\"\"\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m     def _prepare_split(\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#https://huggingface.co/docs/datasets/loading#local-loading-script\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"/home/jupyter/cdc_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b2bce1-47f3-4ac8-9c73-4633bb68ace3",
   "metadata": {},
   "source": [
    "### Hmm. Have to provide train/dev split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbb4aa6e-a9f4-46d7-8495-41b403597a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d63971a-7a7c-48e9-88f8-280ba36d8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "#with open('json_qa_pairs.json') as f:\n",
    "#    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ba942536-b9df-4948-9147-c98ecc54b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('json_qa_pairs.json', dtype={'id':'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "9ed616e8-8006-4bad-ad4f-6cead586bca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['answer_start'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "798ef898-350e-44f5-af87-6165b4124b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"answer_start\"] = pd.to_numeric(df[\"answer_start\"], downcast = \"integer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9fc34089-abe7-46ec-9ff6-dc7f1b5663dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df['answer_start'][0]) #numpy.int16 not json serializable either... ugh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "aed79c3d-3a2f-465c-85a3-6b721be2b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.astype({'answer_start': 'int8'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3b903824-5fd1-41ab-8a0c-b533f365ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"answer_start\"] = df[\"answer_start\"].astype('int8')  #Converts some number to negative??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "40b3cb0d-9234-4769-bb51-dc3b0a22c525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int8"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type(df['answer_start'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "607a927b-d8de-4f3a-bbe8-c6bfe2c5aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df[\"answer_start\"][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "eff233c6-71c9-434c-8b7d-057ffcbddaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"answer_start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "c0165fc9-c9b4-4049-8e04-0521848e5509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df[\"answer_start\"] = df[\"answer_start\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7365c6c6-54cb-4656-b0c8-488f6715ced9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10156"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(df[\"answer_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0c720e05-da0a-4374-b2c7-2ea741e09bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in range(0,len(df[\"answer_start\"])):\n",
    "#    df[\"answer_start\"][x] = df[\"answer_start\"][x].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6c8088e6-d5cc-4856-b02b-ea9c2c6efd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df['answer_start'][0]) #sigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "db55cf65-46e9-4a8d-a232-ccd0ed50f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_st = df[\"answer_start\"].tolist() #convert to python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "8072ba8c-e4d5-4ae9-aec0-786a5dc80484",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96,\n",
       " 125,\n",
       " 375,\n",
       " 471,\n",
       " 281,\n",
       " 375,\n",
       " 477,\n",
       " 114,\n",
       " 156,\n",
       " 259,\n",
       " 228,\n",
       " 436,\n",
       " 455,\n",
       " 542,\n",
       " 8,\n",
       " 132,\n",
       " 8,\n",
       " 326,\n",
       " 185,\n",
       " 140,\n",
       " 209,\n",
       " 346,\n",
       " 531,\n",
       " 577,\n",
       " 29,\n",
       " 239,\n",
       " 46,\n",
       " 230,\n",
       " 297,\n",
       " 306,\n",
       " 0,\n",
       " 103,\n",
       " 345,\n",
       " 103,\n",
       " 369,\n",
       " 83,\n",
       " 408,\n",
       " 209,\n",
       " 408,\n",
       " 490,\n",
       " 634,\n",
       " 589,\n",
       " 150,\n",
       " 165,\n",
       " 280,\n",
       " 552,\n",
       " 84,\n",
       " 66,\n",
       " 208,\n",
       " 208,\n",
       " 128,\n",
       " 99,\n",
       " 271,\n",
       " 196,\n",
       " 64,\n",
       " 193,\n",
       " 224,\n",
       " 371,\n",
       " 430,\n",
       " 140,\n",
       " 452,\n",
       " 343,\n",
       " 32,\n",
       " 107,\n",
       " 283,\n",
       " 283,\n",
       " 374,\n",
       " 514,\n",
       " 28,\n",
       " 58,\n",
       " 156,\n",
       " 323,\n",
       " 375,\n",
       " 463,\n",
       " 29,\n",
       " 173,\n",
       " 112,\n",
       " 450,\n",
       " 136,\n",
       " 11,\n",
       " 388,\n",
       " 712,\n",
       " 602,\n",
       " 582,\n",
       " 62,\n",
       " 104,\n",
       " 34,\n",
       " 356,\n",
       " 600,\n",
       " 817,\n",
       " 876,\n",
       " 260,\n",
       " 316,\n",
       " 309,\n",
       " 528,\n",
       " 252,\n",
       " 322,\n",
       " 729,\n",
       " 516,\n",
       " 516,\n",
       " 45,\n",
       " 92,\n",
       " 295,\n",
       " 443,\n",
       " 443,\n",
       " 525,\n",
       " 542,\n",
       " 619,\n",
       " 270,\n",
       " 59,\n",
       " 201,\n",
       " 270,\n",
       " 420,\n",
       " 491,\n",
       " 636,\n",
       " 588,\n",
       " 86,\n",
       " 110,\n",
       " 232,\n",
       " 275,\n",
       " 426,\n",
       " 529,\n",
       " 489,\n",
       " 581,\n",
       " 0,\n",
       " 0,\n",
       " 323,\n",
       " 50,\n",
       " 367,\n",
       " 435,\n",
       " 573,\n",
       " 538,\n",
       " 0,\n",
       " 62,\n",
       " 4,\n",
       " 218,\n",
       " 254,\n",
       " 490,\n",
       " 557,\n",
       " 0,\n",
       " 82,\n",
       " 330,\n",
       " 437,\n",
       " 538,\n",
       " 27,\n",
       " 39,\n",
       " 119,\n",
       " 300,\n",
       " 265,\n",
       " 472,\n",
       " 565,\n",
       " 8,\n",
       " 196,\n",
       " 3,\n",
       " 556,\n",
       " 700,\n",
       " 712,\n",
       " 0,\n",
       " 186,\n",
       " 281,\n",
       " 268,\n",
       " 419,\n",
       " 0,\n",
       " 182,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 141,\n",
       " 177,\n",
       " 309,\n",
       " 309,\n",
       " 336,\n",
       " 0,\n",
       " 221,\n",
       " 266,\n",
       " 14,\n",
       " 14,\n",
       " 339,\n",
       " 339,\n",
       " 459,\n",
       " 459,\n",
       " 1020,\n",
       " 711,\n",
       " 810,\n",
       " 889,\n",
       " 931,\n",
       " 1036,\n",
       " 1127,\n",
       " 1184,\n",
       " 1192,\n",
       " 143,\n",
       " 267,\n",
       " 267,\n",
       " 465,\n",
       " 508,\n",
       " 673,\n",
       " 72,\n",
       " 149,\n",
       " 260,\n",
       " 352,\n",
       " 458,\n",
       " 0,\n",
       " 53,\n",
       " 392,\n",
       " 467,\n",
       " 544,\n",
       " 513,\n",
       " 65,\n",
       " 42,\n",
       " 254,\n",
       " 0,\n",
       " 437,\n",
       " 510,\n",
       " 577,\n",
       " 26,\n",
       " 146,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 591,\n",
       " 565,\n",
       " 97,\n",
       " 179,\n",
       " 413,\n",
       " 352,\n",
       " 413,\n",
       " 508,\n",
       " 128,\n",
       " 183,\n",
       " 203,\n",
       " 507,\n",
       " 37,\n",
       " 103,\n",
       " 259,\n",
       " 259,\n",
       " 441,\n",
       " 495,\n",
       " 0,\n",
       " 284,\n",
       " 605,\n",
       " 381,\n",
       " 605,\n",
       " 57,\n",
       " 383,\n",
       " 383,\n",
       " 286,\n",
       " 39,\n",
       " 0,\n",
       " 450,\n",
       " 371,\n",
       " 0,\n",
       " 629,\n",
       " 35,\n",
       " 220,\n",
       " 270,\n",
       " 292,\n",
       " 306,\n",
       " 236,\n",
       " 152,\n",
       " 325,\n",
       " 590,\n",
       " 37,\n",
       " 172,\n",
       " 406,\n",
       " 487,\n",
       " 487,\n",
       " 553,\n",
       " 0,\n",
       " 109,\n",
       " 20,\n",
       " 376,\n",
       " 453,\n",
       " 472,\n",
       " 517,\n",
       " 143,\n",
       " 183,\n",
       " 261,\n",
       " 261,\n",
       " 392,\n",
       " 556,\n",
       " 635,\n",
       " 635,\n",
       " 635,\n",
       " 148,\n",
       " 167,\n",
       " 182,\n",
       " 550,\n",
       " 230,\n",
       " 230,\n",
       " 153,\n",
       " 66,\n",
       " 34,\n",
       " 9,\n",
       " 167,\n",
       " 508,\n",
       " 508,\n",
       " 137,\n",
       " 406,\n",
       " 21,\n",
       " 202,\n",
       " 202,\n",
       " 118,\n",
       " 128,\n",
       " 316,\n",
       " 332,\n",
       " 44,\n",
       " 74,\n",
       " 206,\n",
       " 122,\n",
       " 345,\n",
       " 507,\n",
       " 507,\n",
       " 0,\n",
       " 103,\n",
       " 209,\n",
       " 209,\n",
       " 32,\n",
       " 209,\n",
       " 443,\n",
       " 103,\n",
       " 135,\n",
       " 135,\n",
       " 401,\n",
       " 494,\n",
       " 34,\n",
       " 9,\n",
       " 230,\n",
       " 354,\n",
       " 465,\n",
       " 465,\n",
       " 3,\n",
       " 140,\n",
       " 189,\n",
       " 253,\n",
       " 325,\n",
       " 20,\n",
       " 242,\n",
       " 277,\n",
       " 277,\n",
       " 620,\n",
       " 686,\n",
       " 635,\n",
       " 0,\n",
       " 96,\n",
       " 327,\n",
       " 457,\n",
       " 474,\n",
       " 566,\n",
       " 0,\n",
       " 75,\n",
       " 122,\n",
       " 209,\n",
       " 414,\n",
       " 414,\n",
       " 414,\n",
       " 57,\n",
       " 101,\n",
       " 247,\n",
       " 360,\n",
       " 585,\n",
       " 9,\n",
       " 15,\n",
       " 424,\n",
       " 424,\n",
       " 66,\n",
       " 246,\n",
       " 22,\n",
       " 238,\n",
       " 321,\n",
       " 450,\n",
       " 511,\n",
       " 511,\n",
       " 538,\n",
       " 20,\n",
       " 155,\n",
       " 402,\n",
       " 613,\n",
       " 613,\n",
       " 613,\n",
       " 4,\n",
       " 108,\n",
       " 240,\n",
       " 240,\n",
       " 416,\n",
       " 532,\n",
       " 590,\n",
       " 3,\n",
       " 73,\n",
       " 111,\n",
       " 335,\n",
       " 359,\n",
       " 116,\n",
       " 190,\n",
       " 33,\n",
       " 365,\n",
       " 589,\n",
       " 0,\n",
       " 113,\n",
       " 408,\n",
       " 444,\n",
       " 440,\n",
       " 133,\n",
       " 219,\n",
       " 247,\n",
       " 219,\n",
       " 455,\n",
       " 3,\n",
       " 201,\n",
       " 213,\n",
       " 213,\n",
       " 425,\n",
       " 0,\n",
       " 79,\n",
       " 237,\n",
       " 405,\n",
       " 405,\n",
       " 301,\n",
       " 3,\n",
       " 146,\n",
       " 213,\n",
       " 425,\n",
       " 494,\n",
       " 302,\n",
       " 510,\n",
       " 100,\n",
       " 0,\n",
       " 208,\n",
       " 435,\n",
       " 435,\n",
       " 610,\n",
       " 3,\n",
       " 86,\n",
       " 357,\n",
       " 446,\n",
       " 568,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 490,\n",
       " 507,\n",
       " 602,\n",
       " 602,\n",
       " 10,\n",
       " 143,\n",
       " 253,\n",
       " 439,\n",
       " 477,\n",
       " 477,\n",
       " 3,\n",
       " 186,\n",
       " 282,\n",
       " 282,\n",
       " 402,\n",
       " 454,\n",
       " 324,\n",
       " 3,\n",
       " 3,\n",
       " 131,\n",
       " 258,\n",
       " 258,\n",
       " 332,\n",
       " 442,\n",
       " 507,\n",
       " 359,\n",
       " 70,\n",
       " 84,\n",
       " 319,\n",
       " 240,\n",
       " 489,\n",
       " 164,\n",
       " 164,\n",
       " 382,\n",
       " 423,\n",
       " 164,\n",
       " 84,\n",
       " 197,\n",
       " 314,\n",
       " 314,\n",
       " 456,\n",
       " 95,\n",
       " 251,\n",
       " 288,\n",
       " 493,\n",
       " 493,\n",
       " 542,\n",
       " 3,\n",
       " 16,\n",
       " 169,\n",
       " 391,\n",
       " 278,\n",
       " 3,\n",
       " 135,\n",
       " 248,\n",
       " 248,\n",
       " 503,\n",
       " 298,\n",
       " 546,\n",
       " 563,\n",
       " 563,\n",
       " 616,\n",
       " 113,\n",
       " 49,\n",
       " 136,\n",
       " 316,\n",
       " 136,\n",
       " 513,\n",
       " 486,\n",
       " 662,\n",
       " 26,\n",
       " 143,\n",
       " 158,\n",
       " 527,\n",
       " 365,\n",
       " 440,\n",
       " 34,\n",
       " 156,\n",
       " 283,\n",
       " 50,\n",
       " 535,\n",
       " 17,\n",
       " 204,\n",
       " 226,\n",
       " 231,\n",
       " 539,\n",
       " 0,\n",
       " 293,\n",
       " 238,\n",
       " 572,\n",
       " 540,\n",
       " 36,\n",
       " 225,\n",
       " 225,\n",
       " 225,\n",
       " 453,\n",
       " 511,\n",
       " 3,\n",
       " 213,\n",
       " 320,\n",
       " 369,\n",
       " 546,\n",
       " 263,\n",
       " 187,\n",
       " 0,\n",
       " 0,\n",
       " 298,\n",
       " 70,\n",
       " 108,\n",
       " 311,\n",
       " 231,\n",
       " 468,\n",
       " 141,\n",
       " 19,\n",
       " 426,\n",
       " 391,\n",
       " 606,\n",
       " 585,\n",
       " 143,\n",
       " 294,\n",
       " 378,\n",
       " 434,\n",
       " 110,\n",
       " 110,\n",
       " 32,\n",
       " 110,\n",
       " 296,\n",
       " 529,\n",
       " 112,\n",
       " 248,\n",
       " 156,\n",
       " 112,\n",
       " 528,\n",
       " 449,\n",
       " 109,\n",
       " 109,\n",
       " 382,\n",
       " 382,\n",
       " 109,\n",
       " 520,\n",
       " 111,\n",
       " 156,\n",
       " 279,\n",
       " 385,\n",
       " 295,\n",
       " 17,\n",
       " 111,\n",
       " 111,\n",
       " 106,\n",
       " 156,\n",
       " 298,\n",
       " 432,\n",
       " 432,\n",
       " 432,\n",
       " 252,\n",
       " 252,\n",
       " 523,\n",
       " 294,\n",
       " 661,\n",
       " 252,\n",
       " 0,\n",
       " 107,\n",
       " 134,\n",
       " 107,\n",
       " 91,\n",
       " 107,\n",
       " 0,\n",
       " 111,\n",
       " 254,\n",
       " 15,\n",
       " 254,\n",
       " 551,\n",
       " 551,\n",
       " 14,\n",
       " 385,\n",
       " 522,\n",
       " 385,\n",
       " 247,\n",
       " 14,\n",
       " 504,\n",
       " 504,\n",
       " 492,\n",
       " 106,\n",
       " 157,\n",
       " 14,\n",
       " 362,\n",
       " 490,\n",
       " 14,\n",
       " 484,\n",
       " 234,\n",
       " 510,\n",
       " 234,\n",
       " 484,\n",
       " 142,\n",
       " 234,\n",
       " 105,\n",
       " 237,\n",
       " 105,\n",
       " 411,\n",
       " 104,\n",
       " 226,\n",
       " 238,\n",
       " 372,\n",
       " 360,\n",
       " 9,\n",
       " 16,\n",
       " 148,\n",
       " 105,\n",
       " 105,\n",
       " 507,\n",
       " 282,\n",
       " 105,\n",
       " 148,\n",
       " 16,\n",
       " 105,\n",
       " 411,\n",
       " 635,\n",
       " 623,\n",
       " 108,\n",
       " 234,\n",
       " 154,\n",
       " 389,\n",
       " 389,\n",
       " 108,\n",
       " 110,\n",
       " 19,\n",
       " 241,\n",
       " 0,\n",
       " 494,\n",
       " 539,\n",
       " 110,\n",
       " 110,\n",
       " 18,\n",
       " 176,\n",
       " 240,\n",
       " 375,\n",
       " 105,\n",
       " 0,\n",
       " 109,\n",
       " 171,\n",
       " 310,\n",
       " 417,\n",
       " 542,\n",
       " 553,\n",
       " 18,\n",
       " 238,\n",
       " 422,\n",
       " 282,\n",
       " 615,\n",
       " 540,\n",
       " 18,\n",
       " 18,\n",
       " 248,\n",
       " 382,\n",
       " 294,\n",
       " 564,\n",
       " 547,\n",
       " 88,\n",
       " 223,\n",
       " 316,\n",
       " 177,\n",
       " 316,\n",
       " 69,\n",
       " 177,\n",
       " 112,\n",
       " 251,\n",
       " 160,\n",
       " 390,\n",
       " 576,\n",
       " 568,\n",
       " 557,\n",
       " 108,\n",
       " 574,\n",
       " 243,\n",
       " 108,\n",
       " 423,\n",
       " 19,\n",
       " 0,\n",
       " 19,\n",
       " 242,\n",
       " 373,\n",
       " 509,\n",
       " 61,\n",
       " 18,\n",
       " 186,\n",
       " 414,\n",
       " 414,\n",
       " 549,\n",
       " 57,\n",
       " 163,\n",
       " 255,\n",
       " 132,\n",
       " 290,\n",
       " 112,\n",
       " 184,\n",
       " 17,\n",
       " 400,\n",
       " 471,\n",
       " 160,\n",
       " 20,\n",
       " 243,\n",
       " 378,\n",
       " 378,\n",
       " 541,\n",
       " 110,\n",
       " 39,\n",
       " 187,\n",
       " 251,\n",
       " 383,\n",
       " 295,\n",
       " 426,\n",
       " 93,\n",
       " 240,\n",
       " 381,\n",
       " 305,\n",
       " 436,\n",
       " 220,\n",
       " 270,\n",
       " 147,\n",
       " 262,\n",
       " 437,\n",
       " 277,\n",
       " 483,\n",
       " 483,\n",
       " 31,\n",
       " 69,\n",
       " 290,\n",
       " 320,\n",
       " 490,\n",
       " 618,\n",
       " 538,\n",
       " 572,\n",
       " 215,\n",
       " 170,\n",
       " 322,\n",
       " 314,\n",
       " 447,\n",
       " 447,\n",
       " 557,\n",
       " 0,\n",
       " 102,\n",
       " 225,\n",
       " 440,\n",
       " 513,\n",
       " 350,\n",
       " 40,\n",
       " 149,\n",
       " 317,\n",
       " 317,\n",
       " 289,\n",
       " 293,\n",
       " 214,\n",
       " 214,\n",
       " 375,\n",
       " 598,\n",
       " 532,\n",
       " 0,\n",
       " 0,\n",
       " 280,\n",
       " 398,\n",
       " 508,\n",
       " 508,\n",
       " 37,\n",
       " 198,\n",
       " 289,\n",
       " 342,\n",
       " 328,\n",
       " 396,\n",
       " 281,\n",
       " 281,\n",
       " 765,\n",
       " 765,\n",
       " 765,\n",
       " 765,\n",
       " 58,\n",
       " 73,\n",
       " 208,\n",
       " 487,\n",
       " 79,\n",
       " 71,\n",
       " 482,\n",
       " 339,\n",
       " 32,\n",
       " 231,\n",
       " 231,\n",
       " 522,\n",
       " 327,\n",
       " 20,\n",
       " 405,\n",
       " 481,\n",
       " 622,\n",
       " 578,\n",
       " 87,\n",
       " 403,\n",
       " 382,\n",
       " 27,\n",
       " 35,\n",
       " 0,\n",
       " 280,\n",
       " 385,\n",
       " 522,\n",
       " 634,\n",
       " 0,\n",
       " 0,\n",
       " 303,\n",
       " 0,\n",
       " 0,\n",
       " 568,\n",
       " 623,\n",
       " 50,\n",
       " 121,\n",
       " 310,\n",
       " 1,\n",
       " 317,\n",
       " 18,\n",
       " 317,\n",
       " 411,\n",
       " 543,\n",
       " 70,\n",
       " 138,\n",
       " 239,\n",
       " 168,\n",
       " 368,\n",
       " 160,\n",
       " 213,\n",
       " 213,\n",
       " 367,\n",
       " 144,\n",
       " 112,\n",
       " 0,\n",
       " 177,\n",
       " 195,\n",
       " 429,\n",
       " 79,\n",
       " 205,\n",
       " 309,\n",
       " 340,\n",
       " 358,\n",
       " 81,\n",
       " 89,\n",
       " 357,\n",
       " 557,\n",
       " 357,\n",
       " 357,\n",
       " 562,\n",
       " 562,\n",
       " 59,\n",
       " 61,\n",
       " 210,\n",
       " 339,\n",
       " 210,\n",
       " 418,\n",
       " 24,\n",
       " 161,\n",
       " 298,\n",
       " 303,\n",
       " 0,\n",
       " 327,\n",
       " 9,\n",
       " 327,\n",
       " 430,\n",
       " 557,\n",
       " 97,\n",
       " 186,\n",
       " 313,\n",
       " 423,\n",
       " 381,\n",
       " 546,\n",
       " 550,\n",
       " 0,\n",
       " 23,\n",
       " 183,\n",
       " 327,\n",
       " 444,\n",
       " 23,\n",
       " 23,\n",
       " 0,\n",
       " 63,\n",
       " 242,\n",
       " 249,\n",
       " 334,\n",
       " 257,\n",
       " 97,\n",
       " 54,\n",
       " 307,\n",
       " 389,\n",
       " 434,\n",
       " 159,\n",
       " 129,\n",
       " 286,\n",
       " 286,\n",
       " 379,\n",
       " 0,\n",
       " 0,\n",
       " 569,\n",
       " 512,\n",
       " 168,\n",
       " 13,\n",
       " 241,\n",
       " 374,\n",
       " 405,\n",
       " 222,\n",
       " 507,\n",
       " 562,\n",
       " 60,\n",
       " 209,\n",
       " 209,\n",
       " 169,\n",
       " 370,\n",
       " 0,\n",
       " 44,\n",
       " 335,\n",
       " 424,\n",
       " 52,\n",
       " 174,\n",
       " 120,\n",
       " 237,\n",
       " 438,\n",
       " 395,\n",
       " 42,\n",
       " 106,\n",
       " 115,\n",
       " 3,\n",
       " 151,\n",
       " 239,\n",
       " 280,\n",
       " 299,\n",
       " 472,\n",
       " 526,\n",
       " 562,\n",
       " 513,\n",
       " 53,\n",
       " 148,\n",
       " 0,\n",
       " 226,\n",
       " 360,\n",
       " 105,\n",
       " 151,\n",
       " 251,\n",
       " 282,\n",
       " 282,\n",
       " 534,\n",
       " 75,\n",
       " 142,\n",
       " 262,\n",
       " 379,\n",
       " 478,\n",
       " 499,\n",
       " 478,\n",
       " 0,\n",
       " 146,\n",
       " 94,\n",
       " 285,\n",
       " 270,\n",
       " 344,\n",
       " 610,\n",
       " 37,\n",
       " 81,\n",
       " 325,\n",
       " 290,\n",
       " 380,\n",
       " 446,\n",
       " ...]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "4addb074-1188-4106-8c14-eae612c38dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ans_st[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "9f8e6f3d-124a-465c-93f6-b447a842c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"answer_start\"] = ans_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "1c04128e-0c41-4f41-96b5-f011945725ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df[\"answer_start\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac6970-7151-41e2-9c2e-b490b61843be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f884d-0fcd-44ab-9f41-67c0032de38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "3c6db28c-376f-40a9-ba84-449baa24036e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000000000000000000001</td>\n",
       "      <td>f059e215ee14a89be75d577ec5ad4eb2</td>\n",
       "      <td>cdc.gov/coronavirus\\nWhat to Expect after Gett...</td>\n",
       "      <td>What can happen to some people after getting a...</td>\n",
       "      <td>side effects</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100000000000000000000002</td>\n",
       "      <td>f059e215ee14a89be75d577ec5ad4eb2</td>\n",
       "      <td>cdc.gov/coronavirus\\nWhat to Expect after Gett...</td>\n",
       "      <td>What should go away in a few days?</td>\n",
       "      <td>Side effects</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000000000000000000003</td>\n",
       "      <td>f059e215ee14a89be75d577ec5ad4eb2</td>\n",
       "      <td>cdc.gov/coronavirus\\nWhat to Expect after Gett...</td>\n",
       "      <td>What should you do if you have a fever?</td>\n",
       "      <td>Drink a lot of water</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100000000000000000000004</td>\n",
       "      <td>f059e215ee14a89be75d577ec5ad4eb2</td>\n",
       "      <td>cdc.gov/coronavirus\\nWhat to Expect after Gett...</td>\n",
       "      <td>How do you treat a headache?</td>\n",
       "      <td>ask a healthcare provider (or facility staff) ...</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000000000000000000005</td>\n",
       "      <td>f059e215ee14a89be75d577ec5ad4eb2</td>\n",
       "      <td>cdc.gov/coronavirus\\nWhat to Expect after Gett...</td>\n",
       "      <td>What is the name of the washcloth that you app...</td>\n",
       "      <td>clean, cool, wet</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10151</th>\n",
       "      <td>100000000000000000010152</td>\n",
       "      <td>feefd4010c8db878038b3867b0739ecb</td>\n",
       "      <td>30 minutes: Vaccination providers should consi...</td>\n",
       "      <td>How long should a person have an allergic reac...</td>\n",
       "      <td>30 minutes</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10152</th>\n",
       "      <td>100000000000000000010153</td>\n",
       "      <td>feefd4010c8db878038b3867b0739ecb</td>\n",
       "      <td>30 minutes: Vaccination providers should consi...</td>\n",
       "      <td>What is not recommended for vaccine decision-m...</td>\n",
       "      <td>Antibody testing</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10153</th>\n",
       "      <td>100000000000000000010154</td>\n",
       "      <td>feefd4010c8db878038b3867b0739ecb</td>\n",
       "      <td>30 minutes: Vaccination providers should consi...</td>\n",
       "      <td>What should be reported to VAERS?</td>\n",
       "      <td>Adverse events that occur following COVID-19 v...</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10154</th>\n",
       "      <td>100000000000000000010155</td>\n",
       "      <td>89ea205deddff8b58ad49f5babca9ee9</td>\n",
       "      <td>COVID-19 providers are required to\\nreport:\\n...</td>\n",
       "      <td>What are providers of COVID-19 required to rep...</td>\n",
       "      <td>Vaccine administration errors  Serious advers...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10155</th>\n",
       "      <td>100000000000000000010156</td>\n",
       "      <td>89ea205deddff8b58ad49f5babca9ee9</td>\n",
       "      <td>COVID-19 providers are required to\\nreport:\\n...</td>\n",
       "      <td>What are some cases of Multisystem Inflammator...</td>\n",
       "      <td>Cases of COVID-19 that result in hospitalizati...</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10156 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id                             title  \\\n",
       "0      100000000000000000000001  f059e215ee14a89be75d577ec5ad4eb2   \n",
       "1      100000000000000000000002  f059e215ee14a89be75d577ec5ad4eb2   \n",
       "2      100000000000000000000003  f059e215ee14a89be75d577ec5ad4eb2   \n",
       "3      100000000000000000000004  f059e215ee14a89be75d577ec5ad4eb2   \n",
       "4      100000000000000000000005  f059e215ee14a89be75d577ec5ad4eb2   \n",
       "...                         ...                               ...   \n",
       "10151  100000000000000000010152  feefd4010c8db878038b3867b0739ecb   \n",
       "10152  100000000000000000010153  feefd4010c8db878038b3867b0739ecb   \n",
       "10153  100000000000000000010154  feefd4010c8db878038b3867b0739ecb   \n",
       "10154  100000000000000000010155  89ea205deddff8b58ad49f5babca9ee9   \n",
       "10155  100000000000000000010156  89ea205deddff8b58ad49f5babca9ee9   \n",
       "\n",
       "                                                 context  \\\n",
       "0      cdc.gov/coronavirus\\nWhat to Expect after Gett...   \n",
       "1      cdc.gov/coronavirus\\nWhat to Expect after Gett...   \n",
       "2      cdc.gov/coronavirus\\nWhat to Expect after Gett...   \n",
       "3      cdc.gov/coronavirus\\nWhat to Expect after Gett...   \n",
       "4      cdc.gov/coronavirus\\nWhat to Expect after Gett...   \n",
       "...                                                  ...   \n",
       "10151  30 minutes: Vaccination providers should consi...   \n",
       "10152  30 minutes: Vaccination providers should consi...   \n",
       "10153  30 minutes: Vaccination providers should consi...   \n",
       "10154  COVID-19 providers are required to\\nreport:\\n...   \n",
       "10155  COVID-19 providers are required to\\nreport:\\n...   \n",
       "\n",
       "                                                question  \\\n",
       "0      What can happen to some people after getting a...   \n",
       "1                     What should go away in a few days?   \n",
       "2                What should you do if you have a fever?   \n",
       "3                           How do you treat a headache?   \n",
       "4      What is the name of the washcloth that you app...   \n",
       "...                                                  ...   \n",
       "10151  How long should a person have an allergic reac...   \n",
       "10152  What is not recommended for vaccine decision-m...   \n",
       "10153                  What should be reported to VAERS?   \n",
       "10154  What are providers of COVID-19 required to rep...   \n",
       "10155  What are some cases of Multisystem Inflammator...   \n",
       "\n",
       "                                             answer_text  answer_start  \n",
       "0                                           side effects            96  \n",
       "1                                           Side effects           125  \n",
       "2                                   Drink a lot of water           375  \n",
       "3      ask a healthcare provider (or facility staff) ...           471  \n",
       "4                                       clean, cool, wet           281  \n",
       "...                                                  ...           ...  \n",
       "10151                                         30 minutes           109  \n",
       "10152                                   Antibody testing           454  \n",
       "10153  Adverse events that occur following COVID-19 v...           586  \n",
       "10154  Vaccine administration errors  Serious advers...            45  \n",
       "10155  Cases of COVID-19 that result in hospitalizati...           147  \n",
       "\n",
       "[10156 rows x 6 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "728ea157-bae6-4f8d-81e0-c5555ec79948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b4582af5-e03d-4c47-b7ca-68f9acfb7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3)\n",
    "#val, test = train_test_split(test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7ae57305-25d7-49b5-b6ed-6f317d437794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7109"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "17cbc62a-0531-43b9-b084-c8a41c8c9580",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>100000000000000000005384</td>\n",
       "      <td>e94a57098c4a84b0d6c7a56d4069cd31</td>\n",
       "      <td>Should you decide not\\nto receive the Janssen ...</td>\n",
       "      <td>What is only authorized if other COVID-19 vacc...</td>\n",
       "      <td>The Janssen COVID-19 Vaccine</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>100000000000000000006835</td>\n",
       "      <td>a3b7d0642ecafbe24abc57f5e6852ba7</td>\n",
       "      <td>Remember to bring the card when your child ret...</td>\n",
       "      <td>What may your provider include your child's va...</td>\n",
       "      <td>Immunization Information System</td>\n",
       "      <td>781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>100000000000000000005202</td>\n",
       "      <td>37202414135829f322ab4bcd44939b6c</td>\n",
       "      <td>CDC and FDA will use this information to guide...</td>\n",
       "      <td>Who will be looking at health effects after re...</td>\n",
       "      <td>scientists</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3003</th>\n",
       "      <td>100000000000000000003004</td>\n",
       "      <td>ba220eeb6826ab5196a0b8d77503b8ce</td>\n",
       "      <td>The monovalent Novavax booster dose is adminis...</td>\n",
       "      <td>How many months after the last monovalent boos...</td>\n",
       "      <td>2 months</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7693</th>\n",
       "      <td>100000000000000000007694</td>\n",
       "      <td>8a32db8038628066eb9e74798b1f93bb</td>\n",
       "      <td>They are:\\nAnaphylaxis\\nAnaphylaxis is a sever...</td>\n",
       "      <td>What is pericarditis inflammation of?</td>\n",
       "      <td>the outer lining of the heart</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>100000000000000000001660</td>\n",
       "      <td>940280692f10d1951f8b172a84ec1292</td>\n",
       "      <td>COVID-19 Vaccine Administration Fees (updated ...</td>\n",
       "      <td>What does a program or plan cover?</td>\n",
       "      <td>COVID-19 Vaccine administration fees</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7563</th>\n",
       "      <td>100000000000000000007564</td>\n",
       "      <td>530270efddf24e51ee98178e4beb4f40</td>\n",
       "      <td>You will need to show official documentation (...</td>\n",
       "      <td>What is the name of the vaccine candidate that...</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>100000000000000000004611</td>\n",
       "      <td>81089b699b38110a7eb908ae7af13085</td>\n",
       "      <td>10/30/22, 9:41 AM Interactive Home Ventilation...</td>\n",
       "      <td>What can help prevent you from getting and spr...</td>\n",
       "      <td>Good ventilation</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>100000000000000000003623</td>\n",
       "      <td>9f09ca29a2171f252451a2b54b3e7b00</td>\n",
       "      <td>10/30/22, 9:42 AM Domestic Travel During COVID...</td>\n",
       "      <td>What is effective at protecting people from ge...</td>\n",
       "      <td>COVID-19 vaccines</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6501</th>\n",
       "      <td>100000000000000000006502</td>\n",
       "      <td>c2ef094eb03df79cbdad198a54794bf8</td>\n",
       "      <td>Contact a healthcare provider right away after...</td>\n",
       "      <td>Who should see Underlying Medical Conditions A...</td>\n",
       "      <td>Healthcare professionals</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7109 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id                             title  \\\n",
       "5383  100000000000000000005384  e94a57098c4a84b0d6c7a56d4069cd31   \n",
       "6834  100000000000000000006835  a3b7d0642ecafbe24abc57f5e6852ba7   \n",
       "5201  100000000000000000005202  37202414135829f322ab4bcd44939b6c   \n",
       "3003  100000000000000000003004  ba220eeb6826ab5196a0b8d77503b8ce   \n",
       "7693  100000000000000000007694  8a32db8038628066eb9e74798b1f93bb   \n",
       "...                        ...                               ...   \n",
       "1659  100000000000000000001660  940280692f10d1951f8b172a84ec1292   \n",
       "7563  100000000000000000007564  530270efddf24e51ee98178e4beb4f40   \n",
       "4610  100000000000000000004611  81089b699b38110a7eb908ae7af13085   \n",
       "3622  100000000000000000003623  9f09ca29a2171f252451a2b54b3e7b00   \n",
       "6501  100000000000000000006502  c2ef094eb03df79cbdad198a54794bf8   \n",
       "\n",
       "                                                context  \\\n",
       "5383  Should you decide not\\nto receive the Janssen ...   \n",
       "6834  Remember to bring the card when your child ret...   \n",
       "5201  CDC and FDA will use this information to guide...   \n",
       "3003  The monovalent Novavax booster dose is adminis...   \n",
       "7693  They are:\\nAnaphylaxis\\nAnaphylaxis is a sever...   \n",
       "...                                                 ...   \n",
       "1659  COVID-19 Vaccine Administration Fees (updated ...   \n",
       "7563  You will need to show official documentation (...   \n",
       "4610  10/30/22, 9:41 AM Interactive Home Ventilation...   \n",
       "3622  10/30/22, 9:42 AM Domestic Travel During COVID...   \n",
       "6501  Contact a healthcare provider right away after...   \n",
       "\n",
       "                                               question  \\\n",
       "5383  What is only authorized if other COVID-19 vacc...   \n",
       "6834  What may your provider include your child's va...   \n",
       "5201  Who will be looking at health effects after re...   \n",
       "3003  How many months after the last monovalent boos...   \n",
       "7693              What is pericarditis inflammation of?   \n",
       "...                                                 ...   \n",
       "1659                 What does a program or plan cover?   \n",
       "7563  What is the name of the vaccine candidate that...   \n",
       "4610  What can help prevent you from getting and spr...   \n",
       "3622  What is effective at protecting people from ge...   \n",
       "6501  Who should see Underlying Medical Conditions A...   \n",
       "\n",
       "                               answer_text  answer_start  \n",
       "5383          The Janssen COVID-19 Vaccine           319  \n",
       "6834       Immunization Information System           781  \n",
       "5201                            scientists           181  \n",
       "3003                              2 months           495  \n",
       "7693         the outer lining of the heart           725  \n",
       "...                                    ...           ...  \n",
       "1659  COVID-19 Vaccine administration fees           649  \n",
       "7563                              COVID-19           267  \n",
       "4610                      Good ventilation           354  \n",
       "3622                     COVID-19 vaccines           392  \n",
       "6501              Healthcare professionals           378  \n",
       "\n",
       "[7109 rows x 6 columns]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "c4f9c00f-cd4a-421d-9391-1df327f2bf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3047"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "3b915f7f-bb51-41d3-89a1-c307f1a32bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_json('json_qa_pairs_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "9d13ebb9-509b-47a7-a956-d87ee345b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_json('json_qa_pairs_dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf6f7e5-9ea9-47c5-a2f5-95678a211af5",
   "metadata": {},
   "source": [
    "### train/dev split established, try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5de2f885-3aa9-4ed4-8771-77c5ef9e2ce2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://huggingface.co/docs/datasets/loading#local-loading-script\n",
    "\n",
    "#from datasets import load_dataset\n",
    "#dataset = load_dataset(\"/home/jupyter/cdc_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b44bf-d111-4702-919f-a40e06e2ab55",
   "metadata": {},
   "source": [
    "### So discovered a misunderstanding - the loading script does not actually do the formatting\n",
    "### like I thought - that must be preprocessed to match SQuAD format. Ugh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "d6f577ec-896f-4623-af73-ed509c72afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train._get_value(5383, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "e53bfc2b-bfd9-4715-b320-526821023e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5383</th>\n",
       "      <td>100000000000000000005384</td>\n",
       "      <td>e94a57098c4a84b0d6c7a56d4069cd31</td>\n",
       "      <td>Should you decide not\\nto receive the Janssen ...</td>\n",
       "      <td>What is only authorized if other COVID-19 vacc...</td>\n",
       "      <td>The Janssen COVID-19 Vaccine</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id                             title  \\\n",
       "5383  100000000000000000005384  e94a57098c4a84b0d6c7a56d4069cd31   \n",
       "\n",
       "                                                context  \\\n",
       "5383  Should you decide not\\nto receive the Janssen ...   \n",
       "\n",
       "                                               question  \\\n",
       "5383  What is only authorized if other COVID-19 vacc...   \n",
       "\n",
       "                       answer_text  answer_start  \n",
       "5383  The Janssen COVID-19 Vaccine           319  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "590d46ad-ab93-4da4-a767-d6bec3f97465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>100000000000000000000675</td>\n",
       "      <td>20e562776262c6cebe19de217cd8baa5</td>\n",
       "      <td>February 3, 2022: Alaska made updates to data ...</td>\n",
       "      <td>What was the decrease in doses?</td>\n",
       "      <td>3,945</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id                             title  \\\n",
       "674  100000000000000000000675  20e562776262c6cebe19de217cd8baa5   \n",
       "\n",
       "                                               context  \\\n",
       "674  February 3, 2022: Alaska made updates to data ...   \n",
       "\n",
       "                            question answer_text  answer_start  \n",
       "674  What was the decrease in doses?       3,945           105  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0be9f859-d7e1-4c91-8582-4147f4b313ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values = train.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f4c02367-52db-4f90-bb2e-f093b2a1d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "values.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "62caf9f4-f484-450d-aa83-c1a6e8ae3553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     4,     6, ..., 10153, 10154, 10155])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "69d8134f-240b-4169-a114-ff52093aed30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "values = train.index.values\n",
    "values.sort()\n",
    "values = values[:20] #Used to test on smaller dataset\n",
    "prevtitle = 'nofirsttitle'\n",
    "cdcdict = {\"version\": \"v2.0\"}\n",
    "datalist = []\n",
    "#paralist = []\n",
    "\n",
    "for x in values:\n",
    "    #print(\"starting with index {}\".format(x))\n",
    "    #print(\"*********\")\n",
    "    currtitle = train._get_value(x, 'title')\n",
    "    currcont = train._get_value(x, 'context')\n",
    "    currid = train._get_value(x, 'id')\n",
    "    currqas = train._get_value(x, 'question')\n",
    "    #print(\"currqas = {}\".format(currqas))\n",
    "    currans = train._get_value(x, 'answer_text')\n",
    "    #print(\"currans = {}\".format(currans))\n",
    "    currstart = train._get_value(x, 'answer_start')\n",
    "    currstart = currstart.item()  #This converts to Python Int, which avoids JSON writing problems later\n",
    "    #print(\"currstart type {}\".format(currstart))\n",
    "\n",
    "    if currtitle != prevtitle:\n",
    "        \n",
    "        if prevtitle != 'nofirsttitle':\n",
    "            datalist.append(currdict)\n",
    "            #print(\"current datalist {}\".format(datalist))\n",
    "        \n",
    "        y = 0\n",
    "        #print(\"currtitle {} does not equal prevtitle\".format(x))\n",
    "        #print(\"-----------\")\n",
    "\n",
    "        qaslist = []\n",
    "        anslist = []\n",
    "        currdict = {}\n",
    "        #print(\"currdict initialized {}\".format(currdict))\n",
    "        paralist = []\n",
    "        \n",
    "        currdict[\"title\"] = currtitle\n",
    "        #print(\"currdict titled {}\".format(currdict))\n",
    "                \n",
    "        currparagraphs = {}\n",
    "                \n",
    "        currqasdict = {}\n",
    "                \n",
    "        curransdict = {}\n",
    "\n",
    "        curransdict[\"text\"] = currans\n",
    "        curransdict[\"answer_start\"] = currstart\n",
    "        \n",
    "        anslist.append(curransdict)\n",
    "        \n",
    "        currqasdict[\"question\"] = currqas\n",
    "        currqasdict[\"id\"] = currid\n",
    "        currqasdict[\"answers\"] = anslist\n",
    "        currqasdict[\"is_impossible\"] = False\n",
    "        \n",
    "        qaslist.append(currqasdict)\n",
    "        \n",
    "        #print(\"qaslist = {}\".format(qaslist))\n",
    "        #print(\"-----------\")\n",
    "        \n",
    "        #paralist.append(qaslist)\n",
    "        paralist.append(currqasdict)\n",
    "        \n",
    "        #print(\"paralist = {}\".format(paralist))\n",
    "        \n",
    "        #currdict[\"paragraphs\"] = paralist\n",
    "        #currdict[\"title\"][\"paragraphs\"][\"context\"] = currcont\n",
    "        currdict[\"paragraphs\"] = [{\"qas\" : paralist, \"context\" : currcont}]\n",
    "        \n",
    "        #print(\"currdict end of first round {}\".format(currdict))\n",
    "        \n",
    "        prevtitle = currtitle\n",
    "        y += 1\n",
    "        \n",
    "    else:\n",
    "        y += 1\n",
    "        #print(\"currtitle {} equals prevtitle\".format(x))\n",
    "        #print(\"-----------\")\n",
    "        #qaslist = []\n",
    "        anslist = []\n",
    "        #currqasdict = {}\n",
    "        \n",
    "        #print(\"qaslist before processing = {}\".format(qaslist))\n",
    "        \n",
    "        #print(\"currdict current {}\".format(currdict))\n",
    "        \n",
    "                \n",
    "        curransdict = {}\n",
    "        currqasdict = {}\n",
    "        \n",
    "        #print(\"refresehd curransdict = {}\".format(curransdict))\n",
    "\n",
    "        curransdict[\"text\"] = currans\n",
    "        curransdict[\"answer_start\"] = currstart\n",
    "        \n",
    "        #print(\"qaslist after new dict entries = {}\".format(qaslist))\n",
    "        \n",
    "        anslist.append(curransdict)\n",
    "        #print(\"anslist = {}\".format(anslist))\n",
    "        #print(\"qaslist after appending answers to anslist = {}\".format(qaslist))\n",
    "        \n",
    "        currqasdict[\"question\"] = currqas ###This is where the data is getting corrupted\n",
    "        #print(\"qaslist after updating question in currqasdict = {}\".format(qaslist))\n",
    "        currqasdict[\"id\"] = currid\n",
    "        currqasdict[\"answers\"] = anslist\n",
    "        currqasdict[\"is_impossible\"] = False\n",
    "        \n",
    "        #print(\"currqasdict = {}\".format(currqasdict))\n",
    "        \n",
    "        #print(\"qaslist right before processing = {}\".format(qaslist))\n",
    "        \n",
    "        qaslist.append(currqasdict)\n",
    "        \n",
    "        #print(\"qaslist after processing = {}\".format(qaslist))\n",
    "        \n",
    "        #print(\"qaslist = {}\".format(qaslist))\n",
    "        #print(\"-----------\")\n",
    "        \n",
    "        #paralist.append(qaslist)\n",
    "        paralist.append(currqasdict)\n",
    "        \n",
    "        currdict[\"paragraphs\"] = [{\"qas\" : paralist, \"context\" : currcont}]\n",
    "        \n",
    "        #print(\"currdict after intermediate round = {}\".format(currdict))\n",
    "\n",
    "        \n",
    "    #currparagraphs[\"qas\"] = qaslist\n",
    "    #currparagraphs['context'] = currcont\n",
    "    \n",
    "    #print(\"currparagraphs = {}\".format(currparagraphs))\n",
    "    #print(\"&&&&&&&&&&\")\n",
    "    \n",
    "    #paralist.append(currparagraphs)\n",
    "        \n",
    "    #currdict[\"paragraphs\"] = paralist\n",
    "    \n",
    "if y > 0:\n",
    "    datalist.append(currdict)\n",
    "    #print(\"current datalist {}\".format(datalist))\n",
    "\n",
    "cdcdict['data'] = datalist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "2315e390-1889-41fc-98a8-e36911695e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cdcdict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "cbd947e1-3b1d-417a-85b5-94fc72c8bb60",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'f059e215ee14a89be75d577ec5ad4eb2',\n",
       "  'paragraphs': {'qas': [{'question': 'What should you do if you have a fever?',\n",
       "     'id': '100000000000000000000003',\n",
       "     'answers': [{'text': 'Drink a lot of water', 'answer_start': 375}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is the name of the washcloth that you apply to the area of the shot?',\n",
       "     'id': '100000000000000000000005',\n",
       "     'answers': [{'text': 'clean, cool, wet', 'answer_start': 281}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is the name of the person who can give you medicine?',\n",
       "     'id': '100000000000000000000007',\n",
       "     'answers': [{'text': 'healthcare provider', 'answer_start': 477}],\n",
       "     'is_impossible': False}],\n",
       "   'context': 'cdc.gov/coronavirus\\nWhat to Expect after Getting a COVID-19 Vaccine\\nThe COVID-19 shot may cause side effects in some people. Side effects should go away in a few days. On the arm where you got the shot:\\n• Pain\\n• Redness\\n• Swelling\\nIf you are sore where you got the shot:\\n• Apply a clean, cool, wet washcloth over the area\\n• Use or move your arm gently\\nIf you have a fever:\\n• Drink a lot of water\\n• Get plenty of rest\\n• Dress lightly\\nIf you have pain, headache, or fever, ask a healthcare provider (or facility\\nstaff) if you can have medicine.'}},\n",
       " {'title': 'd7bba6b692a38859debc1bf7955686d5',\n",
       "  'paragraphs': {'qas': [{'question': 'What should you wear when you are in a correctional facility?',\n",
       "     'id': '100000000000000000000008',\n",
       "     'answers': [{'text': 'a well-fitting mask', 'answer_start': 114}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'How far away from others should you stay from others?',\n",
       "     'id': '100000000000000000000009',\n",
       "     'answers': [{'text': '6 feet', 'answer_start': 156}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What should go away in a few days?',\n",
       "     'id': '100000000000000000000011',\n",
       "     'answers': [{'text': 'Side effects', 'answer_start': 228}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'How long does it take for your body to build protection after a vaccination?',\n",
       "     'id': '100000000000000000000014',\n",
       "     'answers': [{'text': 'It takes time', 'answer_start': 542}],\n",
       "     'is_impossible': False}],\n",
       "   'context': 'Even after your COVID-19 vaccination, when you are in a correctional facility, it’s important\\nto continue wearing a well-fitting mask, try to stay at least 6 feet away from others as\\nmuch as possible, and wash your hands often. Side effects may make\\nyou feel a little sick or\\neven make it hard to\\ndo daily activities, but\\nthey should go away\\nin a few days. vaccination rates and\\nensure that staff and\\nresidents stay up to date\\non their COVID-19 vaccines. COVID-19 vaccines may\\nnot fully protect you\\nuntil a week or two after\\nyour final shot. It takes\\ntime for your body to\\nbuild protection after\\nany vaccination.'}},\n",
       " {'title': 'a2ebbe91c5fbc2888446cd5e8e0e63c8',\n",
       "  'paragraphs': {'qas': [{'question': 'How long does it take for the redness or pain to get better?',\n",
       "     'id': '100000000000000000000016',\n",
       "     'answers': [{'text': '24 hours', 'answer_start': 132}],\n",
       "     'is_impossible': False}],\n",
       "   'context': 'Ask the facility healthcare provider (or facility staff) for help if:\\n• The redness or pain where you got the shot gets worse after 24 hours\\n• Your side effects are worrying you\\n• Your side effects do not seem to be going away after a few days\\nIn the rest of your body:\\n• Fever\\n• Chills\\n• Tiredness\\n• Headache\\n• Muscle pain\\n• Nausea\\nCOMMON SIDE EFFECTS\\nHELPFUL TIPS\\nREMEMBER'}},\n",
       " {'title': '81fd262a74141ee213c771449bcfd8a4',\n",
       "  'paragraphs': {'qas': [{'question': 'Who recommends COVID-19 vaccines for everyone 6 months and older?',\n",
       "     'id': '100000000000000000000020',\n",
       "     'answers': [{'text': 'CDC', 'answer_start': 140}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is a quick, easy, and free way to get a vaccine?',\n",
       "     'id': '100000000000000000000023',\n",
       "     'answers': [{'text': 'Getting a COVID-19 vaccine is fast, easy, and free. Learn how to find a COVID-19 vaccine near you, and get your vaccine today',\n",
       "       'answer_start': 531}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is the cost of getting a COVID-19 vaccine near you?',\n",
       "     'id': '100000000000000000000024',\n",
       "     'answers': [{'text': 'free', 'answer_start': 577}],\n",
       "     'is_impossible': False}],\n",
       "   'context': '10/30/22, 9:34 AM About COVID-19 Vaccines | CDC\\nEspañol | Other Languages\\nAbout COVID-19 Vaccines\\nCOVID-19 vaccines are safe and effective. CDC recommends COVID-19 vaccines for everyone 6 months and older and boosters\\nfor everyone 5 years and older, if eligible. Learn about the different vaccines available. To find a COVID-19 vaccine near you: Search vaccines.gov, text your zip\\ncode to 438829, or call 1-800-232-0233 to find COVID-19 vaccine locations near you. Benefits of Getting Vaccinated\\nFrequently Asked Questions\\n•\\n•\\n•\\n•\\nGetting a COVID-19 vaccine is fast, easy, and free. Learn how to find a COVID-19 vaccine near you, and get your vaccine\\ntoday!'}},\n",
       " {'title': 'da98716b643874423d2b1e104481ee65',\n",
       "  'paragraphs': {'qas': [{'question': 'How do Vaccines Get to You?',\n",
       "     'id': '100000000000000000000025',\n",
       "     'answers': [{'text': 'Reporting Vaccine Data', 'answer_start': 29}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'When was the last update for COVID-19 vaccines?',\n",
       "     'id': '100000000000000000000026',\n",
       "     'answers': [{'text': 'July 13, 2022', 'answer_start': 239}],\n",
       "     'is_impossible': False}],\n",
       "   'context': '\\uf294 Vaccine Data\\nVaccine Data\\n›\\nReporting Vaccine Data\\n›\\n\\uf3c2 How Vaccines Get to You\\nDeveloping COVID-19 vaccines\\n›\\n\\uf2ff For Healthcare and Public Health\\nUse of COVID-19 Vaccines in the United States: Interim Clinical Considerations\\nLast Updated July 13, 2022'}},\n",
       " {'title': '5ae09f6b7c89da2d1b5973d4cf686c0c',\n",
       "  'paragraphs': {'qas': [{'question': 'What is the minimum age to receive boosters?',\n",
       "     'id': '100000000000000000000030',\n",
       "     'answers': [{'text': '5 years and older', 'answer_start': 306}],\n",
       "     'is_impossible': False}],\n",
       "   'context': 'Español | Other Languages\\nGet the facts about COVID-19 and the virus that causes it, including how it spreads and new variants. CDC is reviewing this page to align with updated guidance. Protect Your Family Members\\nCDC recommends COVID-19 vaccines for everyone 6 months and older and boosters for\\neveryone 5 years and older, if eligible.'}},\n",
       " {'title': '948373cea621f6816d541c171e687f1b',\n",
       "  'paragraphs': {'qas': [{'question': 'How does it spread Variants of the Virus Stress & Coping Animals?',\n",
       "     'id': '100000000000000000000033',\n",
       "     'answers': [{'text': '\\uf1aa \\uf1aa \\uf1aa \\uf1aa', 'answer_start': 345}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is COVID-19?',\n",
       "     'id': '100000000000000000000034',\n",
       "     'answers': [{'text': 'Anyone not up to date with their COVID-19 vaccines',\n",
       "       'answer_start': 103}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is the name of the virus that causes stress in animals?',\n",
       "     'id': '100000000000000000000036',\n",
       "     'answers': [{'text': 'COVID-19', 'answer_start': 83}],\n",
       "     'is_impossible': False}],\n",
       "   'context': 'Some people in your family may need to take more\\nsteps to be better protected from COVID-19, including\\nAnyone not up to date with their COVID-19 vaccines\\nPeople with weakened immune systems or underlying medical conditions\\n•\\n•\\nProtect Yourself & Others\\nHow It Spreads\\nVariants of the Virus\\nStress & Coping\\nAnimals & COVID-19\\nBasics of COVID-19\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\nLast Updated Jan. 24, 2022'}},\n",
       " {'title': 'cb7973e637cb213634c54a9aa03f5427',\n",
       "  'paragraphs': {'qas': [{'question': 'What can happen if you get a COVID-19 vaccine and you think you might be having a severe allergic reaction to a vaccine?',\n",
       "     'id': '100000000000000000000037',\n",
       "     'answers': [{'text': 'seek immediate medical care by calling 911',\n",
       "       'answer_start': 408}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What is rare but can happen?',\n",
       "     'id': '100000000000000000000038',\n",
       "     'answers': [{'text': 'Severe allergic reactions to vaccines',\n",
       "       'answer_start': 209}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'What should you do if you think you might be allergic to the COVID-19 vaccine?',\n",
       "     'id': '100000000000000000000039',\n",
       "     'answers': [{'text': 'seek immediate medical care by calling 911',\n",
       "       'answer_start': 408}],\n",
       "     'is_impossible': False}],\n",
       "   'context': '10/30/22, 9:31 AM Allergic Reactions after COVID-19 Vaccination | CDC\\nEspañol | Other Languages\\nAllergic Reactions after COVID-19 Vaccination\\nIf You Are Having a Severe Allergic Reaction to a COVID-19\\nVaccine\\nSevere allergic reactions to vaccines are rare but can happen. If you get a COVID-19 vaccine and you think you might be\\nhaving a severe allergic reaction after leaving the vaccination provider site, seek immediate medical care by calling 911. A severe allergic reaction can cause:\\ndifficulty breathing or wheezing,\\na drop in blood pressure,\\nswelling of the tongue or throat, or\\na generalized rash or hives, which may include mucus membranes.'}}]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdcdict['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6b588bf8-5a6b-4cc4-890a-f922d9b3a9d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'version': 'v2.0',\n",
       " 'data': [{'title': 'f059e215ee14a89be75d577ec5ad4eb2',\n",
       "   'paragraphs': {'qas': [{'question': 'What should you do if you have a fever?',\n",
       "      'id': '100000000000000000000003',\n",
       "      'answers': [{'text': 'Drink a lot of water', 'answer_start': 375}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is the name of the washcloth that you apply to the area of the shot?',\n",
       "      'id': '100000000000000000000005',\n",
       "      'answers': [{'text': 'clean, cool, wet', 'answer_start': 281}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is the name of the person who can give you medicine?',\n",
       "      'id': '100000000000000000000007',\n",
       "      'answers': [{'text': 'healthcare provider', 'answer_start': 477}],\n",
       "      'is_impossible': False}],\n",
       "    'context': 'cdc.gov/coronavirus\\nWhat to Expect after Getting a COVID-19 Vaccine\\nThe COVID-19 shot may cause side effects in some people. Side effects should go away in a few days. On the arm where you got the shot:\\n• Pain\\n• Redness\\n• Swelling\\nIf you are sore where you got the shot:\\n• Apply a clean, cool, wet washcloth over the area\\n• Use or move your arm gently\\nIf you have a fever:\\n• Drink a lot of water\\n• Get plenty of rest\\n• Dress lightly\\nIf you have pain, headache, or fever, ask a healthcare provider (or facility\\nstaff) if you can have medicine.'}},\n",
       "  {'title': 'd7bba6b692a38859debc1bf7955686d5',\n",
       "   'paragraphs': {'qas': [{'question': 'What should you wear when you are in a correctional facility?',\n",
       "      'id': '100000000000000000000008',\n",
       "      'answers': [{'text': 'a well-fitting mask', 'answer_start': 114}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'How far away from others should you stay from others?',\n",
       "      'id': '100000000000000000000009',\n",
       "      'answers': [{'text': '6 feet', 'answer_start': 156}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What should go away in a few days?',\n",
       "      'id': '100000000000000000000011',\n",
       "      'answers': [{'text': 'Side effects', 'answer_start': 228}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'How long does it take for your body to build protection after a vaccination?',\n",
       "      'id': '100000000000000000000014',\n",
       "      'answers': [{'text': 'It takes time', 'answer_start': 542}],\n",
       "      'is_impossible': False}],\n",
       "    'context': 'Even after your COVID-19 vaccination, when you are in a correctional facility, it’s important\\nto continue wearing a well-fitting mask, try to stay at least 6 feet away from others as\\nmuch as possible, and wash your hands often. Side effects may make\\nyou feel a little sick or\\neven make it hard to\\ndo daily activities, but\\nthey should go away\\nin a few days. vaccination rates and\\nensure that staff and\\nresidents stay up to date\\non their COVID-19 vaccines. COVID-19 vaccines may\\nnot fully protect you\\nuntil a week or two after\\nyour final shot. It takes\\ntime for your body to\\nbuild protection after\\nany vaccination.'}},\n",
       "  {'title': 'a2ebbe91c5fbc2888446cd5e8e0e63c8',\n",
       "   'paragraphs': {'qas': [{'question': 'How long does it take for the redness or pain to get better?',\n",
       "      'id': '100000000000000000000016',\n",
       "      'answers': [{'text': '24 hours', 'answer_start': 132}],\n",
       "      'is_impossible': False}],\n",
       "    'context': 'Ask the facility healthcare provider (or facility staff) for help if:\\n• The redness or pain where you got the shot gets worse after 24 hours\\n• Your side effects are worrying you\\n• Your side effects do not seem to be going away after a few days\\nIn the rest of your body:\\n• Fever\\n• Chills\\n• Tiredness\\n• Headache\\n• Muscle pain\\n• Nausea\\nCOMMON SIDE EFFECTS\\nHELPFUL TIPS\\nREMEMBER'}},\n",
       "  {'title': '81fd262a74141ee213c771449bcfd8a4',\n",
       "   'paragraphs': {'qas': [{'question': 'Who recommends COVID-19 vaccines for everyone 6 months and older?',\n",
       "      'id': '100000000000000000000020',\n",
       "      'answers': [{'text': 'CDC', 'answer_start': 140}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is a quick, easy, and free way to get a vaccine?',\n",
       "      'id': '100000000000000000000023',\n",
       "      'answers': [{'text': 'Getting a COVID-19 vaccine is fast, easy, and free. Learn how to find a COVID-19 vaccine near you, and get your vaccine today',\n",
       "        'answer_start': 531}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is the cost of getting a COVID-19 vaccine near you?',\n",
       "      'id': '100000000000000000000024',\n",
       "      'answers': [{'text': 'free', 'answer_start': 577}],\n",
       "      'is_impossible': False}],\n",
       "    'context': '10/30/22, 9:34 AM About COVID-19 Vaccines | CDC\\nEspañol | Other Languages\\nAbout COVID-19 Vaccines\\nCOVID-19 vaccines are safe and effective. CDC recommends COVID-19 vaccines for everyone 6 months and older and boosters\\nfor everyone 5 years and older, if eligible. Learn about the different vaccines available. To find a COVID-19 vaccine near you: Search vaccines.gov, text your zip\\ncode to 438829, or call 1-800-232-0233 to find COVID-19 vaccine locations near you. Benefits of Getting Vaccinated\\nFrequently Asked Questions\\n•\\n•\\n•\\n•\\nGetting a COVID-19 vaccine is fast, easy, and free. Learn how to find a COVID-19 vaccine near you, and get your vaccine\\ntoday!'}},\n",
       "  {'title': 'da98716b643874423d2b1e104481ee65',\n",
       "   'paragraphs': {'qas': [{'question': 'How do Vaccines Get to You?',\n",
       "      'id': '100000000000000000000025',\n",
       "      'answers': [{'text': 'Reporting Vaccine Data', 'answer_start': 29}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'When was the last update for COVID-19 vaccines?',\n",
       "      'id': '100000000000000000000026',\n",
       "      'answers': [{'text': 'July 13, 2022', 'answer_start': 239}],\n",
       "      'is_impossible': False}],\n",
       "    'context': '\\uf294 Vaccine Data\\nVaccine Data\\n›\\nReporting Vaccine Data\\n›\\n\\uf3c2 How Vaccines Get to You\\nDeveloping COVID-19 vaccines\\n›\\n\\uf2ff For Healthcare and Public Health\\nUse of COVID-19 Vaccines in the United States: Interim Clinical Considerations\\nLast Updated July 13, 2022'}},\n",
       "  {'title': '5ae09f6b7c89da2d1b5973d4cf686c0c',\n",
       "   'paragraphs': {'qas': [{'question': 'What is the minimum age to receive boosters?',\n",
       "      'id': '100000000000000000000030',\n",
       "      'answers': [{'text': '5 years and older', 'answer_start': 306}],\n",
       "      'is_impossible': False}],\n",
       "    'context': 'Español | Other Languages\\nGet the facts about COVID-19 and the virus that causes it, including how it spreads and new variants. CDC is reviewing this page to align with updated guidance. Protect Your Family Members\\nCDC recommends COVID-19 vaccines for everyone 6 months and older and boosters for\\neveryone 5 years and older, if eligible.'}},\n",
       "  {'title': '948373cea621f6816d541c171e687f1b',\n",
       "   'paragraphs': {'qas': [{'question': 'How does it spread Variants of the Virus Stress & Coping Animals?',\n",
       "      'id': '100000000000000000000033',\n",
       "      'answers': [{'text': '\\uf1aa \\uf1aa \\uf1aa \\uf1aa',\n",
       "        'answer_start': 345}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is COVID-19?',\n",
       "      'id': '100000000000000000000034',\n",
       "      'answers': [{'text': 'Anyone not up to date with their COVID-19 vaccines',\n",
       "        'answer_start': 103}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is the name of the virus that causes stress in animals?',\n",
       "      'id': '100000000000000000000036',\n",
       "      'answers': [{'text': 'COVID-19', 'answer_start': 83}],\n",
       "      'is_impossible': False}],\n",
       "    'context': 'Some people in your family may need to take more\\nsteps to be better protected from COVID-19, including\\nAnyone not up to date with their COVID-19 vaccines\\nPeople with weakened immune systems or underlying medical conditions\\n•\\n•\\nProtect Yourself & Others\\nHow It Spreads\\nVariants of the Virus\\nStress & Coping\\nAnimals & COVID-19\\nBasics of COVID-19\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\n\\uf1aa\\nLast Updated Jan. 24, 2022'}},\n",
       "  {'title': 'cb7973e637cb213634c54a9aa03f5427',\n",
       "   'paragraphs': {'qas': [{'question': 'What can happen if you get a COVID-19 vaccine and you think you might be having a severe allergic reaction to a vaccine?',\n",
       "      'id': '100000000000000000000037',\n",
       "      'answers': [{'text': 'seek immediate medical care by calling 911',\n",
       "        'answer_start': 408}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What is rare but can happen?',\n",
       "      'id': '100000000000000000000038',\n",
       "      'answers': [{'text': 'Severe allergic reactions to vaccines',\n",
       "        'answer_start': 209}],\n",
       "      'is_impossible': False},\n",
       "     {'question': 'What should you do if you think you might be allergic to the COVID-19 vaccine?',\n",
       "      'id': '100000000000000000000039',\n",
       "      'answers': [{'text': 'seek immediate medical care by calling 911',\n",
       "        'answer_start': 408}],\n",
       "      'is_impossible': False}],\n",
       "    'context': '10/30/22, 9:31 AM Allergic Reactions after COVID-19 Vaccination | CDC\\nEspañol | Other Languages\\nAllergic Reactions after COVID-19 Vaccination\\nIf You Are Having a Severe Allergic Reaction to a COVID-19\\nVaccine\\nSevere allergic reactions to vaccines are rare but can happen. If you get a COVID-19 vaccine and you think you might be\\nhaving a severe allergic reaction after leaving the vaccination provider site, seek immediate medical care by calling 911. A severe allergic reaction can cause:\\ndifficulty breathing or wheezing,\\na drop in blood pressure,\\nswelling of the tongue or throat, or\\na generalized rash or hives, which may include mucus membranes.'}}]}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdcdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "8f101690-04dd-4a84-a994-80de70e4ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"first20train.json\", \"w\") as outfile:\n",
    "    json.dump(cdcdict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f4407-ebc0-437a-bb3c-faa16741e156",
   "metadata": {},
   "source": [
    "## Success! Our format now matches the SQuAD data exactly. Or so it seems..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5260b-69e3-47a3-bbe0-d70cc0ac776a",
   "metadata": {},
   "source": [
    "## Now to create the real files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4a6dc-6271-48b9-b01f-333b45517178",
   "metadata": {},
   "source": [
    "### First the train files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "07743d4d-9eb4-45b8-aaba-98573936dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = train.index.values\n",
    "values.sort()\n",
    "prevtitle = 'nofirsttitle'\n",
    "cdcdict_train = {\"version\": \"v2.0\"}\n",
    "datalist = []\n",
    "\n",
    "for x in values:\n",
    "    currtitle = train._get_value(x, 'title')\n",
    "    currcont = train._get_value(x, 'context')\n",
    "    currid = train._get_value(x, 'id')\n",
    "    currqas = train._get_value(x, 'question')\n",
    "    currans = train._get_value(x, 'answer_text')\n",
    "    currstart = train._get_value(x, 'answer_start')\n",
    "    currstart = currstart.item()  #This converts to Python Int, which avoids JSON writing problems later\n",
    "\n",
    "    if currtitle != prevtitle:\n",
    "        \n",
    "        if prevtitle != 'nofirsttitle':\n",
    "            datalist.append(currdict)\n",
    "        \n",
    "        y = 0 #Counter to make sure we write the last value at the end of the program\n",
    "\n",
    "        qaslist = []\n",
    "        anslist = []\n",
    "        currdict = {}\n",
    "        paralist = []\n",
    "        \n",
    "        currdict[\"title\"] = currtitle\n",
    "                \n",
    "        currparagraphs = {}\n",
    "                \n",
    "        currqasdict = {}\n",
    "                \n",
    "        curransdict = {}\n",
    "\n",
    "        curransdict[\"text\"] = currans\n",
    "        curransdict[\"answer_start\"] = currstart\n",
    "        \n",
    "        anslist.append(curransdict)\n",
    "        \n",
    "        currqasdict[\"question\"] = currqas\n",
    "        currqasdict[\"id\"] = currid\n",
    "        currqasdict[\"answers\"] = anslist\n",
    "        currqasdict[\"is_impossible\"] = False\n",
    "        \n",
    "        qaslist.append(currqasdict)\n",
    "        \n",
    "        paralist.append(currqasdict)\n",
    "        \n",
    "        currdict[\"paragraphs\"] = [{\"qas\" : paralist, \"context\" : currcont}]\n",
    "                \n",
    "        prevtitle = currtitle\n",
    "        y += 1\n",
    "        \n",
    "    else:\n",
    "        y += 1\n",
    "        anslist = []\n",
    "                        \n",
    "        curransdict = {}\n",
    "        currqasdict = {}\n",
    "        \n",
    "        curransdict[\"text\"] = currans\n",
    "        curransdict[\"answer_start\"] = currstart\n",
    "                \n",
    "        anslist.append(curransdict)\n",
    "        \n",
    "        currqasdict[\"question\"] = currqas \n",
    "        currqasdict[\"id\"] = currid\n",
    "        currqasdict[\"answers\"] = anslist\n",
    "        currqasdict[\"is_impossible\"] = False\n",
    "        \n",
    "        qaslist.append(currqasdict)\n",
    "        \n",
    "        paralist.append(currqasdict)\n",
    "        \n",
    "        currdict[\"paragraphs\"] = [{\"qas\" : paralist, \"context\" : currcont}]\n",
    "            \n",
    "if y > 0:\n",
    "    datalist.append(currdict)\n",
    "\n",
    "cdcdict_train['data'] = datalist\n",
    "\n",
    "import json\n",
    "with open(\"final_qa_pairs_train.json\", \"w\") as outfile:\n",
    "    json.dump(cdcdict_train, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb143fb7-b3f8-4343-9f74-1843f659b027",
   "metadata": {},
   "source": [
    "### Now the dev files - same treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "d4c73663-0cd4-49ed-a30d-e4f4debf6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = test.index.values\n",
    "values.sort()\n",
    "prevtitle = 'nofirsttitle'\n",
    "cdcdict_test = {\"version\": \"v2.0\"}\n",
    "datalist = []\n",
    "\n",
    "for x in values:\n",
    "    currtitle = test._get_value(x, 'title')\n",
    "    currcont = test._get_value(x, 'context')\n",
    "    currid = test._get_value(x, 'id')\n",
    "    currqas = test._get_value(x, 'question')\n",
    "    currans = test._get_value(x, 'answer_text')\n",
    "    currstart = test._get_value(x, 'answer_start')\n",
    "    currstart = currstart.item()  #This converts to Python Int, which avoids JSON writing problems later\n",
    "\n",
    "    if currtitle != prevtitle:\n",
    "        \n",
    "        if prevtitle != 'nofirsttitle':\n",
    "            datalist.append(currdict)\n",
    "        \n",
    "        y = 0 #Counter to make sure we write the last value at the end of the program\n",
    "\n",
    "        qaslist = []\n",
    "        anslist = []\n",
    "        currdict = {}\n",
    "        paralist = []\n",
    "        \n",
    "        currdict[\"title\"] = currtitle\n",
    "                \n",
    "        currparagraphs = {}\n",
    "                \n",
    "        currqasdict = {}\n",
    "                \n",
    "        curransdict = {}\n",
    "\n",
    "        curransdict[\"text\"] = currans\n",
    "        curransdict[\"answer_start\"] = currstart\n",
    "        \n",
    "        anslist.append(curransdict)\n",
    "        \n",
    "        currqasdict[\"question\"] = currqas\n",
    "        currqasdict[\"id\"] = currid\n",
    "        currqasdict[\"answers\"] = anslist\n",
    "        currqasdict[\"is_impossible\"] = False\n",
    "        \n",
    "        qaslist.append(currqasdict)\n",
    "        \n",
    "        paralist.append(currqasdict)\n",
    "        \n",
    "        currdict[\"paragraphs\"] = [{\"qas\" : paralist, \"context\" : currcont}]\n",
    "                \n",
    "        prevtitle = currtitle\n",
    "        y += 1\n",
    "        \n",
    "    else:\n",
    "        y += 1\n",
    "        anslist = []\n",
    "                        \n",
    "        curransdict = {}\n",
    "        currqasdict = {}\n",
    "        \n",
    "        curransdict[\"text\"] = currans\n",
    "        curransdict[\"answer_start\"] = currstart\n",
    "                \n",
    "        anslist.append(curransdict)\n",
    "        \n",
    "        currqasdict[\"question\"] = currqas \n",
    "        currqasdict[\"id\"] = currid\n",
    "        currqasdict[\"answers\"] = anslist\n",
    "        currqasdict[\"is_impossible\"] = False\n",
    "        \n",
    "        qaslist.append(currqasdict)\n",
    "        \n",
    "        paralist.append(currqasdict)\n",
    "        \n",
    "        currdict[\"paragraphs\"] = [{\"qas\" : paralist, \"context\" : currcont}]\n",
    "            \n",
    "if y > 0:\n",
    "    datalist.append(currdict)\n",
    "\n",
    "cdcdict_test['data'] = datalist\n",
    "\n",
    "import json\n",
    "with open(\"final_qa_pairs_dev.json\", \"w\") as outfile:\n",
    "    json.dump(cdcdict_test, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e67332-1d56-40e7-98b7-18f037125c92",
   "metadata": {},
   "source": [
    "## Have modified cdc_test2.py to point to these new files, going to attempt to load without further modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "61ae3f29-d3a7-480f-b977-60db7c882fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "25d61ead-5c59-49c5-bb33-1c94a0d5e7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cdc_test2/plain_text to /home/jupyter/.cache/huggingface/datasets/cdc_test2/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 6904.20it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 1275.25it/s]\n",
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cdc_test2 downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/cdc_test2/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"cdc_test2.py\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621a8ed-8271-4e62-8940-c2a2d6225691",
   "metadata": {},
   "source": [
    "# SUCCESS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "315c2ed9-37ef-4269-a56f-b48572577784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 7109\n",
       "})"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "b07b9c79-1fe3-4b05-95bf-59cb96e0a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC_Covid19_Data_2022_10_30.zip\n"
     ]
    }
   ],
   "source": [
    "ls gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "f9cfe73d-1493-43e8-8d47-838de87ccba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "986483b8-c7c1-4645-8f43-e7ee549409d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir gcs/cdcmodel_train01 #Storing our checkpoints on cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "13005ba3-e1e0-4ea6-9379-7a29e3dc72c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/QA_tune\n"
     ]
    }
   ],
   "source": [
    "cd QA_tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2d0090-17e9-404a-98c0-77e9bc1a558a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 112247, done.\u001b[K\n",
      "remote: Total 112247 (delta 0), reused 0 (delta 0), pack-reused 112247\u001b[K\n",
      "Receiving objects: 100% (112247/112247), 105.67 MiB | 32.73 MiB/s, done.\n",
      "Resolving deltas: 100% (83385/83385), done.\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://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@github.com/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "9c7c28b9-42ef-4eb0-bf82-fba682f5a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering\n"
     ]
    }
   ],
   "source": [
    "cd transformers/examples/pytorch/question-answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "6996a554-d1e7-40d8-9016-9c52b3a6dee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering'"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2080c722-e7c7-4414-b296-528ca24de833",
   "metadata": {},
   "source": [
    "## Note: had to move loading script and data files to this folder. When specifying \"dataset\" as the thing to run, the QA training script will look for a subfolder named dataset and a script named dataset.py, so I renamed cdc_test2.py accordingly and made a copy of all of hte data in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "119ab00d-27e0-43b4-9efd-d154fd1e0ad4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.6.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.11.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "226be59f-6652-41a7-9c17-c404ed2d8bc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.28.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.11.4)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from evaluate) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from evaluate) (4.64.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.70.13)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2022.8.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from evaluate) (2.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->evaluate) (3.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->evaluate) (2022.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "8c31d46e-8cdb-419d-a621-ed3af6dcd77b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md         \u001b[0m\u001b[01;32mrun_qa_beam_search.py\u001b[0m*            trainer_qa.py\n",
      "\u001b[01;34m__pycache__\u001b[0m/      run_qa_beam_search_no_trainer.py  trainer_seq2seq_qa.py\n",
      "requirements.txt  \u001b[01;32mrun_qa_no_trainer.py\u001b[0m*             utils_qa.py\n",
      "\u001b[01;32mrun_qa.py\u001b[0m*        run_seq2seq_qa.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "4441a701-a50c-4204-906c-8f86b7bc78d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#!python run_qa.py \\\\   \\n  --model_name_or_path bigscience/bloom-560m   --dataset_name squad_v2   --do_train   --per_device_train_batch_size 12 \\\\  #Batch size 12 = out of memory issues\\n  --learning_rate 3e-5   --num_train_epochs 2   --max_seq_length 384   --doc_stride 128   --output_dir /tmp/debug_bloom_squad/   --eval_accumulation_steps 1   --version_2_with_negative   --overwrite_output_dir   --fp16'"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#!python run_qa.py \\   \n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name squad_v2 \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size 12 \\  #Batch size 12 = out of memory issues\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /tmp/debug_bloom_squad/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de27e48-ac6a-495c-af7f-0b5f362a7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f6d82a3-ef96-47fd-bc11-c96b889c899d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.maxsize #setting token value for no-id as maxsize -1 in run_qa.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "f9549b4b-dbda-4c5c-bb79-96488d347053",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/13/2022 21:50:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/13/2022 21:50:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train01/runs/Nov13_21-50-47_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train01/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train01/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/13/2022 21:50:48 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/13/2022 21:50:48 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/13/2022 21:50:48 - INFO - datasets.builder - Generating dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "Downloading and preparing dataset dataset/plain_text to /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb...\n",
      "11/13/2022 21:50:48 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files: 100%|██████████████████| 2/2 [00:00<00:00, 10965.50it/s]\n",
      "11/13/2022 21:50:48 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "11/13/2022 21:50:48 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1916.96it/s]\n",
      "11/13/2022 21:50:48 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
      "11/13/2022 21:50:48 - INFO - datasets.builder - Generating train split\n",
      "11/13/2022 21:50:48 - INFO - datasets.builder - Generating validation split\n",
      "11/13/2022 21:50:49 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset dataset downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 829.98it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-13 21:50:49,271 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-13 21:50:49,343 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-13 21:50:49,529 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-13 21:50:49,529 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-13 21:50:49,529 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-13 21:50:49,529 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-13 21:50:50,546 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-13 21:51:04,752 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-13 21:51:04,752 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                 | 0/8 [00:00<?, ?ba/s]11/13/2022 21:51:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "Running tokenizer on train dataset:  88%|███████▉ | 7/8 [00:02<00:00,  2.49ba/s]\n",
      "[INFO|trainer.py:557] 2022-11-13 21:51:26,373 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-13 21:51:26,482 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-13 21:51:26,482 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-13 21:51:26,482 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1610] 2022-11-13 21:51:26,482 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-13 21:51:26,482 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-13 21:51:26,482 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-13 21:51:26,482 >>   Total optimization steps = 2512\n",
      "  0%|                                                  | 0/2512 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.5795, 'learning_rate': 2.4148089171974522e-05, 'epoch': 0.4}         \n",
      " 20%|███████▉                                | 500/2512 [02:12<08:41,  3.86it/s][INFO|trainer.py:2671] 2022-11-13 21:53:39,614 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train01/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-13 21:53:39,960 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-13 21:54:03,930 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-13 21:54:04,394 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-13 21:54:04,818 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.8146, 'learning_rate': 1.8176751592356687e-05, 'epoch': 0.8}         \n",
      " 40%|███████████████▌                       | 1000/2512 [05:49<06:32,  3.85it/s][INFO|trainer.py:2671] 2022-11-13 21:57:16,063 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train01/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-13 21:57:16,418 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-13 21:57:36,199 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-13 21:57:36,549 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-13 21:57:36,925 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4918, 'learning_rate': 1.2205414012738854e-05, 'epoch': 1.19}        \n",
      " 60%|███████████████████████▎               | 1500/2512 [09:16<04:22,  3.86it/s][INFO|trainer.py:2671] 2022-11-13 22:00:43,371 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train01/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-13 22:00:43,704 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-13 22:01:08,947 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-13 22:01:09,314 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-13 22:01:09,711 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4106, 'learning_rate': 6.257961783439491e-06, 'epoch': 1.59}         \n",
      " 80%|███████████████████████████████        | 2000/2512 [12:38<02:12,  3.85it/s][INFO|trainer.py:2671] 2022-11-13 22:04:05,763 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train01/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-13 22:04:06,142 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-13 22:04:25,836 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-13 22:04:26,185 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-13 22:04:26,481 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4758, 'learning_rate': 2.8662420382165606e-07, 'epoch': 1.99}        \n",
      "100%|██████████████████████████████████████▊| 2500/2512 [16:03<00:03,  3.85it/s][INFO|trainer.py:2671] 2022-11-13 22:07:30,216 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train01/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-13 22:07:30,625 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-13 22:07:56,223 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-13 22:07:56,568 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-13 22:07:56,902 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train01/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████████| 2512/2512 [17:22<00:00,  1.41it/s][INFO|trainer.py:1852] 2022-11-13 22:08:48,610 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1042.1281, 'train_samples_per_second': 14.463, 'train_steps_per_second': 2.41, 'train_loss': 4.552647050019282, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████████| 2512/2512 [17:22<00:00,  2.41it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-13 22:08:48,640 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train01/\n",
      "[INFO|configuration_utils.py:447] 2022-11-13 22:08:48,972 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train01/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-13 22:09:11,236 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train01/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-13 22:09:11,557 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train01/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-13 22:09:11,915 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train01/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     4.5526\n",
      "  train_runtime            = 0:17:22.12\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =     14.463\n",
      "  train_steps_per_second   =       2.41\n",
      "[INFO|modelcard.py:444] 2022-11-13 22:09:14,445 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'dataset', 'type': 'dataset', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train01/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a56939-251b-4660-bbc6-29771736a8c8",
   "metadata": {},
   "source": [
    "## Successfully recreated SQuAD test on CDC data. Now to see if my 'cleaner' data allows for some additional functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b3661639-aeb9-4fd0-a1d7-5b59ccd5852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "9421f8dd-d6bd-442f-aa0c-0bfcf30f0b6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 01:54:56 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 01:54:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train02/runs/Nov14_01-54-55_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train02/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train02/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 01:54:56 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 01:54:56 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 01:54:56 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 01:54:56 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 01:54:56 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 01:54:56 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 53.40it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 01:54:56,596 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 01:54:56,615 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 01:54:56,771 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 01:54:56,771 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 01:54:56,771 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 01:54:56,771 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 01:54:57,538 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-14 01:55:11,299 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 01:55:11,299 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/14/2022 01:55:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "Running tokenizer on validation dataset:   0%|            | 0/4 [00:00<?, ?ba/s]11/14/2022 01:55:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "Running tokenizer on validation dataset:  75%|███ | 3/4 [00:01<00:00,  1.75ba/s]\n",
      "[INFO|trainer.py:557] 2022-11-14 01:55:17,102 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 01:55:17,147 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 01:55:17,147 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-14 01:55:17,147 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1610] 2022-11-14 01:55:17,147 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-14 01:55:17,147 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-14 01:55:17,147 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 01:55:17,147 >>   Total optimization steps = 2512\n",
      "  0%|                                                  | 0/2512 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.5795, 'learning_rate': 2.4148089171974522e-05, 'epoch': 0.4}         \n",
      " 20%|███████▉                                | 500/2512 [02:09<08:41,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 01:57:26,791 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train02/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 01:57:27,215 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 01:57:46,253 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 01:57:46,624 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 01:57:47,112 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.8146, 'learning_rate': 1.8176751592356687e-05, 'epoch': 0.8}         \n",
      " 40%|███████████████▌                       | 1000/2512 [05:31<06:32,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 02:00:48,337 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train02/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:00:48,725 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:01:15,369 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:01:15,748 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:01:16,299 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4918, 'learning_rate': 1.2205414012738854e-05, 'epoch': 1.19}        \n",
      " 60%|███████████████████████▎               | 1500/2512 [08:57<04:22,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 02:04:14,409 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train02/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:04:14,777 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:04:39,125 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:04:39,484 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:04:39,825 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4106, 'learning_rate': 6.257961783439491e-06, 'epoch': 1.59}         \n",
      " 80%|███████████████████████████████        | 2000/2512 [12:20<02:12,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 02:07:37,685 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train02/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:07:38,015 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:07:58,083 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:07:58,491 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:07:58,863 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4758, 'learning_rate': 2.8662420382165606e-07, 'epoch': 1.99}        \n",
      "100%|██████████████████████████████████████▊| 2500/2512 [15:38<00:03,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 02:10:56,077 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train02/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:10:56,478 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:11:17,205 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:11:17,534 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:11:17,875 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train02/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████████| 2512/2512 [16:51<00:00,  1.49it/s][INFO|trainer.py:1852] 2022-11-14 02:12:08,290 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1011.1429, 'train_samples_per_second': 14.906, 'train_steps_per_second': 2.484, 'train_loss': 4.552647050019282, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████████| 2512/2512 [16:51<00:00,  2.48it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-14 02:12:08,318 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train02/\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:12:08,696 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train02/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:12:30,460 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train02/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:12:30,825 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train02/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:12:31,135 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train02/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     4.5526\n",
      "  train_runtime            = 0:16:51.14\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =     14.906\n",
      "  train_steps_per_second   =      2.484\n",
      "11/14/2022 02:12:33 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-14 02:12:33,141 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-14 02:12:33,144 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-14 02:12:33,144 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-14 02:12:33,144 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:34<00:00, 11.36it/s]11/14/2022 02:13:10 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 34/3047 [00:00<00:08, 336.16it/s]\u001b[A\n",
      "  3%|█                                       | 82/3047 [00:00<00:07, 419.32it/s]\u001b[A\n",
      "  4%|█▋                                     | 133/3047 [00:00<00:06, 458.49it/s]\u001b[A\n",
      "  6%|██▎                                    | 183/3047 [00:00<00:06, 473.63it/s]\u001b[A\n",
      "  8%|██▉                                    | 234/3047 [00:00<00:05, 485.36it/s]\u001b[A\n",
      "  9%|███▋                                   | 285/3047 [00:00<00:05, 492.03it/s]\u001b[A\n",
      " 11%|████▎                                  | 335/3047 [00:00<00:05, 492.62it/s]\u001b[A\n",
      " 13%|████▉                                  | 385/3047 [00:00<00:05, 492.65it/s]\u001b[A\n",
      " 14%|█████▌                                 | 435/3047 [00:00<00:05, 494.19it/s]\u001b[A\n",
      " 16%|██████▏                                | 485/3047 [00:01<00:05, 445.78it/s]\u001b[A\n",
      " 17%|██████▊                                | 531/3047 [00:01<00:05, 446.56it/s]\u001b[A\n",
      " 19%|███████▍                               | 577/3047 [00:01<00:06, 400.08it/s]\u001b[A\n",
      " 20%|███████▉                               | 624/3047 [00:01<00:05, 417.23it/s]\u001b[A\n",
      " 22%|████████▌                              | 672/3047 [00:01<00:05, 433.32it/s]\u001b[A\n",
      " 24%|█████████▏                             | 717/3047 [00:01<00:06, 376.32it/s]\u001b[A\n",
      " 25%|█████████▋                             | 759/3047 [00:01<00:05, 386.65it/s]\u001b[A\n",
      " 26%|██████████▎                            | 804/3047 [00:01<00:05, 402.58it/s]\u001b[A\n",
      " 28%|██████████▉                            | 850/3047 [00:01<00:05, 417.61it/s]\u001b[A\n",
      " 29%|███████████▍                           | 895/3047 [00:02<00:05, 426.28it/s]\u001b[A\n",
      " 31%|████████████                           | 943/3047 [00:02<00:04, 441.10it/s]\u001b[A\n",
      " 33%|████████████▋                          | 992/3047 [00:02<00:04, 454.31it/s]\u001b[A\n",
      " 34%|█████████████                         | 1043/3047 [00:02<00:04, 469.12it/s]\u001b[A\n",
      " 36%|█████████████▋                        | 1095/3047 [00:02<00:04, 482.24it/s]\u001b[A\n",
      " 38%|██████████████▎                       | 1146/3047 [00:02<00:03, 487.72it/s]\u001b[A\n",
      " 39%|██████████████▉                       | 1196/3047 [00:02<00:03, 490.40it/s]\u001b[A\n",
      " 41%|███████████████▌                      | 1246/3047 [00:02<00:03, 492.51it/s]\u001b[A\n",
      " 43%|████████████████▏                     | 1297/3047 [00:02<00:03, 496.19it/s]\u001b[A\n",
      " 44%|████████████████▊                     | 1347/3047 [00:02<00:03, 496.46it/s]\u001b[A\n",
      " 46%|█████████████████▍                    | 1397/3047 [00:03<00:03, 491.52it/s]\u001b[A\n",
      " 47%|██████████████████                    | 1447/3047 [00:03<00:03, 487.43it/s]\u001b[A\n",
      " 49%|██████████████████▋                   | 1496/3047 [00:03<00:04, 321.52it/s]\u001b[A\n",
      " 51%|███████████████████▎                  | 1547/3047 [00:03<00:04, 362.39it/s]\u001b[A\n",
      " 52%|███████████████████▉                  | 1597/3047 [00:03<00:03, 393.19it/s]\u001b[A\n",
      " 54%|████████████████████▌                 | 1648/3047 [00:03<00:03, 421.04it/s]\u001b[A\n",
      " 56%|█████████████████████▏                | 1699/3047 [00:03<00:03, 444.08it/s]\u001b[A\n",
      " 57%|█████████████████████▊                | 1750/3047 [00:03<00:02, 459.86it/s]\u001b[A\n",
      " 59%|██████████████████████▍               | 1799/3047 [00:04<00:02, 467.97it/s]\u001b[A\n",
      " 61%|███████████████████████               | 1848/3047 [00:04<00:02, 469.07it/s]\u001b[A\n",
      " 62%|███████████████████████▋              | 1897/3047 [00:04<00:02, 474.77it/s]\u001b[A\n",
      " 64%|████████████████████████▎             | 1946/3047 [00:04<00:02, 462.00it/s]\u001b[A\n",
      " 66%|████████████████████████▉             | 1996/3047 [00:04<00:02, 472.21it/s]\u001b[A\n",
      " 67%|█████████████████████████▌            | 2047/3047 [00:04<00:02, 481.66it/s]\u001b[A\n",
      " 69%|██████████████████████████▏           | 2098/3047 [00:04<00:01, 489.07it/s]\u001b[A\n",
      " 70%|██████████████████████████▊           | 2148/3047 [00:04<00:01, 492.00it/s]\u001b[A\n",
      " 72%|███████████████████████████▍          | 2198/3047 [00:04<00:01, 489.25it/s]\u001b[A\n",
      " 74%|████████████████████████████          | 2248/3047 [00:04<00:01, 486.42it/s]\u001b[A\n",
      " 75%|████████████████████████████▋         | 2299/3047 [00:05<00:01, 490.70it/s]\u001b[A\n",
      " 77%|█████████████████████████████▎        | 2349/3047 [00:05<00:01, 491.86it/s]\u001b[A\n",
      " 79%|█████████████████████████████▉        | 2399/3047 [00:05<00:01, 460.11it/s]\u001b[A\n",
      " 80%|██████████████████████████████▌       | 2446/3047 [00:05<00:01, 433.55it/s]\u001b[A\n",
      " 82%|███████████████████████████████       | 2492/3047 [00:05<00:01, 439.54it/s]\u001b[A\n",
      " 83%|███████████████████████████████▋      | 2542/3047 [00:05<00:01, 454.33it/s]\u001b[A\n",
      " 85%|████████████████████████████████▎     | 2593/3047 [00:05<00:00, 468.99it/s]\u001b[A\n",
      " 87%|████████████████████████████████▉     | 2644/3047 [00:05<00:00, 478.79it/s]\u001b[A\n",
      " 88%|█████████████████████████████████▌    | 2695/3047 [00:05<00:00, 486.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████▏   | 2744/3047 [00:06<00:00, 480.96it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▊   | 2793/3047 [00:06<00:00, 477.01it/s]\u001b[A\n",
      " 93%|███████████████████████████████████▍  | 2841/3047 [00:06<00:00, 471.30it/s]\u001b[A\n",
      " 95%|████████████████████████████████████  | 2889/3047 [00:06<00:00, 459.62it/s]\u001b[A\n",
      " 96%|████████████████████████████████████▌ | 2936/3047 [00:06<00:00, 461.98it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████▏| 2983/3047 [00:06<00:00, 455.03it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 3047/3047 [00:06<00:00, 453.09it/s]\u001b[A\n",
      "11/14/2022 02:13:17 - INFO - utils_qa - Saving predictions to /home/jupyter/gcs/cdcmodel_train02/eval_predictions.json.\n",
      "11/14/2022 02:13:17 - INFO - utils_qa - Saving nbest_preds to /home/jupyter/gcs/cdcmodel_train02/eval_nbest_predictions.json.\n",
      "11/14/2022 02:13:18 - INFO - utils_qa - Saving null_odds to /home/jupyter/gcs/cdcmodel_train02/eval_null_odds.json.\n",
      "100%|█████████████████████████████████████████| 404/404 [00:45<00:00,  8.80it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                  =    2.0\n",
      "  eval_HasAns_exact      = 0.4923\n",
      "  eval_HasAns_f1         = 0.9263\n",
      "  eval_HasAns_total      =   3047\n",
      "  eval_best_exact        = 0.4923\n",
      "  eval_best_exact_thresh =    0.0\n",
      "  eval_best_f1           = 0.9263\n",
      "  eval_best_f1_thresh    =    0.0\n",
      "  eval_exact             = 0.4923\n",
      "  eval_f1                = 0.9263\n",
      "  eval_samples           =   3229\n",
      "  eval_total             =   3047\n",
      "[INFO|modelcard.py:444] 2022-11-14 02:13:20,057 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'dataset', 'type': 'dataset', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "#Attempt --do_eval\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train02/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66ac3c-8cc3-42a1-ad03-bb109a49ddcd",
   "metadata": {},
   "source": [
    "## Now let's compare this to the baseline bert-base-uncased model with the same parameters (and we may try the bert-large option as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "eb8350e2-d848-4ac2-bc3c-670311ff476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_bert_base_uncased01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b74c9e33-776d-4996-bc55-3abb9e9a43af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 02:18:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 02:18:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_bert_base_uncased01/runs/Nov14_02-18-26_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_bert_base_uncased01,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_bert_base_uncased01,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 02:18:27 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 02:18:27 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 02:18:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 02:18:27 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 02:18:27 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 02:18:27 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 30.39it/s]\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 729kB/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 02:18:27,787 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 02:18:27,792 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 31.4kB/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 02:18:28,056 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 02:18:28,057 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Downloading: 100%|███████████████████████████| 232k/232k [00:00<00:00, 2.02MB/s]\n",
      "Downloading: 100%|███████████████████████████| 466k/466k [00:00<00:00, 3.17MB/s]\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:18:29,119 >> loading file vocab.txt from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:18:29,120 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:18:29,120 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:18:29,120 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:18:29,120 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 02:18:29,120 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 02:18:29,121 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Downloading: 100%|███████████████████████████| 440M/440M [00:05<00:00, 79.4MB/s]\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 02:18:35,230 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bert-base-uncased/snapshots/5546055f03398095e385d7dc625e636cc8910bf2/pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2597] 2022-11-14 02:18:36,596 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 02:18:36,596 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                 | 0/8 [00:00<?, ?ba/s]11/14/2022 02:18:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-f1858f8e673d96e8.arrow\n",
      "Running tokenizer on train dataset:  88%|███████▉ | 7/8 [00:03<00:00,  1.87ba/s]\n",
      "Running tokenizer on validation dataset:   0%|            | 0/4 [00:00<?, ?ba/s]11/14/2022 02:18:40 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a90dbab951593548.arrow\n",
      "Running tokenizer on validation dataset:  75%|███ | 3/4 [00:01<00:00,  1.71ba/s]\n",
      "[INFO|trainer.py:557] 2022-11-14 02:18:45,663 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 02:18:45,687 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 02:18:45,687 >>   Num examples = 7488\n",
      "[INFO|trainer.py:1609] 2022-11-14 02:18:45,687 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1610] 2022-11-14 02:18:45,687 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-14 02:18:45,687 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-14 02:18:45,687 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 02:18:45,687 >>   Total optimization steps = 2496\n",
      "{'loss': 2.7848, 'learning_rate': 2.4038461538461542e-05, 'epoch': 0.4}         \n",
      " 20%|████████                                | 500/2496 [00:40<02:38, 12.60it/s][INFO|trainer.py:2671] 2022-11-14 02:19:26,298 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:19:26,690 >> Configuration saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:19:29,687 >> Model weights saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:19:30,022 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:19:30,380 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.7098, 'learning_rate': 1.8028846153846152e-05, 'epoch': 0.8}         \n",
      " 40%|███████████████▋                       | 1000/2496 [01:33<01:59, 12.52it/s][INFO|trainer.py:2671] 2022-11-14 02:20:19,113 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:20:19,479 >> Configuration saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:20:24,656 >> Model weights saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:20:24,975 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:20:25,329 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.353, 'learning_rate': 1.2019230769230771e-05, 'epoch': 1.2}          \n",
      " 60%|███████████████████████▍               | 1500/2496 [02:28<01:19, 12.48it/s][INFO|trainer.py:2671] 2022-11-14 02:21:14,460 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:21:14,774 >> Configuration saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:21:18,174 >> Model weights saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:21:18,482 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:21:18,781 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.0327, 'learning_rate': 6.0096153846153855e-06, 'epoch': 1.6}         \n",
      " 80%|███████████████████████████████▎       | 2000/2496 [03:22<00:40, 12.17it/s][INFO|trainer.py:2671] 2022-11-14 02:22:08,080 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:22:08,422 >> Configuration saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:22:12,133 >> Model weights saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:22:12,445 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:22:12,766 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/checkpoint-2000/special_tokens_map.json\n",
      "100%|██████████████████████████████████████▉| 2495/2496 [04:17<00:00, 12.37it/s][INFO|trainer.py:1852] 2022-11-14 02:23:03,074 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 257.3869, 'train_samples_per_second': 58.185, 'train_steps_per_second': 9.697, 'train_loss': 1.5600739625784068, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████████| 2496/2496 [04:17<00:00,  9.70it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-14 02:23:03,099 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_bert_base_uncased01\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:23:03,373 >> Configuration saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:23:07,149 >> Model weights saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:23:07,627 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:23:07,994 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_bert_base_uncased01/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     1.5601\n",
      "  train_runtime            = 0:04:17.38\n",
      "  train_samples            =       7488\n",
      "  train_samples_per_second =     58.185\n",
      "  train_steps_per_second   =      9.697\n",
      "11/14/2022 02:23:10 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-14 02:23:10,143 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-14 02:23:10,145 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-14 02:23:10,145 >>   Num examples = 3214\n",
      "[INFO|trainer.py:2927] 2022-11-14 02:23:10,146 >>   Batch size = 8\n",
      "100%|█████████████████████████████████████████| 402/402 [00:10<00:00, 37.35it/s]11/14/2022 02:23:24 - INFO - utils_qa - Post-processing 3047 example predictions split into 3214 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 38/3047 [00:00<00:07, 377.45it/s]\u001b[A\n",
      "  2%|▉                                       | 76/3047 [00:00<00:07, 375.97it/s]\u001b[A\n",
      "  4%|█▍                                     | 114/3047 [00:00<00:08, 342.63it/s]\u001b[A\n",
      "  5%|█▉                                     | 152/3047 [00:00<00:08, 353.38it/s]\u001b[A\n",
      "  6%|██▍                                    | 189/3047 [00:00<00:07, 357.99it/s]\u001b[A\n",
      "  7%|██▉                                    | 227/3047 [00:00<00:07, 363.63it/s]\u001b[A\n",
      "  9%|███▍                                   | 266/3047 [00:00<00:07, 369.96it/s]\u001b[A\n",
      " 10%|███▉                                   | 304/3047 [00:00<00:07, 368.90it/s]\u001b[A\n",
      " 11%|████▍                                  | 342/3047 [00:00<00:07, 370.94it/s]\u001b[A\n",
      " 12%|████▊                                  | 380/3047 [00:01<00:07, 368.98it/s]\u001b[A\n",
      " 14%|█████▎                                 | 418/3047 [00:01<00:07, 369.83it/s]\u001b[A\n",
      " 15%|█████▊                                 | 456/3047 [00:01<00:07, 349.62it/s]\u001b[A\n",
      " 16%|██████▎                                | 492/3047 [00:01<00:07, 323.18it/s]\u001b[A\n",
      " 17%|██████▋                                | 527/3047 [00:01<00:07, 329.17it/s]\u001b[A\n",
      " 18%|███████▏                               | 561/3047 [00:01<00:07, 310.99it/s]\u001b[A\n",
      " 20%|███████▋                               | 598/3047 [00:01<00:07, 325.29it/s]\u001b[A\n",
      " 21%|████████                               | 632/3047 [00:01<00:07, 329.13it/s]\u001b[A\n",
      " 22%|████████▌                              | 669/3047 [00:01<00:07, 338.58it/s]\u001b[A\n",
      " 23%|█████████                              | 704/3047 [00:02<00:07, 301.62it/s]\u001b[A\n",
      " 24%|█████████▍                             | 742/3047 [00:02<00:07, 320.32it/s]\u001b[A\n",
      " 25%|█████████▉                             | 775/3047 [00:02<00:07, 320.23it/s]\u001b[A\n",
      " 27%|██████████▍                            | 813/3047 [00:02<00:06, 335.96it/s]\u001b[A\n",
      " 28%|██████████▊                            | 848/3047 [00:02<00:06, 339.23it/s]\u001b[A\n",
      " 29%|███████████▎                           | 885/3047 [00:02<00:06, 347.18it/s]\u001b[A\n",
      " 30%|███████████▊                           | 921/3047 [00:02<00:06, 342.68it/s]\u001b[A\n",
      " 31%|████████████▎                          | 958/3047 [00:02<00:05, 348.33it/s]\u001b[A\n",
      " 33%|████████████▋                          | 995/3047 [00:02<00:05, 354.63it/s]\u001b[A\n",
      " 34%|████████████▉                         | 1033/3047 [00:02<00:05, 360.72it/s]\u001b[A\n",
      " 35%|█████████████▎                        | 1071/3047 [00:03<00:05, 366.01it/s]\u001b[A\n",
      " 36%|█████████████▊                        | 1109/3047 [00:03<00:05, 369.74it/s]\u001b[A\n",
      " 38%|██████████████▎                       | 1147/3047 [00:03<00:05, 369.18it/s]\u001b[A\n",
      " 39%|██████████████▊                       | 1184/3047 [00:03<00:05, 368.81it/s]\u001b[A\n",
      " 40%|███████████████▏                      | 1221/3047 [00:03<00:04, 367.62it/s]\u001b[A\n",
      " 41%|███████████████▋                      | 1259/3047 [00:03<00:04, 370.78it/s]\u001b[A\n",
      " 43%|████████████████▏                     | 1297/3047 [00:03<00:04, 372.70it/s]\u001b[A\n",
      " 44%|████████████████▋                     | 1335/3047 [00:03<00:04, 374.55it/s]\u001b[A\n",
      " 45%|█████████████████                     | 1373/3047 [00:03<00:04, 372.31it/s]\u001b[A\n",
      " 46%|█████████████████▌                    | 1411/3047 [00:04<00:04, 358.68it/s]\u001b[A\n",
      " 47%|██████████████████                    | 1447/3047 [00:04<00:04, 356.99it/s]\u001b[A\n",
      " 49%|██████████████████▍                   | 1483/3047 [00:04<00:07, 210.21it/s]\u001b[A\n",
      " 50%|██████████████████▉                   | 1520/3047 [00:04<00:06, 240.78it/s]\u001b[A\n",
      " 51%|███████████████████▍                  | 1556/3047 [00:04<00:05, 265.91it/s]\u001b[A\n",
      " 52%|███████████████████▊                  | 1590/3047 [00:04<00:05, 282.53it/s]\u001b[A\n",
      " 53%|████████████████████▎                 | 1626/3047 [00:04<00:04, 299.90it/s]\u001b[A\n",
      " 55%|████████████████████▊                 | 1664/3047 [00:04<00:04, 319.62it/s]\u001b[A\n",
      " 56%|█████████████████████▏                | 1702/3047 [00:05<00:04, 333.87it/s]\u001b[A\n",
      " 57%|█████████████████████▋                | 1740/3047 [00:05<00:03, 345.99it/s]\u001b[A\n",
      " 58%|██████████████████████▏               | 1778/3047 [00:05<00:03, 353.99it/s]\u001b[A\n",
      " 60%|██████████████████████▋               | 1816/3047 [00:05<00:03, 359.29it/s]\u001b[A\n",
      " 61%|███████████████████████               | 1853/3047 [00:05<00:03, 358.08it/s]\u001b[A\n",
      " 62%|███████████████████████▌              | 1891/3047 [00:05<00:03, 362.17it/s]\u001b[A\n",
      " 63%|████████████████████████              | 1928/3047 [00:05<00:03, 346.91it/s]\u001b[A\n",
      " 65%|████████████████████████▌             | 1966/3047 [00:05<00:03, 354.35it/s]\u001b[A\n",
      " 66%|████████████████████████▉             | 2003/3047 [00:05<00:02, 358.73it/s]\u001b[A\n",
      " 67%|█████████████████████████▍            | 2041/3047 [00:06<00:02, 363.92it/s]\u001b[A\n",
      " 68%|█████████████████████████▉            | 2079/3047 [00:06<00:02, 367.85it/s]\u001b[A\n",
      " 69%|██████████████████████████▍           | 2116/3047 [00:06<00:02, 367.87it/s]\u001b[A\n",
      " 71%|██████████████████████████▊           | 2153/3047 [00:06<00:02, 365.99it/s]\u001b[A\n",
      " 72%|███████████████████████████▎          | 2190/3047 [00:06<00:02, 364.48it/s]\u001b[A\n",
      " 73%|███████████████████████████▊          | 2227/3047 [00:06<00:02, 363.36it/s]\u001b[A\n",
      " 74%|████████████████████████████▏         | 2265/3047 [00:06<00:02, 365.82it/s]\u001b[A\n",
      " 76%|████████████████████████████▋         | 2302/3047 [00:06<00:02, 366.57it/s]\u001b[A\n",
      " 77%|█████████████████████████████▏        | 2339/3047 [00:06<00:01, 365.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████▋        | 2376/3047 [00:06<00:01, 359.07it/s]\u001b[A\n",
      " 79%|██████████████████████████████        | 2412/3047 [00:07<00:01, 340.45it/s]\u001b[A\n",
      " 80%|██████████████████████████████▌       | 2447/3047 [00:07<00:01, 335.58it/s]\u001b[A\n",
      " 82%|██████████████████████████████▉       | 2484/3047 [00:07<00:01, 343.91it/s]\u001b[A\n",
      " 83%|███████████████████████████████▍      | 2522/3047 [00:07<00:01, 351.66it/s]\u001b[A\n",
      " 84%|███████████████████████████████▉      | 2560/3047 [00:07<00:01, 357.53it/s]\u001b[A\n",
      " 85%|████████████████████████████████▍     | 2598/3047 [00:07<00:01, 363.65it/s]\u001b[A\n",
      " 86%|████████████████████████████████▊     | 2635/3047 [00:07<00:01, 365.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████▎    | 2672/3047 [00:07<00:01, 365.22it/s]\u001b[A\n",
      " 89%|█████████████████████████████████▊    | 2709/3047 [00:07<00:00, 364.85it/s]\u001b[A\n",
      " 90%|██████████████████████████████████▏   | 2746/3047 [00:07<00:00, 358.31it/s]\u001b[A\n",
      " 91%|██████████████████████████████████▋   | 2782/3047 [00:08<00:00, 352.07it/s]\u001b[A\n",
      " 92%|███████████████████████████████████▏  | 2818/3047 [00:08<00:00, 349.18it/s]\u001b[A\n",
      " 94%|███████████████████████████████████▌  | 2853/3047 [00:08<00:00, 349.02it/s]\u001b[A\n",
      " 95%|████████████████████████████████████  | 2890/3047 [00:08<00:00, 352.90it/s]\u001b[A\n",
      " 96%|████████████████████████████████████▌ | 2927/3047 [00:08<00:00, 356.63it/s]\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 2964/3047 [00:08<00:00, 358.58it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████▍| 3000/3047 [00:08<00:00, 358.57it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 3047/3047 [00:08<00:00, 344.86it/s]\u001b[A\n",
      "11/14/2022 02:23:32 - INFO - utils_qa - Saving predictions to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/eval_predictions.json.\n",
      "11/14/2022 02:23:33 - INFO - utils_qa - Saving nbest_preds to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/eval_nbest_predictions.json.\n",
      "11/14/2022 02:23:34 - INFO - utils_qa - Saving null_odds to /home/jupyter/gcs/cdcmodel_bert_base_uncased01/eval_null_odds.json.\n",
      "100%|█████████████████████████████████████████| 402/402 [00:24<00:00, 16.11it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                  =     2.0\n",
      "  eval_HasAns_exact      = 56.8428\n",
      "  eval_HasAns_f1         = 71.2395\n",
      "  eval_HasAns_total      =    3047\n",
      "  eval_best_exact        = 56.8428\n",
      "  eval_best_exact_thresh =     0.0\n",
      "  eval_best_f1           = 71.2395\n",
      "  eval_best_f1_thresh    =     0.0\n",
      "  eval_exact             = 56.8428\n",
      "  eval_f1                = 71.2395\n",
      "  eval_samples           =    3214\n",
      "  eval_total             =    3047\n",
      "[INFO|modelcard.py:444] 2022-11-14 02:23:35,937 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'dataset', 'type': 'dataset', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bert-base-uncased \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_bert_base_uncased01 \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab1b40-feb6-49cc-ba8b-727c9edfd5f9",
   "metadata": {},
   "source": [
    "### Compare the above with the BLOOM 560M model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "5682e1c7-f85b-4cdc-8690-5e873adcb423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bert\n",
    "#  epoch                  =     2.0\n",
    "#  eval_HasAns_exact      = 56.8428\n",
    "#  eval_HasAns_f1         = 71.2395\n",
    "#  eval_HasAns_total      =    3047\n",
    "#  eval_best_exact        = 56.8428\n",
    "#  eval_best_exact_thresh =     0.0\n",
    "#  eval_best_f1           = 71.2395\n",
    "#  eval_best_f1_thresh    =     0.0\n",
    "#  eval_exact             = 56.8428\n",
    "#  eval_f1                = 71.2395\n",
    "#  eval_samples           =    3214\n",
    "#  eval_total             =    3047\n",
    "\n",
    "#Equivalent BLOOM-560M\n",
    "#  epoch                  =    2.0\n",
    "#  eval_HasAns_exact      = 0.4923\n",
    "#  eval_HasAns_f1         = 0.9263\n",
    "#  eval_HasAns_total      =   3047\n",
    "#  eval_best_exact        = 0.4923\n",
    "#  eval_best_exact_thresh =    0.0\n",
    "#  eval_best_f1           = 0.9263\n",
    "#  eval_best_f1_thresh    =    0.0\n",
    "#  eval_exact             = 0.4923\n",
    "#  eval_f1                = 0.9263\n",
    "#  eval_samples           =   3229\n",
    "#  eval_total             =   3047"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7cec5-b420-456a-bcb8-bd11915599c8",
   "metadata": {},
   "source": [
    "### The small BLOOM model with light training is getting kicked pretty good by bert-base-uncased, so let's see if we can improve on this. Can we try the next larger model with the same settings on our single Workbench instance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "f05b8d27-3efb-4cce-90b7-967a3def0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train03/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "3ae691e2-a7d5-4fa6-95dd-cd25cebe5d86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 02:45:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 02:45:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train03/runs/Nov14_02-45-18_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train03/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train03/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 02:45:19 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 02:45:19 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 02:45:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 02:45:19 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 02:45:19 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 02:45:19 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 698.88it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 02:45:19,818 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-1b1/snapshots/1e718be072aa40714c9dc34e35ea6b64979a65ad/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 02:45:19,821 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-1b1\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:45:19,958 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-1b1/snapshots/1e718be072aa40714c9dc34e35ea6b64979a65ad/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:45:19,958 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:45:19,958 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-1b1/snapshots/1e718be072aa40714c9dc34e35ea6b64979a65ad/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:45:19,958 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-1b1/snapshots/1e718be072aa40714c9dc34e35ea6b64979a65ad/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 02:45:20,427 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-1b1/snapshots/1e718be072aa40714c9dc34e35ea6b64979a65ad/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-14 02:45:30,033 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 02:45:30,033 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-1b1 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/14/2022 02:45:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-1b8a51dbe31d03b3.arrow\n",
      "11/14/2022 02:45:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-1bae83e6ef5802d7.arrow\n",
      "[INFO|trainer.py:557] 2022-11-14 02:45:34,202 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 02:45:34,213 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 02:45:34,213 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-14 02:45:34,213 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1610] 2022-11-14 02:45:34,213 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1611] 2022-11-14 02:45:34,213 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1612] 2022-11-14 02:45:34,213 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 02:45:34,213 >>   Total optimization steps = 15072\n",
      "  0%|                                                 | 0/15072 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 681, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 622, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1504, in train\n",
      "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1804, in _inner_training_loop\n",
      "    self.scaler.step(self.optimizer)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\", line 338, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py\", line 285, in _maybe_opt_step\n",
      "    retval = optimizer.step(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py\", line 349, in step\n",
      "    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 15.78 GiB total capacity; 14.41 GiB already allocated; 18.94 MiB free; 14.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                                 | 0/15072 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#first attempt failed w/out of memory. Trying smaller batch size. (3 vs. 6 failed, trying 1 - also failed)\n",
    "#!python run_qa.py \\\n",
    "#  --model_name_or_path bigscience/bloom-1b1 \\\n",
    "#  --dataset_name dataset \\\n",
    "#  --do_train \\\n",
    "#  --do_eval \\\n",
    "#  --per_device_train_batch_size 1 \\\n",
    "#  --learning_rate 3e-5 \\\n",
    "#  --num_train_epochs 2 \\\n",
    "#  --max_seq_length 384 \\\n",
    "#  --doc_stride 128 \\\n",
    "#  --output_dir /home/jupyter/gcs/cdcmodel_train03/ \\\n",
    "#  --eval_accumulation_steps 1 \\\n",
    "#  --version_2_with_negative \\\n",
    "#  --overwrite_output_dir \\\n",
    "#  --fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d41c63-05f1-4df1-96fd-8f8dad06fddc",
   "metadata": {},
   "source": [
    "### No dice. Going up to the next model size will require more hardware (even batch size of 1 is failing), or different settings that I don't know yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08efcd40-88a0-4a72-ab45-022a9922b68f",
   "metadata": {},
   "source": [
    "### OK, so now let's try more epochs on the smaller dataset and see if that helps us out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "c97d6baf-63ee-4fdb-a17a-e7407d670c42",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 02:49:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 02:49:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train03/runs/Nov14_02-49-18_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train03/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train03/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 02:49:19 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 02:49:19 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 02:49:19 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 02:49:19 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 02:49:19 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 02:49:19 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 650.23it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 02:49:19,208 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 02:49:19,211 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:49:19,349 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:49:19,349 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:49:19,349 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 02:49:19,350 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 02:49:20,042 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-14 02:49:34,460 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 02:49:34,460 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/14/2022 02:49:34 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/14/2022 02:49:34 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-14 02:49:38,089 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 02:49:38,098 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 02:49:38,098 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-14 02:49:38,098 >>   Num Epochs = 4\n",
      "[INFO|trainer.py:1610] 2022-11-14 02:49:38,098 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-14 02:49:38,098 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-14 02:49:38,098 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 02:49:38,098 >>   Total optimization steps = 5024\n",
      "  0%|                                                  | 0/5024 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.5644, 'learning_rate': 2.707404458598726e-05, 'epoch': 0.4}          \n",
      " 10%|███▉                                    | 500/5024 [02:09<19:37,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 02:51:47,973 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:51:48,264 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:52:08,146 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:52:08,475 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:52:08,841 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.48, 'learning_rate': 2.4088375796178346e-05, 'epoch': 0.8}           \n",
      " 20%|███████▊                               | 1000/5024 [05:44<17:24,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 02:55:22,968 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:55:23,299 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:55:44,288 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:55:44,706 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:55:45,024 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.613, 'learning_rate': 2.1102707006369426e-05, 'epoch': 1.19}         \n",
      " 30%|███████████▋                           | 1500/5024 [09:23<15:21,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 02:59:01,313 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 02:59:01,662 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 02:59:20,830 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 02:59:21,184 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 02:59:21,567 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.3321, 'learning_rate': 1.811703821656051e-05, 'epoch': 1.59}         \n",
      " 40%|███████████████▌                       | 2000/5024 [13:00<13:08,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 03:02:38,594 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:02:38,927 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:03:00,763 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:03:01,142 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:03:01,441 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5518, 'learning_rate': 1.5131369426751593e-05, 'epoch': 1.99}        \n",
      " 50%|███████████████████▍                   | 2500/5024 [16:30<11:01,  3.81it/s][INFO|trainer.py:2671] 2022-11-14 03:06:08,804 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:06:09,151 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:06:31,120 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:06:31,479 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:06:31,866 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.2554, 'learning_rate': 1.2145700636942675e-05, 'epoch': 2.39}        \n",
      " 60%|███████████████████████▎               | 3000/5024 [19:59<08:46,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 03:09:37,400 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:09:37,783 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:09:58,745 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:09:59,176 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:09:59,555 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.1153, 'learning_rate': 9.160031847133758e-06, 'epoch': 2.79}         \n",
      " 70%|███████████████████████████▏           | 3500/5024 [23:34<06:35,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 03:13:12,990 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:13:13,328 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:13:41,459 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:13:41,807 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:13:42,150 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.989, 'learning_rate': 6.174363057324841e-06, 'epoch': 3.18}          \n",
      " 80%|███████████████████████████████        | 4000/5024 [27:07<04:26,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 03:16:46,079 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:16:46,391 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:17:06,211 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:17:06,600 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:17:07,061 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2254, 'learning_rate': 3.1946656050955412e-06, 'epoch': 3.58}        \n",
      " 90%|██████████████████████████████████▉    | 4500/5024 [30:32<02:16,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 03:20:10,873 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-4500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:20:11,196 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:20:32,068 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:20:32,457 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:20:32,768 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6211, 'learning_rate': 2.0899681528662422e-07, 'epoch': 3.98}        \n",
      "100%|██████████████████████████████████████▊| 5000/5024 [33:59<00:06,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 03:23:37,812 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/checkpoint-5000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:23:38,204 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:23:58,176 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:23:58,497 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:23:58,845 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████████| 5024/5024 [35:21<00:00,  3.73it/s][INFO|trainer.py:1852] 2022-11-14 03:24:59,361 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2121.2627, 'train_samples_per_second': 14.21, 'train_steps_per_second': 2.368, 'train_loss': 3.6621472421725083, 'epoch': 4.0}\n",
      "100%|███████████████████████████████████████| 5024/5024 [35:21<00:00,  2.37it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-14 03:24:59,388 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train03/\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:24:59,760 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train03/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:25:20,295 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train03/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:25:20,685 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train03/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:25:21,025 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train03/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  train_loss               =     3.6621\n",
      "  train_runtime            = 0:35:21.26\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =      14.21\n",
      "  train_steps_per_second   =      2.368\n",
      "11/14/2022 03:25:23 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-14 03:25:23,010 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-14 03:25:23,027 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-14 03:25:23,027 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-14 03:25:23,027 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:35<00:00, 11.29it/s]11/14/2022 03:26:01 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▏                                       | 19/3047 [00:00<00:16, 188.24it/s]\u001b[A\n",
      "  2%|▋                                       | 48/3047 [00:00<00:12, 245.18it/s]\u001b[A\n",
      "  3%|█▏                                      | 87/3047 [00:00<00:10, 279.80it/s]\u001b[A\n",
      "  4%|█▋                                     | 132/3047 [00:00<00:08, 339.74it/s]\u001b[A\n",
      "  6%|██▎                                    | 176/3047 [00:00<00:07, 373.64it/s]\u001b[A\n",
      "  7%|██▊                                    | 222/3047 [00:00<00:07, 401.64it/s]\u001b[A\n",
      "  9%|███▍                                   | 267/3047 [00:00<00:06, 416.81it/s]\u001b[A\n",
      " 10%|███▉                                   | 312/3047 [00:00<00:06, 424.79it/s]\u001b[A\n",
      " 12%|████▌                                  | 355/3047 [00:00<00:06, 422.75it/s]\u001b[A\n",
      " 13%|█████                                  | 399/3047 [00:01<00:06, 425.19it/s]\u001b[A\n",
      " 15%|█████▋                                 | 442/3047 [00:01<00:06, 423.68it/s]\u001b[A\n",
      " 16%|██████▏                                | 485/3047 [00:01<00:06, 383.39it/s]\u001b[A\n",
      " 17%|██████▋                                | 525/3047 [00:01<00:06, 386.50it/s]\u001b[A\n",
      " 19%|███████▏                               | 565/3047 [00:01<00:07, 339.70it/s]\u001b[A\n",
      " 20%|███████▋                               | 604/3047 [00:01<00:06, 351.69it/s]\u001b[A\n",
      " 21%|████████▎                              | 647/3047 [00:01<00:06, 371.28it/s]\u001b[A\n",
      " 23%|████████▊                              | 686/3047 [00:01<00:06, 363.04it/s]\u001b[A\n",
      " 24%|█████████▎                             | 723/3047 [00:02<00:07, 314.00it/s]\u001b[A\n",
      " 25%|█████████▋                             | 758/3047 [00:02<00:07, 321.68it/s]\u001b[A\n",
      " 26%|██████████▏                            | 793/3047 [00:02<00:06, 328.11it/s]\u001b[A\n",
      " 27%|██████████▋                            | 837/3047 [00:02<00:06, 358.19it/s]\u001b[A\n",
      " 29%|███████████▏                           | 874/3047 [00:02<00:06, 360.31it/s]\u001b[A\n",
      " 30%|███████████▋                           | 912/3047 [00:02<00:05, 363.76it/s]\u001b[A\n",
      " 31%|████████████▏                          | 956/3047 [00:02<00:05, 384.59it/s]\u001b[A\n",
      " 33%|████████████▊                          | 999/3047 [00:02<00:05, 396.68it/s]\u001b[A\n",
      " 34%|████████████▉                         | 1042/3047 [00:02<00:04, 404.91it/s]\u001b[A\n",
      " 36%|█████████████▌                        | 1084/3047 [00:02<00:04, 408.87it/s]\u001b[A\n",
      " 37%|██████████████                        | 1128/3047 [00:03<00:04, 416.85it/s]\u001b[A\n",
      " 38%|██████████████▌                       | 1170/3047 [00:03<00:04, 417.77it/s]\u001b[A\n",
      " 40%|███████████████                       | 1212/3047 [00:03<00:04, 418.42it/s]\u001b[A\n",
      " 41%|███████████████▋                      | 1256/3047 [00:03<00:04, 423.76it/s]\u001b[A\n",
      " 43%|████████████████▏                     | 1300/3047 [00:03<00:04, 426.34it/s]\u001b[A\n",
      " 44%|████████████████▋                     | 1343/3047 [00:03<00:03, 427.42it/s]\u001b[A\n",
      " 45%|█████████████████▎                    | 1386/3047 [00:03<00:03, 417.41it/s]\u001b[A\n",
      " 47%|█████████████████▊                    | 1428/3047 [00:03<00:03, 414.41it/s]\u001b[A\n",
      " 48%|██████████████████▎                   | 1470/3047 [00:03<00:05, 284.98it/s]\u001b[A\n",
      " 49%|██████████████████▊                   | 1504/3047 [00:04<00:05, 263.04it/s]\u001b[A\n",
      " 51%|███████████████████▎                  | 1548/3047 [00:04<00:04, 300.74it/s]\u001b[A\n",
      " 52%|███████████████████▊                  | 1592/3047 [00:04<00:04, 332.93it/s]\u001b[A\n",
      " 54%|████████████████████▍                 | 1635/3047 [00:04<00:03, 357.18it/s]\u001b[A\n",
      " 55%|████████████████████▉                 | 1678/3047 [00:04<00:03, 376.37it/s]\u001b[A\n",
      " 57%|█████████████████████▍                | 1723/3047 [00:04<00:03, 395.15it/s]\u001b[A\n",
      " 58%|██████████████████████                | 1767/3047 [00:04<00:03, 405.78it/s]\u001b[A\n",
      " 59%|██████████████████████▌               | 1812/3047 [00:04<00:02, 416.27it/s]\u001b[A\n",
      " 61%|███████████████████████▏              | 1855/3047 [00:04<00:02, 412.22it/s]\u001b[A\n",
      " 62%|███████████████████████▋              | 1899/3047 [00:05<00:02, 419.86it/s]\u001b[A\n",
      " 64%|████████████████████████▏             | 1942/3047 [00:05<00:02, 399.64it/s]\u001b[A\n",
      " 65%|████████████████████████▋             | 1984/3047 [00:05<00:02, 403.66it/s]\u001b[A\n",
      " 67%|█████████████████████████▎            | 2028/3047 [00:05<00:02, 413.14it/s]\u001b[A\n",
      " 68%|█████████████████████████▊            | 2070/3047 [00:05<00:02, 409.94it/s]\u001b[A\n",
      " 69%|██████████████████████████▎           | 2112/3047 [00:05<00:02, 407.66it/s]\u001b[A\n",
      " 71%|██████████████████████████▊           | 2154/3047 [00:05<00:02, 410.46it/s]\u001b[A\n",
      " 72%|███████████████████████████▍          | 2196/3047 [00:05<00:02, 409.75it/s]\u001b[A\n",
      " 73%|███████████████████████████▉          | 2238/3047 [00:05<00:01, 410.14it/s]\u001b[A\n",
      " 75%|████████████████████████████▍         | 2283/3047 [00:05<00:01, 421.19it/s]\u001b[A\n",
      " 76%|█████████████████████████████         | 2328/3047 [00:06<00:01, 428.34it/s]\u001b[A\n",
      " 78%|█████████████████████████████▌        | 2371/3047 [00:06<00:01, 422.75it/s]\u001b[A\n",
      " 79%|██████████████████████████████        | 2414/3047 [00:06<00:01, 404.45it/s]\u001b[A\n",
      " 81%|██████████████████████████████▌       | 2455/3047 [00:06<00:01, 376.07it/s]\u001b[A\n",
      " 82%|███████████████████████████████       | 2495/3047 [00:06<00:01, 381.25it/s]\u001b[A\n",
      " 83%|███████████████████████████████▋      | 2539/3047 [00:06<00:01, 395.56it/s]\u001b[A\n",
      " 85%|████████████████████████████████▏     | 2584/3047 [00:06<00:01, 410.16it/s]\u001b[A\n",
      " 86%|████████████████████████████████▋     | 2626/3047 [00:06<00:01, 406.06it/s]\u001b[A\n",
      " 88%|█████████████████████████████████▎    | 2667/3047 [00:06<00:00, 405.05it/s]\u001b[A\n",
      " 89%|█████████████████████████████████▊    | 2710/3047 [00:07<00:00, 411.10it/s]\u001b[A\n",
      " 90%|██████████████████████████████████▎   | 2755/3047 [00:07<00:00, 420.61it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▉   | 2801/3047 [00:07<00:00, 429.44it/s]\u001b[A\n",
      " 93%|███████████████████████████████████▍  | 2845/3047 [00:07<00:00, 431.08it/s]\u001b[A\n",
      " 95%|████████████████████████████████████  | 2889/3047 [00:07<00:00, 432.99it/s]\u001b[A\n",
      " 96%|████████████████████████████████████▌ | 2933/3047 [00:07<00:00, 432.60it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████▏| 2977/3047 [00:07<00:00, 425.49it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 3047/3047 [00:07<00:00, 387.89it/s]\u001b[A\n",
      "11/14/2022 03:26:09 - INFO - utils_qa - Saving predictions to /home/jupyter/gcs/cdcmodel_train03/eval_predictions.json.\n",
      "11/14/2022 03:26:09 - INFO - utils_qa - Saving nbest_preds to /home/jupyter/gcs/cdcmodel_train03/eval_nbest_predictions.json.\n",
      "11/14/2022 03:26:10 - INFO - utils_qa - Saving null_odds to /home/jupyter/gcs/cdcmodel_train03/eval_null_odds.json.\n",
      "100%|█████████████████████████████████████████| 404/404 [00:48<00:00,  8.31it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                  =    4.0\n",
      "  eval_HasAns_exact      = 0.1969\n",
      "  eval_HasAns_f1         = 0.7233\n",
      "  eval_HasAns_total      =   3047\n",
      "  eval_best_exact        = 0.1969\n",
      "  eval_best_exact_thresh =    0.0\n",
      "  eval_best_f1           = 0.7233\n",
      "  eval_best_f1_thresh    =    0.0\n",
      "  eval_exact             = 0.1969\n",
      "  eval_f1                = 0.7233\n",
      "  eval_samples           =   3229\n",
      "  eval_total             =   3047\n",
      "[INFO|modelcard.py:444] 2022-11-14 03:26:12,680 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'dataset', 'type': 'dataset', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "#Try double training epochs to see if any improvement is made (from 2 to 4)\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 4 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train03/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "19a8e2a7-f4b6-4cff-b751-f0eb1136b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hmm, interesting. F1 score went down. Overfitting?\n",
    "\n",
    "#  epoch                  =    4.0\n",
    "#  eval_HasAns_exact      = 0.1969\n",
    "#  eval_HasAns_f1         = 0.7233\n",
    "#  eval_HasAns_total      =   3047\n",
    "#  eval_best_exact        = 0.1969\n",
    "#  eval_best_exact_thresh =    0.0\n",
    "#  eval_best_f1           = 0.7233\n",
    "#  eval_best_f1_thresh    =    0.0\n",
    "#  eval_exact             = 0.1969\n",
    "#  eval_f1                = 0.7233\n",
    "#  eval_samples           =   3229\n",
    "#  eval_total             =   3047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "9af20fcc-2a14-4a3e-be84-b6bb3b65236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "2939a20d-6fe1-4c47-a59c-12d747c31b21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 03:29:23 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 03:29:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train04/runs/Nov14_03-29-22_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train04/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train04/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 03:29:23 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 03:29:23 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 03:29:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 03:29:23 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 03:29:23 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 03:29:23 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 37.27it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 03:29:23,914 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 03:29:23,927 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:29:24,122 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:29:24,122 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:29:24,122 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:29:24,122 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 03:29:25,047 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-14 03:29:39,393 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 03:29:39,393 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/14/2022 03:29:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/14/2022 03:29:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-14 03:29:42,962 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 03:29:43,007 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 03:29:43,007 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-14 03:29:43,007 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1610] 2022-11-14 03:29:43,007 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-14 03:29:43,007 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-14 03:29:43,007 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 03:29:43,007 >>   Total optimization steps = 1256\n",
      "  0%|                                                  | 0/1256 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.4562, 'learning_rate': 1.8296178343949047e-05, 'epoch': 0.4}         \n",
      " 40%|███████████████▉                        | 500/1256 [02:09<03:16,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 03:31:52,966 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train04/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:31:53,303 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:32:14,038 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:32:14,421 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:32:14,734 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.338, 'learning_rate': 6.353503184713376e-06, 'epoch': 0.8}           \n",
      " 80%|███████████████████████████████        | 1000/1256 [05:46<01:06,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 03:35:29,411 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train04/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:35:29,725 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:35:49,927 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:35:50,210 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:35:50,513 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train04/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████████| 1256/1256 [08:07<00:00,  3.84it/s][INFO|trainer.py:1852] 2022-11-14 03:37:50,532 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 487.5249, 'train_samples_per_second': 15.458, 'train_steps_per_second': 2.576, 'train_loss': 5.3383256462728905, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 1256/1256 [08:07<00:00,  2.58it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-14 03:37:50,554 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train04/\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:37:50,866 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train04/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:38:12,512 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train04/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:38:12,849 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train04/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:38:13,196 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train04/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     5.3383\n",
      "  train_runtime            = 0:08:07.52\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =     15.458\n",
      "  train_steps_per_second   =      2.576\n",
      "11/14/2022 03:38:15 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-14 03:38:15,324 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-14 03:38:15,326 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-14 03:38:15,326 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-14 03:38:15,326 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:35<00:00, 11.19it/s]11/14/2022 03:38:53 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▎                                       | 28/3047 [00:00<00:10, 278.70it/s]\u001b[A\n",
      "  2%|▉                                       | 70/3047 [00:00<00:08, 357.36it/s]\u001b[A\n",
      "  4%|█▍                                     | 114/3047 [00:00<00:07, 392.61it/s]\u001b[A\n",
      "  5%|██                                     | 157/3047 [00:00<00:07, 405.05it/s]\u001b[A\n",
      "  7%|██▌                                    | 200/3047 [00:00<00:06, 410.98it/s]\u001b[A\n",
      "  8%|███                                    | 243/3047 [00:00<00:06, 416.18it/s]\u001b[A\n",
      "  9%|███▋                                   | 285/3047 [00:00<00:06, 416.35it/s]\u001b[A\n",
      " 11%|████▏                                  | 327/3047 [00:00<00:06, 415.15it/s]\u001b[A\n",
      " 12%|████▋                                  | 369/3047 [00:00<00:06, 410.59it/s]\u001b[A\n",
      " 13%|█████▎                                 | 411/3047 [00:01<00:06, 410.84it/s]\u001b[A\n",
      " 15%|█████▊                                 | 453/3047 [00:01<00:06, 412.57it/s]\u001b[A\n",
      " 16%|██████▎                                | 495/3047 [00:01<00:06, 370.71it/s]\u001b[A\n",
      " 18%|██████▉                                | 539/3047 [00:01<00:06, 387.91it/s]\u001b[A\n",
      " 19%|███████▍                               | 579/3047 [00:01<00:07, 344.82it/s]\u001b[A\n",
      " 20%|███████▉                               | 620/3047 [00:01<00:06, 361.48it/s]\u001b[A\n",
      " 22%|████████▍                              | 663/3047 [00:01<00:06, 379.28it/s]\u001b[A\n",
      " 23%|████████▉                              | 702/3047 [00:01<00:07, 325.80it/s]\u001b[A\n",
      " 24%|█████████▌                             | 745/3047 [00:01<00:06, 350.79it/s]\u001b[A\n",
      " 26%|██████████                             | 782/3047 [00:02<00:06, 343.63it/s]\u001b[A\n",
      " 27%|██████████▌                            | 828/3047 [00:02<00:05, 373.62it/s]\u001b[A\n",
      " 28%|███████████                            | 868/3047 [00:02<00:05, 380.66it/s]\u001b[A\n",
      " 30%|███████████▌                           | 908/3047 [00:02<00:05, 386.00it/s]\u001b[A\n",
      " 31%|████████████▏                          | 952/3047 [00:02<00:05, 400.52it/s]\u001b[A\n",
      " 33%|████████████▋                          | 996/3047 [00:02<00:04, 411.22it/s]\u001b[A\n",
      " 34%|████████████▉                         | 1042/3047 [00:02<00:04, 423.49it/s]\u001b[A\n",
      " 36%|█████████████▌                        | 1085/3047 [00:02<00:04, 419.94it/s]\u001b[A\n",
      " 37%|██████████████                        | 1128/3047 [00:02<00:04, 420.20it/s]\u001b[A\n",
      " 39%|██████████████▋                       | 1174/3047 [00:02<00:04, 429.16it/s]\u001b[A\n",
      " 40%|███████████████▏                      | 1219/3047 [00:03<00:04, 434.80it/s]\u001b[A\n",
      " 42%|███████████████▊                      | 1265/3047 [00:03<00:04, 439.23it/s]\u001b[A\n",
      " 43%|████████████████▎                     | 1310/3047 [00:03<00:03, 440.97it/s]\u001b[A\n",
      " 44%|████████████████▉                     | 1355/3047 [00:03<00:03, 442.69it/s]\u001b[A\n",
      " 46%|█████████████████▍                    | 1400/3047 [00:03<00:03, 442.89it/s]\u001b[A\n",
      " 47%|██████████████████                    | 1445/3047 [00:03<00:03, 440.09it/s]\u001b[A\n",
      " 49%|██████████████████▌                   | 1490/3047 [00:03<00:05, 276.32it/s]\u001b[A\n",
      " 50%|███████████████████▏                  | 1536/3047 [00:04<00:04, 314.56it/s]\u001b[A\n",
      " 52%|███████████████████▋                  | 1581/3047 [00:04<00:04, 345.49it/s]\u001b[A\n",
      " 53%|████████████████████▎                 | 1624/3047 [00:04<00:03, 364.73it/s]\u001b[A\n",
      " 55%|████████████████████▊                 | 1670/3047 [00:04<00:03, 387.89it/s]\u001b[A\n",
      " 56%|█████████████████████▍                | 1715/3047 [00:04<00:03, 403.01it/s]\u001b[A\n",
      " 58%|█████████████████████▉                | 1761/3047 [00:04<00:03, 418.47it/s]\u001b[A\n",
      " 59%|██████████████████████▌               | 1806/3047 [00:04<00:02, 427.19it/s]\u001b[A\n",
      " 61%|███████████████████████               | 1851/3047 [00:04<00:02, 422.94it/s]\u001b[A\n",
      " 62%|███████████████████████▋              | 1897/3047 [00:04<00:02, 431.68it/s]\u001b[A\n",
      " 64%|████████████████████████▏             | 1941/3047 [00:04<00:02, 415.58it/s]\u001b[A\n",
      " 65%|████████████████████████▋             | 1984/3047 [00:05<00:02, 415.89it/s]\u001b[A\n",
      " 67%|█████████████████████████▎            | 2027/3047 [00:05<00:02, 414.83it/s]\u001b[A\n",
      " 68%|█████████████████████████▊            | 2069/3047 [00:05<00:02, 413.72it/s]\u001b[A\n",
      " 69%|██████████████████████████▎           | 2112/3047 [00:05<00:02, 416.37it/s]\u001b[A\n",
      " 71%|██████████████████████████▉           | 2155/3047 [00:05<00:02, 417.62it/s]\u001b[A\n",
      " 72%|███████████████████████████▍          | 2197/3047 [00:05<00:02, 415.82it/s]\u001b[A\n",
      " 73%|███████████████████████████▉          | 2239/3047 [00:05<00:01, 417.01it/s]\u001b[A\n",
      " 75%|████████████████████████████▍         | 2284/3047 [00:05<00:01, 425.13it/s]\u001b[A\n",
      " 77%|█████████████████████████████         | 2331/3047 [00:05<00:01, 436.50it/s]\u001b[A\n",
      " 78%|█████████████████████████████▌        | 2375/3047 [00:05<00:01, 432.20it/s]\u001b[A\n",
      " 79%|██████████████████████████████▏       | 2419/3047 [00:06<00:01, 412.37it/s]\u001b[A\n",
      " 81%|██████████████████████████████▋       | 2461/3047 [00:06<00:01, 373.26it/s]\u001b[A\n",
      " 82%|███████████████████████████████▎      | 2506/3047 [00:06<00:01, 393.34it/s]\u001b[A\n",
      " 84%|███████████████████████████████▊      | 2550/3047 [00:06<00:01, 405.52it/s]\u001b[A\n",
      " 85%|████████████████████████████████▍     | 2596/3047 [00:06<00:01, 418.56it/s]\u001b[A\n",
      " 87%|████████████████████████████████▉     | 2642/3047 [00:06<00:00, 429.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████▌    | 2688/3047 [00:06<00:00, 437.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████    | 2733/3047 [00:06<00:00, 440.78it/s]\u001b[A\n",
      " 91%|██████████████████████████████████▋   | 2778/3047 [00:06<00:00, 442.72it/s]\u001b[A\n",
      " 93%|███████████████████████████████████▏  | 2823/3047 [00:07<00:00, 443.63it/s]\u001b[A\n",
      " 94%|███████████████████████████████████▊  | 2868/3047 [00:07<00:00, 441.93it/s]\u001b[A\n",
      " 96%|████████████████████████████████████▎ | 2913/3047 [00:07<00:00, 437.35it/s]\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 2957/3047 [00:07<00:00, 433.91it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████▍| 3001/3047 [00:07<00:00, 426.97it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 3047/3047 [00:07<00:00, 402.39it/s]\u001b[A\n",
      "11/14/2022 03:39:01 - INFO - utils_qa - Saving predictions to /home/jupyter/gcs/cdcmodel_train04/eval_predictions.json.\n",
      "11/14/2022 03:39:01 - INFO - utils_qa - Saving nbest_preds to /home/jupyter/gcs/cdcmodel_train04/eval_nbest_predictions.json.\n",
      "11/14/2022 03:39:02 - INFO - utils_qa - Saving null_odds to /home/jupyter/gcs/cdcmodel_train04/eval_null_odds.json.\n",
      "100%|█████████████████████████████████████████| 404/404 [00:47<00:00,  8.46it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                  =    1.0\n",
      "  eval_HasAns_exact      = 0.4266\n",
      "  eval_HasAns_f1         =  0.928\n",
      "  eval_HasAns_total      =   3047\n",
      "  eval_best_exact        = 0.4266\n",
      "  eval_best_exact_thresh =    0.0\n",
      "  eval_best_f1           =  0.928\n",
      "  eval_best_f1_thresh    =    0.0\n",
      "  eval_exact             = 0.4266\n",
      "  eval_f1                =  0.928\n",
      "  eval_samples           =   3229\n",
      "  eval_total             =   3047\n",
      "[INFO|modelcard.py:444] 2022-11-14 03:39:04,010 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'dataset', 'type': 'dataset', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "#Try with only one epoch - see what happens\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 1 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train04/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "de3cfe64-e6d1-4a9c-8b66-1ce8d8871e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interesting. A single epoch did result in a slightly higher F1 score of .928 vs. .923\n",
    "\n",
    "#  epoch                  =    1.0\n",
    "#  eval_HasAns_exact      = 0.4266\n",
    "#  eval_HasAns_f1         =  0.928\n",
    "#  eval_HasAns_total      =   3047\n",
    "#  eval_best_exact        = 0.4266\n",
    "#  eval_best_exact_thresh =    0.0\n",
    "#  eval_best_f1           =  0.928\n",
    "#  eval_best_f1_thresh    =    0.0\n",
    "#  eval_exact             = 0.4266\n",
    "#  eval_f1                =  0.928\n",
    "#  eval_samples           =   3229\n",
    "#  eval_total             =   3047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "1610b6be-48b1-4072-9953-3a0cb7f08bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train05/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "ac877d11-80f8-4c38-9307-07adf899183a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 03:45:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 03:45:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train05/runs/Nov14_03-45-48_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=0.1,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train05/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train05/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 03:45:49 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 03:45:49 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 03:45:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 03:45:49 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 03:45:49 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 03:45:49 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 743.54it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 03:45:49,761 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 03:45:49,764 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:45:49,896 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:45:49,897 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:45:49,897 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:45:49,897 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 03:45:50,373 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-14 03:45:55,504 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 03:45:55,504 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/14/2022 03:45:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/14/2022 03:45:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-14 03:45:59,162 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 03:45:59,173 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 03:45:59,173 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-14 03:45:59,173 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1610] 2022-11-14 03:45:59,173 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-14 03:45:59,173 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-14 03:45:59,173 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 03:45:59,173 >>   Total optimization steps = 126\n",
      "  0%|                                                   | 0/126 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|█████████████████████████████████████████| 126/126 [00:32<00:00,  3.85it/s][INFO|trainer.py:1852] 2022-11-14 03:46:31,666 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 32.4926, 'train_samples_per_second': 23.193, 'train_steps_per_second': 3.878, 'train_loss': 9.1413332015749, 'epoch': 0.1}\n",
      "100%|█████████████████████████████████████████| 126/126 [00:32<00:00,  3.88it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-14 03:46:31,688 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train05/\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:46:32,044 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train05/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:46:53,711 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train05/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:46:54,092 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train05/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:46:54,401 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train05/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        0.1\n",
      "  train_loss               =     9.1413\n",
      "  train_runtime            = 0:00:32.49\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =     23.193\n",
      "  train_steps_per_second   =      3.878\n",
      "11/14/2022 03:46:56 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-14 03:46:56,477 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-14 03:46:56,479 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-14 03:46:56,479 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-14 03:46:56,479 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:34<00:00, 11.56it/s]11/14/2022 03:47:34 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▋                                       | 48/3047 [00:00<00:06, 479.26it/s]\u001b[A\n",
      "  3%|█▎                                      | 96/3047 [00:00<00:06, 478.74it/s]\u001b[A\n",
      "  5%|█▊                                     | 146/3047 [00:00<00:05, 484.98it/s]\u001b[A\n",
      "  6%|██▍                                    | 195/3047 [00:00<00:05, 485.25it/s]\u001b[A\n",
      "  8%|███▏                                   | 245/3047 [00:00<00:05, 486.71it/s]\u001b[A\n",
      " 10%|███▊                                   | 294/3047 [00:00<00:05, 484.25it/s]\u001b[A\n",
      " 11%|████▍                                  | 343/3047 [00:00<00:05, 474.51it/s]\u001b[A\n",
      " 13%|█████                                  | 391/3047 [00:00<00:05, 469.90it/s]\u001b[A\n",
      " 14%|█████▌                                 | 439/3047 [00:00<00:05, 470.04it/s]\u001b[A\n",
      " 16%|██████▏                                | 487/3047 [00:01<00:07, 358.61it/s]\u001b[A\n",
      " 17%|██████▊                                | 530/3047 [00:01<00:06, 374.46it/s]\u001b[A\n",
      " 19%|███████▎                               | 571/3047 [00:01<00:07, 347.17it/s]\u001b[A\n",
      " 20%|███████▊                               | 614/3047 [00:01<00:06, 367.91it/s]\u001b[A\n",
      " 22%|████████▍                              | 660/3047 [00:01<00:06, 390.48it/s]\u001b[A\n",
      " 23%|████████▉                              | 701/3047 [00:01<00:06, 336.50it/s]\u001b[A\n",
      " 24%|█████████▌                             | 746/3047 [00:01<00:06, 363.48it/s]\u001b[A\n",
      " 26%|██████████                             | 785/3047 [00:01<00:06, 359.34it/s]\u001b[A\n",
      " 27%|██████████▋                            | 832/3047 [00:02<00:05, 387.66it/s]\u001b[A\n",
      " 29%|███████████▏                           | 874/3047 [00:02<00:05, 395.61it/s]\u001b[A\n",
      " 30%|███████████▋                           | 918/3047 [00:02<00:05, 406.55it/s]\u001b[A\n",
      " 32%|████████████▎                          | 965/3047 [00:02<00:04, 422.52it/s]\u001b[A\n",
      " 33%|████████████▌                         | 1012/3047 [00:02<00:04, 435.49it/s]\u001b[A\n",
      " 35%|█████████████▏                        | 1062/3047 [00:02<00:04, 453.24it/s]\u001b[A\n",
      " 36%|█████████████▊                        | 1111/3047 [00:02<00:04, 463.86it/s]\u001b[A\n",
      " 38%|██████████████▍                       | 1159/3047 [00:02<00:04, 465.78it/s]\u001b[A\n",
      " 40%|███████████████                       | 1206/3047 [00:02<00:03, 462.91it/s]\u001b[A\n",
      " 41%|███████████████▋                      | 1253/3047 [00:02<00:03, 456.76it/s]\u001b[A\n",
      " 43%|████████████████▏                     | 1302/3047 [00:03<00:03, 464.26it/s]\u001b[A\n",
      " 44%|████████████████▊                     | 1351/3047 [00:03<00:03, 469.83it/s]\u001b[A\n",
      " 46%|█████████████████▍                    | 1399/3047 [00:03<00:03, 469.76it/s]\u001b[A\n",
      " 47%|██████████████████                    | 1447/3047 [00:03<00:03, 462.21it/s]\u001b[A\n",
      " 49%|██████████████████▋                   | 1494/3047 [00:03<00:05, 297.45it/s]\u001b[A\n",
      " 51%|███████████████████▏                  | 1543/3047 [00:03<00:04, 336.78it/s]\u001b[A\n",
      " 52%|███████████████████▊                  | 1591/3047 [00:03<00:03, 368.39it/s]\u001b[A\n",
      " 54%|████████████████████▍                 | 1639/3047 [00:03<00:03, 394.57it/s]\u001b[A\n",
      " 55%|█████████████████████                 | 1688/3047 [00:04<00:03, 418.09it/s]\u001b[A\n",
      " 57%|█████████████████████▋                | 1737/3047 [00:04<00:03, 434.90it/s]\u001b[A\n",
      " 59%|██████████████████████▏               | 1784/3047 [00:04<00:02, 428.20it/s]\u001b[A\n",
      " 60%|██████████████████████▊               | 1829/3047 [00:04<00:02, 418.37it/s]\u001b[A\n",
      " 62%|███████████████████████▎              | 1874/3047 [00:04<00:02, 425.77it/s]\u001b[A\n",
      " 63%|███████████████████████▉              | 1918/3047 [00:04<00:02, 418.75it/s]\u001b[A\n",
      " 65%|████████████████████████▌             | 1966/3047 [00:04<00:02, 433.52it/s]\u001b[A\n",
      " 66%|█████████████████████████             | 2014/3047 [00:04<00:02, 444.55it/s]\u001b[A\n",
      " 68%|█████████████████████████▋            | 2060/3047 [00:04<00:02, 446.37it/s]\u001b[A\n",
      " 69%|██████████████████████████▎           | 2108/3047 [00:05<00:02, 455.66it/s]\u001b[A\n",
      " 71%|██████████████████████████▉           | 2156/3047 [00:05<00:01, 461.65it/s]\u001b[A\n",
      " 72%|███████████████████████████▍          | 2204/3047 [00:05<00:01, 465.02it/s]\u001b[A\n",
      " 74%|████████████████████████████          | 2251/3047 [00:05<00:01, 462.69it/s]\u001b[A\n",
      " 75%|████████████████████████████▋         | 2299/3047 [00:05<00:01, 465.46it/s]\u001b[A\n",
      " 77%|█████████████████████████████▎        | 2347/3047 [00:05<00:01, 469.10it/s]\u001b[A\n",
      " 79%|█████████████████████████████▊        | 2394/3047 [00:05<00:01, 455.57it/s]\u001b[A\n",
      " 80%|██████████████████████████████▍       | 2440/3047 [00:05<00:01, 397.85it/s]\u001b[A\n",
      " 81%|██████████████████████████████▉       | 2482/3047 [00:05<00:01, 397.68it/s]\u001b[A\n",
      " 83%|███████████████████████████████▍      | 2525/3047 [00:06<00:01, 404.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████      | 2569/3047 [00:06<00:01, 413.44it/s]\u001b[A\n",
      " 86%|████████████████████████████████▌     | 2616/3047 [00:06<00:01, 427.08it/s]\u001b[A\n",
      " 87%|█████████████████████████████████▏    | 2662/3047 [00:06<00:00, 434.78it/s]\u001b[A\n",
      " 89%|█████████████████████████████████▊    | 2709/3047 [00:06<00:00, 442.94it/s]\u001b[A\n",
      " 90%|██████████████████████████████████▎   | 2755/3047 [00:06<00:00, 447.29it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▉   | 2801/3047 [00:06<00:00, 449.23it/s]\u001b[A\n",
      " 93%|███████████████████████████████████▌  | 2847/3047 [00:06<00:00, 447.46it/s]\u001b[A\n",
      " 95%|████████████████████████████████████  | 2892/3047 [00:06<00:00, 447.75it/s]\u001b[A\n",
      " 96%|████████████████████████████████████▋ | 2938/3047 [00:06<00:00, 448.75it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████▏| 2983/3047 [00:07<00:00, 438.79it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 3047/3047 [00:07<00:00, 424.18it/s]\u001b[A\n",
      "11/14/2022 03:47:41 - INFO - utils_qa - Saving predictions to /home/jupyter/gcs/cdcmodel_train05/eval_predictions.json.\n",
      "11/14/2022 03:47:41 - INFO - utils_qa - Saving nbest_preds to /home/jupyter/gcs/cdcmodel_train05/eval_nbest_predictions.json.\n",
      "11/14/2022 03:47:42 - INFO - utils_qa - Saving null_odds to /home/jupyter/gcs/cdcmodel_train05/eval_null_odds.json.\n",
      "100%|█████████████████████████████████████████| 404/404 [00:46<00:00,  8.70it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                  =    0.1\n",
      "  eval_HasAns_exact      = 0.1313\n",
      "  eval_HasAns_f1         = 0.4958\n",
      "  eval_HasAns_total      =   3047\n",
      "  eval_best_exact        = 0.1313\n",
      "  eval_best_exact_thresh =    0.0\n",
      "  eval_best_f1           = 0.4958\n",
      "  eval_best_f1_thresh    =    0.0\n",
      "  eval_exact             = 0.1313\n",
      "  eval_f1                = 0.4958\n",
      "  eval_samples           =   3229\n",
      "  eval_total             =   3047\n",
      "[INFO|modelcard.py:444] 2022-11-14 03:47:43,895 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'dataset', 'type': 'dataset', 'config': 'plain_text', 'split': 'train', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "#Can we do 0 epochs or will that error out? Errors out. Less than 1?\n",
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 0.1 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train05/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "2ff3ddfd-8bd4-4bc0-a1ba-1028c0ae6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, so 0 training, or close to it, is not better than 1 epoch.\n",
    "\n",
    "#  epoch                  =    0.1\n",
    "#  eval_HasAns_exact      = 0.1313\n",
    "#  eval_HasAns_f1         = 0.4958\n",
    "#  eval_HasAns_total      =   3047\n",
    "#  eval_best_exact        = 0.1313\n",
    "#  eval_best_exact_thresh =    0.0\n",
    "#  eval_best_f1           = 0.4958\n",
    "#  eval_best_f1_thresh    =    0.0\n",
    "#  eval_exact             = 0.1313\n",
    "#  eval_f1                = 0.4958\n",
    "#  eval_samples           =   3229\n",
    "#  eval_total             =   3047"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff8225-e05c-4eea-af3f-c74aa0e2ab8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OK, final test - let's do a whole bunch of epochs and see what happens. This will need to run for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "0564e06e-5d38-4213-b594-78742feb5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train06/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "740f7dff-72ab-4605-80ed-22241a893e05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14/2022 03:52:07 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/14/2022 03:52:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train06/runs/Nov14_03-52-07_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=100.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train06/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train06/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/14/2022 03:52:07 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/14/2022 03:52:07 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 03:52:07 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/14/2022 03:52:07 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/14/2022 03:52:07 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/14/2022 03:52:07 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 621.19it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-14 03:52:08,137 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-14 03:52:08,143 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:52:08,279 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:52:08,279 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:52:08,280 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-14 03:52:08,280 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-14 03:52:08,851 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-14 03:52:14,196 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-14 03:52:14,196 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/14/2022 03:52:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/14/2022 03:52:14 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-14 03:52:17,825 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-14 03:52:17,836 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-14 03:52:17,837 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-14 03:52:17,837 >>   Num Epochs = 100\n",
      "[INFO|trainer.py:1610] 2022-11-14 03:52:17,837 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-14 03:52:17,837 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-14 03:52:17,837 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-14 03:52:17,837 >>   Total optimization steps = 125600\n",
      "  0%|                                                | 0/125600 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.6975, 'learning_rate': 2.988296178343949e-05, 'epoch': 0.4}          \n",
      "  0%|▏                                   | 500/125600 [02:10<9:05:33,  3.82it/s][INFO|trainer.py:2671] 2022-11-14 03:54:28,432 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:54:28,760 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:54:51,428 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:54:53,675 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:54:54,015 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.413, 'learning_rate': 2.9763535031847134e-05, 'epoch': 0.8}          \n",
      "  1%|▎                                  | 1000/125600 [05:50<9:00:52,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 03:58:08,721 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 03:58:09,024 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 03:58:34,980 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 03:58:35,296 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 03:58:35,654 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5707, 'learning_rate': 2.964410828025478e-05, 'epoch': 1.19}         \n",
      "  1%|▍                                  | 1500/125600 [09:31<8:58:08,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 04:01:49,953 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:01:50,432 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:02:10,213 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:02:10,714 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:02:11,246 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4966, 'learning_rate': 2.952468152866242e-05, 'epoch': 1.59}         \n",
      "  2%|▌                                  | 2000/125600 [13:22<8:56:31,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 04:05:40,273 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:05:40,615 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:06:02,812 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:06:03,191 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:06:03,503 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.7477, 'learning_rate': 2.9405254777070064e-05, 'epoch': 1.99}        \n",
      "  2%|▋                                  | 2500/125600 [16:52<8:52:25,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:09:11,037 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:09:11,497 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:09:33,996 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:09:34,404 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:09:34,803 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4618, 'learning_rate': 2.9285828025477707e-05, 'epoch': 2.39}        \n",
      "  2%|▊                                  | 3000/125600 [20:26<8:50:08,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:12:44,464 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:12:44,776 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:13:10,987 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:13:11,412 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:13:11,811 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4774, 'learning_rate': 2.916640127388535e-05, 'epoch': 2.79}         \n",
      "  3%|▉                                  | 3500/125600 [23:58<8:49:10,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:16:16,659 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:16:16,959 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:16:37,931 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:16:38,288 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:16:38,635 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4214, 'learning_rate': 2.9046974522292994e-05, 'epoch': 3.18}        \n",
      "  3%|█                                  | 4000/125600 [27:38<8:46:50,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:19:56,529 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:19:56,944 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:20:18,195 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:20:18,607 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:20:18,979 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5773, 'learning_rate': 2.8927786624203823e-05, 'epoch': 3.58}        \n",
      "  4%|█▎                                 | 4500/125600 [31:11<8:44:12,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:23:29,363 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-4500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:23:29,675 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:23:50,628 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:23:50,954 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:23:51,278 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.2549, 'learning_rate': 2.8808359872611467e-05, 'epoch': 3.98}        \n",
      "  4%|█▍                                 | 5000/125600 [34:42<8:43:09,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 04:27:00,110 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-5000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:27:00,539 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:27:24,652 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:27:24,991 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:27:25,425 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4922, 'learning_rate': 2.868893312101911e-05, 'epoch': 4.38}         \n",
      "  4%|█▌                                 | 5500/125600 [38:24<8:38:43,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 04:30:42,987 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-5500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:30:43,326 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:31:04,345 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:31:04,660 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:31:05,016 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-5500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "  4%|█▌                                 | 5576/125600 [40:08<8:38:39,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5596, 'learning_rate': 2.845031847133758e-05, 'epoch': 5.18}         \n",
      "  5%|█▊                                 | 6500/125600 [45:21<8:35:30,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:37:40,002 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-6500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:37:40,382 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:38:02,396 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:38:02,785 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:38:03,146 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-6500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.7913, 'learning_rate': 2.8331130573248408e-05, 'epoch': 5.57}        \n",
      "  6%|█▉                                 | 7000/125600 [48:46<8:33:41,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 04:41:04,363 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-7000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:41:04,676 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:41:23,574 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:41:23,909 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:41:24,242 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0112, 'learning_rate': 2.821170382165605e-05, 'epoch': 5.97}         \n",
      "  6%|██                                 | 7500/125600 [52:09<8:30:32,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 04:44:27,434 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-7500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:44:27,737 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:44:48,849 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:44:49,188 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:44:49,531 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-7500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6159, 'learning_rate': 2.8092277070063698e-05, 'epoch': 6.37}        \n",
      "  6%|██▏                                | 8000/125600 [55:32<8:28:11,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 04:47:51,000 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-8000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:47:51,325 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:48:14,738 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:48:15,047 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:48:15,400 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-8000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "  6%|██▎                                | 8097/125600 [57:25<8:28:45,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6099, 'learning_rate': 2.785342356687898e-05, 'epoch': 7.17}         \n",
      "  7%|██▎                              | 9000/125600 [1:02:29<8:23:54,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 04:54:47,600 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-9000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:54:47,951 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:55:13,054 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:55:13,410 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:55:13,780 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5667, 'learning_rate': 2.7733996815286625e-05, 'epoch': 7.56}        \n",
      "  8%|██▍                              | 9500/125600 [1:05:54<8:21:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 04:58:12,798 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-9500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 04:58:13,126 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 04:58:33,391 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 04:58:33,768 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 04:58:34,135 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-9500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5934, 'learning_rate': 2.7614570063694268e-05, 'epoch': 7.96}        \n",
      "  8%|██▌                             | 10000/125600 [1:09:12<8:21:05,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 05:01:30,350 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-10000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:01:30,738 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:01:51,521 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:01:51,831 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:01:52,171 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.7481, 'learning_rate': 2.7495382165605094e-05, 'epoch': 8.36}        \n",
      "  8%|██▋                             | 10500/125600 [1:12:32<8:17:59,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 05:04:50,826 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-10500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:04:51,183 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:05:12,605 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:05:12,919 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:05:13,287 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-10500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8187, 'learning_rate': 2.7256528662420384e-05, 'epoch': 9.16}        \n",
      "  9%|██▉                             | 11500/125600 [1:19:12<8:13:46,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 05:11:30,755 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-11500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:11:31,188 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:11:53,386 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:11:53,695 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:11:54,105 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-11500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8765, 'learning_rate': 2.7137101910828027e-05, 'epoch': 9.55}        \n",
      " 10%|███                             | 12000/125600 [1:22:36<8:13:33,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 05:14:54,111 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-12000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:14:54,436 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:15:13,692 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:15:14,003 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:15:14,304 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8891, 'learning_rate': 2.7017914012738853e-05, 'epoch': 9.95}        \n",
      " 10%|███▏                            | 12500/125600 [1:25:56<8:09:07,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 05:18:14,520 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-12500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:18:14,864 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:18:38,234 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:18:38,626 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:18:38,942 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-12500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6341, 'learning_rate': 2.6898487261146496e-05, 'epoch': 10.35}       \n",
      " 10%|███▎                            | 13000/125600 [1:29:26<8:06:32,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 05:21:44,797 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-13000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:21:45,152 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:22:07,880 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:22:08,216 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:22:08,535 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-13000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 10%|███▎                            | 13111/125600 [1:31:05<8:06:25,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 11%|███▍                            | 13653/125600 [1:34:35<8:04:42,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1625, 'learning_rate': 2.6062738853503186e-05, 'epoch': 13.14}       \n",
      " 13%|████▏                           | 16500/125600 [1:52:40<7:51:52,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 05:44:58,335 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-16500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:44:58,698 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:45:20,784 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:45:21,158 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:45:21,552 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-16500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.221, 'learning_rate': 2.594355095541401e-05, 'epoch': 13.54}         \n",
      " 14%|████▎                           | 17000/125600 [1:55:58<7:51:02,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 05:48:16,557 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-17000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:48:16,849 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:48:38,344 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:48:38,679 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:48:38,945 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3075, 'learning_rate': 2.5824124203821655e-05, 'epoch': 13.93}       \n",
      " 14%|████▍                           | 17500/125600 [1:59:18<7:47:21,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 05:51:36,487 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-17500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:51:36,803 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:51:56,470 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:51:56,814 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:51:57,176 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-17500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0438, 'learning_rate': 2.57046974522293e-05, 'epoch': 14.33}         \n",
      " 14%|████▌                           | 18000/125600 [2:02:33<7:44:51,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 05:54:51,572 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-18000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:54:51,932 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:55:13,678 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:55:13,992 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:55:14,407 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2311, 'learning_rate': 2.5585270700636945e-05, 'epoch': 14.73}       \n",
      " 15%|████▋                           | 18500/125600 [2:05:54<7:43:15,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 05:58:12,845 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-18500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 05:58:13,172 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 05:58:31,758 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 05:58:32,064 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 05:58:32,360 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-18500/special_tokens_map.json\n",
      "{'loss': 2.3121, 'learning_rate': 2.5465843949044585e-05, 'epoch': 15.13}       \n",
      " 15%|████▊                           | 19000/125600 [2:09:17<7:40:55,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 06:01:35,823 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-19000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:01:36,146 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:01:59,820 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:02:00,272 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:02:00,565 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8979, 'learning_rate': 2.5346417197452228e-05, 'epoch': 15.53}       \n",
      " 16%|████▉                           | 19500/125600 [2:12:50<7:38:09,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 06:05:08,174 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-19500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:05:08,490 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:05:31,075 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:05:31,452 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:05:31,745 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-19500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0793, 'learning_rate': 2.5226990445859875e-05, 'epoch': 15.92}       \n",
      " 16%|█████                           | 20000/125600 [2:16:11<7:35:59,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 06:08:29,195 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-20000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:08:29,559 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:08:50,779 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:08:51,199 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:08:51,540 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4946, 'learning_rate': 2.5107563694267518e-05, 'epoch': 16.32}       \n",
      " 16%|█████▏                          | 20500/125600 [2:19:28<7:33:38,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 06:11:46,423 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-20500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:11:46,766 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:12:09,185 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:12:09,552 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:12:09,895 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-20500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6039, 'learning_rate': 2.4988136942675158e-05, 'epoch': 16.72}       \n",
      " 17%|█████▎                          | 21000/125600 [2:22:46<7:30:57,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 06:15:04,661 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-21000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:15:05,047 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:15:25,209 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:15:25,563 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:15:25,873 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-21000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5405, 'learning_rate': 2.4749761146496816e-05, 'epoch': 17.52}       \n",
      " 18%|█████▌                          | 22000/125600 [2:29:27<7:26:35,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 06:21:45,541 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-22000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:21:45,859 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:22:06,342 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:22:06,754 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:22:07,160 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2666, 'learning_rate': 2.463033439490446e-05, 'epoch': 17.91}        \n",
      " 18%|█████▋                          | 22500/125600 [2:32:48<7:26:19,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 06:25:06,151 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-22500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:25:06,533 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:25:26,171 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:25:26,567 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:25:27,026 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-22500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8855, 'learning_rate': 2.4510907643312103e-05, 'epoch': 18.31}       \n",
      " 18%|█████▊                          | 23000/125600 [2:36:05<7:22:39,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 06:28:23,334 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-23000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:28:23,641 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:28:43,540 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:28:43,848 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:28:44,192 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5188, 'learning_rate': 2.439171974522293e-05, 'epoch': 18.71}        \n",
      " 19%|█████▉                          | 23500/125600 [2:39:23<7:20:15,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 06:31:41,840 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-23500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:31:42,145 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:32:09,782 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:32:10,176 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:32:10,562 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-23500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0856, 'learning_rate': 2.4272292993630572e-05, 'epoch': 19.11}       \n",
      " 19%|██████                          | 24000/125600 [2:42:54<7:17:46,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 06:35:12,169 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-24000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:35:12,571 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:35:33,381 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:35:33,703 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:35:34,056 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-24000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 19%|█████▉                         | 24009/125600 [2:44:05<40:52:15,  1.45s/it]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3206, 'learning_rate': 2.3555732484076436e-05, 'epoch': 21.5}        \n",
      " 21%|██████▉                         | 27000/125600 [3:03:00<7:05:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 06:55:18,303 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-27000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 06:55:18,638 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 06:55:39,872 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 06:55:40,253 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 06:55:40,561 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-27000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0621, 'learning_rate': 2.295907643312102e-05, 'epoch': 23.49}        \n",
      " 23%|███████▌                        | 29500/125600 [3:19:46<6:54:51,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 07:12:04,695 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-29500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:12:05,308 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:12:32,526 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:12:32,912 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:12:33,209 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-29500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1662, 'learning_rate': 2.283964968152866e-05, 'epoch': 23.89}        \n",
      " 24%|███████▋                        | 30000/125600 [3:23:15<6:52:08,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 07:15:33,670 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-30000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:15:34,081 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:15:53,837 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:15:54,140 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:15:54,468 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9482, 'learning_rate': 2.2720222929936307e-05, 'epoch': 24.28}       \n",
      " 24%|███████▊                        | 30500/125600 [3:26:31<6:50:08,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 07:18:49,169 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-30500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:18:49,522 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:19:09,225 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:19:09,578 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:19:09,947 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-30500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9606, 'learning_rate': 2.260079617834395e-05, 'epoch': 24.68}        \n",
      " 25%|███████▉                        | 31000/125600 [3:29:51<6:49:53,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 07:22:09,502 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-31000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:22:09,853 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:22:30,027 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:22:30,446 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:22:30,769 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2075, 'learning_rate': 2.2481369426751594e-05, 'epoch': 25.08}       \n",
      " 25%|████████                        | 31500/125600 [3:33:10<6:45:50,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 07:25:28,924 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-31500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:25:29,264 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:25:49,035 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:25:49,367 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:25:49,688 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-31500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 25%|████████                        | 31609/125600 [3:34:50<6:45:27,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0392, 'learning_rate': 2.164562101910828e-05, 'epoch': 27.87}        \n",
      " 28%|████████▉                       | 35000/125600 [3:56:42<6:31:12,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 07:49:00,287 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-35000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:49:00,669 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:49:30,068 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:49:30,375 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:49:30,677 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9726, 'learning_rate': 2.1526194267515926e-05, 'epoch': 28.26}       \n",
      " 28%|█████████                       | 35500/125600 [4:00:11<6:29:21,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 07:52:29,539 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-35500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:52:29,853 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:52:50,090 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:52:50,399 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:52:50,711 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-35500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6027, 'learning_rate': 2.1406767515923566e-05, 'epoch': 28.66}       \n",
      " 29%|█████████▏                      | 36000/125600 [4:03:38<6:27:09,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 07:55:56,911 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-36000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:55:57,350 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:56:16,880 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:56:17,159 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:56:17,487 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5043, 'learning_rate': 2.128734076433121e-05, 'epoch': 29.06}        \n",
      " 29%|█████████▎                      | 36500/125600 [4:07:12<6:23:58,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 07:59:30,126 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-36500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 07:59:30,504 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 07:59:51,640 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 07:59:51,969 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 07:59:52,300 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-36500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 29%|█████████▎                      | 36592/125600 [4:09:02<6:23:26,  3.87it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0965, 'learning_rate': 2.10484872611465e-05, 'epoch': 29.86}         \n",
      " 30%|█████████▌                      | 37500/125600 [4:14:07<6:21:21,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 08:06:25,868 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-37500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:06:26,268 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-37500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:06:46,440 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-37500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:06:46,784 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-37500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:06:47,101 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-37500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9389, 'learning_rate': 2.092906050955414e-05, 'epoch': 30.25}        \n",
      " 30%|█████████▋                      | 38000/125600 [4:17:25<6:20:52,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 08:09:43,220 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-38000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:09:43,557 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:10:04,553 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:10:04,902 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:10:05,272 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1853, 'learning_rate': 2.0809633757961783e-05, 'epoch': 30.65}       \n",
      " 31%|█████████▊                      | 38500/125600 [4:20:46<6:17:29,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 08:13:04,088 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-38500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:13:04,473 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:13:23,226 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:13:23,557 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:13:23,874 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-38500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.946, 'learning_rate': 2.0690207006369427e-05, 'epoch': 31.05}        \n",
      " 31%|█████████▉                      | 39000/125600 [4:23:58<6:15:28,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 08:16:16,981 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-39000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:16:17,353 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-39000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:16:41,372 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-39000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:16:41,692 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-39000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:16:42,102 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-39000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 31%|█████████▉                      | 39058/125600 [4:25:25<6:14:33,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 32%|██████████                      | 39642/125600 [4:29:09<6:11:31,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2492, 'learning_rate': 1.9854936305732484e-05, 'epoch': 33.84}       \n",
      " 34%|██████████▊                     | 42500/125600 [4:47:26<5:59:15,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 08:39:44,070 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-42500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:39:44,422 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-42500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:40:12,546 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-42500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:40:12,940 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-42500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:40:13,236 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-42500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7799, 'learning_rate': 1.9735509554140127e-05, 'epoch': 34.24}       \n",
      " 34%|██████████▉                     | 43000/125600 [4:50:53<5:57:30,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 08:43:11,283 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-43000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:43:11,582 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:43:32,253 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:43:32,567 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:43:32,909 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1029, 'learning_rate': 1.9616321656050956e-05, 'epoch': 34.63}       \n",
      " 35%|███████████                     | 43500/125600 [4:54:15<5:53:33,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 08:46:33,241 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-43500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:46:33,547 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:47:00,159 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:47:00,466 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:47:00,826 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-43500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8489, 'learning_rate': 1.94968949044586e-05, 'epoch': 35.03}         \n",
      " 35%|███████████▏                    | 44000/125600 [4:57:40<5:53:25,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 08:49:58,299 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-44000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 08:49:59,638 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-44000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 08:50:20,963 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-44000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 08:50:21,272 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-44000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 08:50:21,585 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-44000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 35%|███████████▏                    | 44093/125600 [4:59:14<5:54:58,  3.83it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 36%|███████████▍                    | 44774/125600 [5:03:18<5:49:34,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9194, 'learning_rate': 1.8661146496815285e-05, 'epoch': 37.82}       \n",
      " 38%|████████████                    | 47500/125600 [5:21:23<5:37:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 09:13:41,789 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-47500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 09:13:42,157 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-47500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 09:14:04,562 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-47500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 09:14:04,943 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-47500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 09:14:05,281 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-47500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9433, 'learning_rate': 1.8541719745222932e-05, 'epoch': 38.22}       \n",
      " 38%|████████████▏                   | 48000/125600 [5:24:45<5:37:04,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 09:17:03,126 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-48000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 09:17:03,465 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 09:17:25,261 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 09:17:25,550 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 09:17:25,856 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9242, 'learning_rate': 1.8422292993630575e-05, 'epoch': 38.61}       \n",
      " 39%|████████████▎                   | 48500/125600 [5:28:04<5:33:25,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 09:20:22,083 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-48500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 09:20:22,436 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 09:20:43,882 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 09:20:44,249 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 09:20:44,580 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-48500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9324, 'learning_rate': 1.8302866242038215e-05, 'epoch': 39.01}       \n",
      " 39%|████████████▍                   | 49000/125600 [5:31:30<5:30:15,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 09:23:48,691 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-49000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 09:23:49,124 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-49000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 09:24:18,269 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-49000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 09:24:18,583 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-49000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 09:24:18,910 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-49000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 39%|████████████▍                   | 49028/125600 [5:32:54<5:32:23,  3.84it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6372, 'learning_rate': 1.758654458598726e-05, 'epoch': 41.4}         \n",
      " 41%|█████████████▏                  | 52000/125600 [5:51:51<5:17:47,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 09:44:09,500 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-52000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 09:44:09,867 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-52000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 09:44:30,530 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-52000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 09:44:30,933 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-52000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 09:44:31,284 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-52000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8598, 'learning_rate': 1.686998407643312e-05, 'epoch': 43.79}        \n",
      " 44%|██████████████                  | 55000/125600 [6:11:48<5:04:52,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 10:04:06,398 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-55000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:04:06,753 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:04:29,970 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:04:30,341 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:04:30,762 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8612, 'learning_rate': 1.6750557324840765e-05, 'epoch': 44.19}       \n",
      " 44%|██████████████▏                 | 55500/125600 [6:15:06<5:02:15,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 10:07:24,877 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-55500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:07:25,244 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:07:46,069 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:07:46,368 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:07:46,661 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-55500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7965, 'learning_rate': 1.6631369426751594e-05, 'epoch': 44.59}       \n",
      " 45%|██████████████▎                 | 56000/125600 [6:18:23<5:00:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 10:10:41,710 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-56000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:10:42,263 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:11:13,754 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:11:14,047 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:11:14,401 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8743, 'learning_rate': 1.6511942675159237e-05, 'epoch': 44.98}       \n",
      " 45%|██████████████▍                 | 56500/125600 [6:22:12<4:59:51,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 10:14:30,953 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-56500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:14:31,345 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:14:51,742 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:14:52,089 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:14:52,419 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-56500/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:18:09,629 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:18:09,997 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:18:10,278 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8212, 'learning_rate': 1.627308917197452e-05, 'epoch': 45.78}        \n",
      " 46%|██████████████▋                 | 57500/125600 [6:28:53<4:54:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 10:21:11,516 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-57500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:21:11,829 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:21:33,145 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:21:33,451 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:21:33,755 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-57500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7475, 'learning_rate': 1.6153662420382167e-05, 'epoch': 46.18}       \n",
      " 46%|██████████████▊                 | 58000/125600 [6:32:11<4:54:08,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 10:24:29,335 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-58000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:24:29,640 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:24:50,471 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:24:50,754 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:24:51,107 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5722, 'learning_rate': 1.603423566878981e-05, 'epoch': 46.58}        \n",
      " 47%|██████████████▉                 | 58500/125600 [6:35:30<4:49:17,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 10:27:48,545 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-58500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:27:48,931 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:28:10,202 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:28:10,540 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:28:10,906 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-58500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7929, 'learning_rate': 1.591480891719745e-05, 'epoch': 46.97}        \n",
      " 47%|███████████████                 | 59000/125600 [6:38:55<4:47:28,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 10:31:13,549 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-59000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:31:13,887 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:31:33,024 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:31:33,304 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:31:33,644 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59000/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:34:51,536 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:34:51,853 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:34:52,192 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-59500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6686, 'learning_rate': 1.5676194267515923e-05, 'epoch': 47.77}       \n",
      " 48%|███████████████▎                | 60000/125600 [6:45:31<4:42:37,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 10:37:49,683 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-60000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:37:50,004 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-60000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:38:09,043 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-60000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:38:09,447 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:38:09,839 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-60000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 48%|███████████████▎                | 60315/125600 [6:47:58<4:41:05,  3.87it/s][INFO|modeling_utils.py:1624] 2022-11-14 10:51:21,332 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:51:21,676 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:51:22,009 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6225, 'learning_rate': 1.5079299363057326e-05, 'epoch': 49.76}       \n",
      " 50%|███████████████▉                | 62500/125600 [7:01:58<4:32:23,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 10:54:16,848 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-62500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:54:17,315 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:54:38,898 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:54:39,194 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:54:39,510 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-62500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8492, 'learning_rate': 1.4959872611464969e-05, 'epoch': 50.16}       \n",
      " 50%|████████████████                | 63000/125600 [7:05:18<4:30:24,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 10:57:36,767 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-63000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 10:57:37,084 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 10:57:58,912 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 10:57:59,195 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 10:57:59,541 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.948, 'learning_rate': 1.4840445859872612e-05, 'epoch': 50.56}        \n",
      " 51%|████████████████▏               | 63500/125600 [7:08:39<4:28:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 11:00:57,449 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-63500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:00:57,761 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:01:19,577 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:01:19,909 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:01:20,375 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-63500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4926, 'learning_rate': 1.472125796178344e-05, 'epoch': 50.96}        \n",
      " 51%|████████████████▎               | 64000/125600 [7:12:00<4:26:48,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 11:04:18,213 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-64000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:04:18,490 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-64000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:04:39,300 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-64000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:04:39,653 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-64000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:04:39,979 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-64000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.672, 'learning_rate': 1.4482404458598726e-05, 'epoch': 51.75}        \n",
      " 52%|████████████████▌               | 65000/125600 [7:18:37<4:22:11,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 11:10:55,274 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-65000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:10:55,714 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:11:16,594 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:11:16,914 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:11:17,228 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7793, 'learning_rate': 1.436297770700637e-05, 'epoch': 52.15}        \n",
      " 52%|████████████████▋               | 65500/125600 [7:21:56<4:19:25,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 11:14:14,505 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-65500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:14:14,820 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:14:34,641 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:14:34,939 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:14:35,239 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-65500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6569, 'learning_rate': 1.4243789808917199e-05, 'epoch': 52.55}       \n",
      " 53%|████████████████▊               | 66000/125600 [7:25:13<4:17:18,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 11:17:31,846 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-66000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:17:32,202 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:17:52,536 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:17:52,846 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:17:53,160 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2555, 'learning_rate': 1.412436305732484e-05, 'epoch': 52.95}        \n",
      " 53%|████████████████▉               | 66500/125600 [7:28:42<4:16:54,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 11:21:00,968 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-66500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:21:01,322 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:21:22,527 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:21:22,847 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:21:23,154 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-66500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0117, 'learning_rate': 1.3288853503184714e-05, 'epoch': 55.73}       \n",
      " 56%|█████████████████▊              | 70000/125600 [7:52:01<4:00:04,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 11:44:19,688 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-70000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 11:44:20,040 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-70000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 11:44:40,721 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-70000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 11:44:41,036 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-70000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 11:44:41,453 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-70000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5688, 'learning_rate': 1.2691958598726116e-05, 'epoch': 57.72}       \n",
      " 58%|██████████████████▍             | 72500/125600 [8:08:47<3:49:25,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 12:01:05,300 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-72500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:01:05,636 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-72500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:01:25,927 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-72500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:01:27,109 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-72500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:01:27,449 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-72500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6947, 'learning_rate': 1.2572531847133758e-05, 'epoch': 58.12}       \n",
      " 58%|██████████████████▌             | 73000/125600 [8:12:06<3:47:10,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 12:04:24,501 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-73000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:04:24,840 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:04:45,578 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:04:45,885 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:04:46,253 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5709, 'learning_rate': 1.2453105095541403e-05, 'epoch': 58.52}       \n",
      " 59%|██████████████████▋             | 73500/125600 [8:15:26<3:44:45,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 12:07:44,333 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-73500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:07:44,713 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:08:03,991 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:08:04,321 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:08:04,653 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-73500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8282, 'learning_rate': 1.2333678343949045e-05, 'epoch': 58.92}       \n",
      " 59%|██████████████████▊             | 74000/125600 [8:18:46<3:43:46,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 12:11:04,190 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-74000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:11:04,521 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-74000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:11:25,330 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-74000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:11:25,673 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-74000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:11:25,988 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-74000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6447, 'learning_rate': 1.221425159235669e-05, 'epoch': 59.32}        \n",
      " 59%|██████████████████▉             | 74500/125600 [8:22:04<3:41:08,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 12:14:22,970 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-74500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:14:23,232 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-74500/config.json\n",
      "{'loss': 2.1066, 'learning_rate': 1.2095063694267515e-05, 'epoch': 59.71}       \n",
      " 60%|███████████████████             | 75000/125600 [8:25:27<3:38:12,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 12:17:45,409 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-75000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:17:45,711 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:18:07,121 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:18:07,491 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:18:07,941 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7348, 'learning_rate': 1.197563694267516e-05, 'epoch': 60.11}        \n",
      " 60%|███████████████████▏            | 75500/125600 [8:28:43<3:39:26,  3.81it/s][INFO|trainer.py:2671] 2022-11-14 12:21:01,894 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-75500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:21:02,240 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:21:21,651 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:21:22,171 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:21:22,506 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-75500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5, 'learning_rate': 1.1856210191082802e-05, 'epoch': 60.51}          \n",
      " 61%|███████████████████▎            | 76000/125600 [8:32:01<3:34:38,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 12:24:19,266 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-76000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:24:19,580 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:24:40,846 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:24:41,203 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:24:41,512 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8146, 'learning_rate': 1.1736783439490447e-05, 'epoch': 60.91}       \n",
      " 61%|███████████████████▍            | 76500/125600 [8:35:21<3:34:11,  3.82it/s][INFO|trainer.py:2671] 2022-11-14 12:27:39,967 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-76500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:27:40,308 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:28:02,850 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:28:03,219 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:28:03,553 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-76500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7271, 'learning_rate': 1.1617356687898089e-05, 'epoch': 61.31}       \n",
      " 61%|███████████████████▌            | 77000/125600 [8:38:43<3:34:09,  3.78it/s][INFO|trainer.py:2671] 2022-11-14 12:31:01,960 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-77000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 12:31:02,644 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 12:31:23,027 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:31:23,356 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:31:23,702 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 61%|███████████████████▌            | 77013/125600 [8:39:55<7:20:45,  1.84it/s][INFO|modeling_utils.py:1624] 2022-11-14 12:34:43,343 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:34:43,635 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:34:43,936 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-77500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 62%|███████████████████▊            | 77749/125600 [8:44:17<3:27:25,  3.84it/s][INFO|modeling_utils.py:1624] 2022-11-14 12:51:47,358 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-80000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 12:51:47,704 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 12:51:48,013 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-80000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4319, 'learning_rate': 1.0304140127388535e-05, 'epoch': 65.68}       \n",
      " 66%|█████████████████████           | 82500/125600 [9:16:10<3:05:43,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 13:08:28,167 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-82500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:08:28,558 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-82500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:08:48,673 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-82500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:08:49,008 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-82500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:08:49,347 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-82500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.78, 'learning_rate': 1.0185191082802548e-05, 'epoch': 66.08}         \n",
      " 66%|█████████████████████▏          | 83000/125600 [9:19:26<3:03:39,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 13:11:44,711 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-83000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:11:45,087 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:12:05,290 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:12:05,649 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:12:05,947 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5097, 'learning_rate': 1.0065764331210192e-05, 'epoch': 66.48}       \n",
      " 66%|█████████████████████▎          | 83500/125600 [9:22:42<3:01:34,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 13:15:00,868 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-83500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:15:01,255 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:15:23,384 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:15:23,731 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:15:24,052 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-83500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6796, 'learning_rate': 9.946337579617835e-06, 'epoch': 66.88}        \n",
      " 67%|█████████████████████▍          | 84000/125600 [9:26:03<2:59:30,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 13:18:21,492 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-84000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:18:21,857 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:18:42,084 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:18:42,440 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:18:42,748 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7987, 'learning_rate': 9.826910828025479e-06, 'epoch': 67.28}        \n",
      " 67%|█████████████████████▌          | 84500/125600 [9:29:21<2:57:09,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 13:21:40,194 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-84500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:21:40,559 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:22:03,942 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:22:04,278 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:22:04,628 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-84500/special_tokens_map.json\n",
      "{'loss': 1.5529, 'learning_rate': 9.707484076433122e-06, 'epoch': 67.68}        \n",
      " 68%|█████████████████████▋          | 85000/125600 [9:32:50<2:55:14,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 13:25:08,869 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-85000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:25:09,209 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-85000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:25:28,373 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-85000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:25:28,700 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-85000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:25:29,093 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-85000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7112, 'learning_rate': 9.110589171974523e-06, 'epoch': 69.67}        \n",
      " 70%|██████████████████████▎         | 87500/125600 [9:49:49<2:44:18,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 13:42:07,630 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-87500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:42:08,027 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-87500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:42:29,544 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-87500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:42:29,917 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-87500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:42:30,217 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-87500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6018, 'learning_rate': 8.513694267515923e-06, 'epoch': 71.66}        \n",
      " 72%|██████████████████████▏        | 90000/125600 [10:06:33<2:33:19,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 13:58:51,325 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-90000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 13:58:51,619 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 13:59:10,748 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 13:59:11,094 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 13:59:11,433 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4916, 'learning_rate': 8.394267515923567e-06, 'epoch': 72.05}        \n",
      " 72%|██████████████████████▎        | 90500/125600 [10:09:50<2:31:24,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 14:02:08,713 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-90500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:02:09,073 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:02:38,060 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:02:38,425 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:02:38,766 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-90500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8035, 'learning_rate': 8.27484076433121e-06, 'epoch': 72.45}         \n",
      " 72%|██████████████████████▍        | 91000/125600 [10:13:26<2:29:51,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:05:44,055 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-91000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:05:44,441 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:06:06,874 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:06:07,279 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:06:07,627 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3387, 'learning_rate': 8.155414012738854e-06, 'epoch': 72.85}        \n",
      " 73%|██████████████████████▌        | 91500/125600 [10:16:47<2:28:05,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 14:09:05,575 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-91500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:09:05,903 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:09:25,661 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:09:26,055 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:09:26,460 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-91500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5534, 'learning_rate': 7.91656050955414e-06, 'epoch': 73.65}         \n",
      " 74%|██████████████████████▊        | 92500/125600 [10:23:33<2:23:15,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:15:51,892 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-92500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:15:52,251 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-92500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:16:12,710 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-92500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:16:13,050 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-92500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:16:13,388 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-92500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3926, 'learning_rate': 7.797133757961784e-06, 'epoch': 74.04}        \n",
      " 74%|██████████████████████▉        | 93000/125600 [10:26:51<2:21:26,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 14:19:09,902 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-93000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:19:10,220 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:19:32,510 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:19:32,857 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:19:33,183 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5332, 'learning_rate': 7.677707006369427e-06, 'epoch': 74.44}        \n",
      " 74%|███████████████████████        | 93500/125600 [10:30:12<2:18:57,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:22:30,448 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-93500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:22:30,790 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:22:54,756 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:22:55,085 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:22:55,396 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-93500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.662, 'learning_rate': 7.319665605095542e-06, 'epoch': 75.64}         \n",
      " 76%|███████████████████████▍       | 95000/125600 [10:40:16<2:11:55,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 14:32:34,545 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-95000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:32:34,832 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:32:56,414 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:32:56,727 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:32:57,042 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6435, 'learning_rate': 7.200238853503185e-06, 'epoch': 76.04}        \n",
      " 76%|███████████████████████▌       | 95500/125600 [10:43:42<2:10:21,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:36:00,944 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-95500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:36:01,254 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:36:26,194 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:36:26,558 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:36:26,939 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-95500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5282, 'learning_rate': 7.080812101910829e-06, 'epoch': 76.43}        \n",
      " 76%|███████████████████████▋       | 96000/125600 [10:47:05<2:08:00,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:39:23,821 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-96000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:39:24,163 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:39:48,577 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:39:48,888 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:39:49,179 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6214, 'learning_rate': 6.961385350318472e-06, 'epoch': 76.83}        \n",
      " 77%|███████████████████████▊       | 96500/125600 [10:50:29<2:05:36,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 14:42:47,100 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-96500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:42:47,461 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:43:07,782 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:43:08,102 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:43:08,443 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-96500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.2999, 'learning_rate': 6.842197452229299e-06, 'epoch': 77.23}        \n",
      " 77%|███████████████████████▉       | 97000/125600 [10:53:53<2:03:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 14:46:11,297 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-97000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:46:11,611 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:46:31,302 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:46:31,606 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:46:31,954 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4861, 'learning_rate': 6.722770700636943e-06, 'epoch': 77.63}        \n",
      " 78%|████████████████████████       | 97500/125600 [10:57:11<2:01:12,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 14:49:29,917 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-97500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:49:30,203 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:49:49,677 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:49:49,956 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:49:50,270 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-97500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.493, 'learning_rate': 6.603343949044586e-06, 'epoch': 78.03}         \n",
      " 78%|████████████████████████▏      | 98000/125600 [11:00:29<1:59:19,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 14:52:47,428 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-98000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:52:48,502 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:53:09,054 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:53:09,365 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:53:09,677 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5782, 'learning_rate': 6.484156050955414e-06, 'epoch': 78.42}        \n",
      " 78%|████████████████████████▎      | 98500/125600 [11:03:49<1:57:12,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:56:07,737 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-98500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:56:08,069 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:56:29,519 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:56:29,921 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:56:30,238 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-98500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6043, 'learning_rate': 6.364729299363058e-06, 'epoch': 78.82}        \n",
      " 79%|████████████████████████▍      | 99000/125600 [11:07:10<1:55:00,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 14:59:28,218 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-99000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 14:59:28,556 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 14:59:49,402 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 14:59:49,764 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 14:59:50,107 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7383, 'learning_rate': 6.245302547770701e-06, 'epoch': 79.22}        \n",
      " 79%|████████████████████████▌      | 99500/125600 [11:10:31<1:53:11,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 15:02:49,897 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-99500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:02:50,186 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:03:23,499 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:03:23,871 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:03:24,176 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-99500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5453, 'learning_rate': 6.1258757961783444e-06, 'epoch': 79.62}       \n",
      " 80%|███████████████████████▉      | 100000/125600 [11:14:03<1:51:20,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 15:06:21,139 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-100000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:06:21,493 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:06:42,154 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:06:42,513 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:06:42,806 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6063, 'learning_rate': 6.006449044585987e-06, 'epoch': 80.02}        \n",
      " 80%|████████████████████████      | 100500/125600 [11:17:24<1:48:33,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 15:09:42,080 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-100500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:09:42,399 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:10:02,578 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:10:02,939 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:10:03,265 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-100500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5471, 'learning_rate': 5.88702229299363e-06, 'epoch': 80.41}         \n",
      " 80%|████████████████████████      | 101000/125600 [11:20:42<1:46:35,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 15:13:00,252 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-101000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:13:00,567 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:13:21,454 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:13:21,745 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:13:22,106 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6526, 'learning_rate': 5.767595541401274e-06, 'epoch': 80.81}        \n",
      " 81%|████████████████████████▏     | 101500/125600 [11:24:05<1:44:07,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 15:16:23,730 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-101500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:16:24,067 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:16:44,281 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:16:44,587 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:16:44,885 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-101500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 81%|████████████████████████▎     | 101648/125600 [11:25:52<1:43:30,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3771, 'learning_rate': 5.528980891719745e-06, 'epoch': 81.61}        \n",
      " 82%|████████████████████████▍     | 102500/125600 [11:30:45<1:40:29,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 15:23:03,382 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-102500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:23:03,765 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-102500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:23:26,924 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-102500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:23:27,508 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-102500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:23:27,832 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-102500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5052, 'learning_rate': 5.409554140127389e-06, 'epoch': 82.01}        \n",
      " 82%|████████████████████████▌     | 103000/125600 [11:34:07<1:37:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 15:26:25,228 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-103000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:26:25,538 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:26:44,599 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:26:44,919 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:26:45,265 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7024, 'learning_rate': 5.290127388535032e-06, 'epoch': 82.4}         \n",
      " 82%|████████████████████████▋     | 103500/125600 [11:37:24<1:35:57,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 15:29:42,342 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-103500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:29:42,623 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:30:03,536 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:30:03,842 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:30:04,200 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-103500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6985, 'learning_rate': 5.170700636942675e-06, 'epoch': 82.8}         \n",
      " 83%|████████████████████████▊     | 104000/125600 [11:40:45<1:34:02,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 15:33:03,508 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-104000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:33:03,800 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:33:30,828 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:33:31,147 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:33:31,545 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5487, 'learning_rate': 5.051512738853503e-06, 'epoch': 83.2}         \n",
      " 83%|████████████████████████▉     | 104500/125600 [11:44:09<1:31:12,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 15:36:27,372 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-104500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:36:27,749 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:36:48,534 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:36:48,838 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:36:49,163 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-104500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3429, 'learning_rate': 4.932085987261146e-06, 'epoch': 83.6}         \n",
      " 84%|█████████████████████████     | 105000/125600 [11:47:27<1:28:59,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 15:39:45,872 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-105000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:39:46,168 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:40:05,631 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:40:05,937 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:40:06,271 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6937, 'learning_rate': 4.812659235668789e-06, 'epoch': 84.0}         \n",
      " 84%|█████████████████████████▏    | 105500/125600 [11:50:45<1:27:00,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 15:43:03,411 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-105500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:43:03,718 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:43:25,017 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:43:25,308 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:43:25,712 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-105500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5639, 'learning_rate': 4.6934713375796184e-06, 'epoch': 84.39}       \n",
      " 84%|█████████████████████████▎    | 106000/125600 [11:54:05<1:25:10,  3.83it/s][INFO|trainer.py:2671] 2022-11-14 15:46:23,147 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-106000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:46:23,554 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:46:43,864 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:46:44,131 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:46:44,434 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5466, 'learning_rate': 4.574044585987262e-06, 'epoch': 84.79}        \n",
      " 85%|█████████████████████████▍    | 106500/125600 [11:57:22<1:22:25,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 15:49:40,098 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-106500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:49:40,390 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:50:09,051 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:50:09,373 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:50:09,681 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-106500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4726, 'learning_rate': 4.454617834394905e-06, 'epoch': 85.19}        \n",
      " 85%|█████████████████████████▌    | 107000/125600 [12:00:46<1:20:17,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 15:53:04,834 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-107000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:53:05,198 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:53:26,109 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:53:26,448 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:53:26,821 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5633, 'learning_rate': 4.3351910828025485e-06, 'epoch': 85.59}       \n",
      " 86%|█████████████████████████▋    | 107500/125600 [12:04:05<1:18:24,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 15:56:23,565 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-107500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:56:23,859 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 15:56:43,016 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 15:56:43,332 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 15:56:43,668 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-107500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5687, 'learning_rate': 4.215764331210192e-06, 'epoch': 85.99}        \n",
      " 86%|█████████████████████████▊    | 108000/125600 [12:07:21<1:15:55,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 15:59:39,863 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-108000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 15:59:40,286 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:00:01,971 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:00:02,372 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:00:02,746 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.2633, 'learning_rate': 4.096337579617834e-06, 'epoch': 86.39}        \n",
      " 86%|█████████████████████████▉    | 108500/125600 [12:10:43<1:15:01,  3.80it/s][INFO|trainer.py:2671] 2022-11-14 16:03:01,690 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-108500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:03:02,029 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:03:23,095 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:03:23,477 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:03:23,806 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-108500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5035, 'learning_rate': 3.976910828025478e-06, 'epoch': 86.78}        \n",
      " 87%|██████████████████████████    | 109000/125600 [12:14:02<1:12:03,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 16:06:20,069 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-109000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:06:20,471 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:06:41,189 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:06:41,504 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:06:41,864 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6182, 'learning_rate': 3.857484076433121e-06, 'epoch': 87.18}        \n",
      " 87%|██████████████████████████▏   | 109500/125600 [12:17:23<1:09:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 16:09:41,417 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-109500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:09:41,786 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:10:08,242 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:10:08,571 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:10:08,888 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-109500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6295, 'learning_rate': 3.7380573248407645e-06, 'epoch': 87.58}       \n",
      " 88%|██████████████████████████▎   | 110000/125600 [12:20:53<1:07:35,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 16:13:11,154 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-110000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:13:11,444 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:13:31,142 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:13:31,461 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:13:31,740 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5837, 'learning_rate': 3.618630573248408e-06, 'epoch': 87.98}        \n",
      " 88%|██████████████████████████▍   | 110500/125600 [12:24:10<1:05:09,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 16:16:28,631 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-110500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:16:28,963 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:16:49,433 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:16:49,692 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:16:49,986 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-110500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3664, 'learning_rate': 3.4992038216560512e-06, 'epoch': 88.38}       \n",
      " 88%|██████████████████████████▌   | 111000/125600 [12:27:37<1:02:54,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 16:19:55,415 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-111000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:19:55,718 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:20:17,822 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:20:18,173 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:20:18,474 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6751, 'learning_rate': 3.3797770700636946e-06, 'epoch': 88.77}       \n",
      " 89%|██████████████████████████▋   | 111500/125600 [12:30:57<1:01:16,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 16:23:15,749 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-111500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:23:16,060 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:23:36,839 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:23:37,155 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:23:37,558 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-111500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6508, 'learning_rate': 2.544267515923567e-06, 'epoch': 91.56}        \n",
      " 92%|█████████████████████████████▎  | 115000/125600 [12:54:42<45:54,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 16:47:00,279 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-115000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 16:47:00,668 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-115000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 16:47:20,667 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-115000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 16:47:21,041 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-115000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 16:47:21,392 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-115000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4956, 'learning_rate': 1.947611464968153e-06, 'epoch': 93.55}        \n",
      " 94%|█████████████████████████████▉  | 117500/125600 [13:11:33<34:59,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:03:51,599 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-117500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:03:51,970 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-117500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:04:13,139 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-117500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:04:13,489 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-117500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:04:13,801 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-117500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5299, 'learning_rate': 1.8281847133757964e-06, 'epoch': 93.95}       \n",
      " 94%|██████████████████████████████  | 118000/125600 [13:14:55<32:47,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:07:13,180 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-118000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:07:13,555 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:07:33,159 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:07:33,490 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:07:33,832 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5027, 'learning_rate': 1.7087579617834395e-06, 'epoch': 94.35}       \n",
      " 94%|██████████████████████████████▏ | 118500/125600 [13:18:12<30:35,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 17:10:30,605 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-118500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:10:30,969 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:10:55,872 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:10:56,227 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:10:56,936 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-118500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5207, 'learning_rate': 1.5893312101910827e-06, 'epoch': 94.75}       \n",
      " 95%|██████████████████████████████▎ | 119000/125600 [13:21:43<28:55,  3.80it/s][INFO|trainer.py:2671] 2022-11-14 17:14:01,826 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-119000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:14:02,195 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:14:22,975 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:14:23,283 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:14:23,628 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3726, 'learning_rate': 1.469904458598726e-06, 'epoch': 95.14}        \n",
      " 95%|██████████████████████████████▍ | 119500/125600 [13:25:08<26:19,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:17:26,081 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-119500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:17:26,411 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:17:48,030 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:17:48,344 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:17:48,715 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-119500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3402, 'learning_rate': 1.3504777070063694e-06, 'epoch': 95.54}       \n",
      " 96%|██████████████████████████████▌ | 120000/125600 [13:28:26<24:09,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:20:44,675 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-120000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:20:45,005 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:21:07,238 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:21:07,549 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:21:07,847 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7752, 'learning_rate': 1.2312898089171974e-06, 'epoch': 95.94}       \n",
      " 96%|██████████████████████████████▋ | 120500/125600 [13:31:47<22:07,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 17:24:05,640 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-120500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:24:06,017 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:24:25,901 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:24:26,214 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:24:26,566 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-120500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6895, 'learning_rate': 1.1121019108280256e-06, 'epoch': 96.34}       \n",
      " 96%|██████████████████████████████▊ | 121000/125600 [13:35:04<19:50,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:27:23,001 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-121000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:27:23,349 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:27:43,447 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:27:43,779 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:27:44,136 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.633, 'learning_rate': 9.926751592356687e-07, 'epoch': 96.74}         \n",
      " 97%|██████████████████████████████▉ | 121500/125600 [13:38:23<17:41,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:30:41,468 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-121500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:30:41,786 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:31:03,122 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:31:03,550 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:31:03,869 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-121500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6033, 'learning_rate': 8.732484076433121e-07, 'epoch': 97.13}        \n",
      " 97%|███████████████████████████████ | 122000/125600 [13:41:43<15:32,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:34:01,328 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-122000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:34:01,817 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:34:25,804 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:34:26,122 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:34:26,428 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3726, 'learning_rate': 7.538216560509554e-07, 'epoch': 97.53}        \n",
      " 98%|███████████████████████████████▏| 122500/125600 [13:45:07<13:22,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:37:25,517 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-122500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:37:25,899 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:37:46,849 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:37:47,171 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:37:47,517 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-122500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3725, 'learning_rate': 6.343949044585987e-07, 'epoch': 97.93}        \n",
      " 98%|███████████████████████████████▎| 123000/125600 [13:48:25<11:17,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 17:40:43,735 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-123000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:40:44,068 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:41:04,780 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:41:05,113 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:41:05,419 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5944, 'learning_rate': 5.149681528662421e-07, 'epoch': 98.33}        \n",
      " 98%|███████████████████████████████▍| 123500/125600 [13:51:42<09:04,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:44:00,548 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-123500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:44:00,908 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:44:22,870 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:44:23,196 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:44:23,522 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-123500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.466, 'learning_rate': 3.9554140127388536e-07, 'epoch': 98.73}        \n",
      " 99%|███████████████████████████████▌| 124000/125600 [13:55:12<06:54,  3.86it/s][INFO|trainer.py:2671] 2022-11-14 17:47:30,741 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-124000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:47:31,091 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:47:51,337 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:47:51,641 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:47:52,013 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.421, 'learning_rate': 2.7611464968152867e-07, 'epoch': 99.12}        \n",
      " 99%|███████████████████████████████▋| 124500/125600 [13:58:30<04:44,  3.87it/s][INFO|trainer.py:2671] 2022-11-14 17:50:48,513 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-124500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:50:48,876 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:51:10,718 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:51:11,082 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:51:11,398 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-124500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6345, 'learning_rate': 1.569267515923567e-07, 'epoch': 99.52}        \n",
      "100%|███████████████████████████████▊| 125000/125600 [14:01:49<02:36,  3.84it/s][INFO|trainer.py:2671] 2022-11-14 17:54:07,089 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-125000\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:54:07,433 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:54:32,406 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:54:32,736 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:54:33,069 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5021, 'learning_rate': 3.7500000000000005e-08, 'epoch': 99.92}       \n",
      "100%|███████████████████████████████▉| 125500/125600 [14:05:13<00:25,  3.85it/s][INFO|trainer.py:2671] 2022-11-14 17:57:31,407 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/checkpoint-125500\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:57:31,909 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:57:52,578 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:57:52,955 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:57:53,313 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/checkpoint-125500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|████████████████████████████████| 125600/125600 [14:06:57<00:00,  3.86it/s][INFO|trainer.py:1852] 2022-11-14 17:59:15,055 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 50817.2182, 'train_samples_per_second': 14.83, 'train_steps_per_second': 2.472, 'train_loss': 1.9559951608499904, 'epoch': 100.0}\n",
      "100%|████████████████████████████████| 125600/125600 [14:06:57<00:00,  2.47it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-14 17:59:15,198 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train06/\n",
      "[INFO|configuration_utils.py:447] 2022-11-14 17:59:15,559 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train06/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-14 17:59:45,185 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train06/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-14 17:59:45,559 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train06/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-14 17:59:45,890 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train06/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       100.0\n",
      "  train_loss               =       1.956\n",
      "  train_runtime            = 14:06:57.21\n",
      "  train_samples            =        7536\n",
      "  train_samples_per_second =       14.83\n",
      "  train_steps_per_second   =       2.472\n",
      "11/14/2022 17:59:47 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-14 17:59:47,996 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-14 17:59:48,015 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-14 17:59:48,016 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-14 17:59:48,016 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:34<00:00, 11.51it/s]11/14/2022 18:00:25 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                       | 12/3047 [00:00<00:25, 118.18it/s]\u001b[A\n",
      "  2%|▊                                       | 61/3047 [00:00<00:08, 334.25it/s]\u001b[A\n",
      "  4%|█▍                                     | 112/3047 [00:00<00:07, 411.03it/s]\u001b[A\n",
      "  5%|██                                     | 163/3047 [00:00<00:06, 447.83it/s]\u001b[A\n",
      "  7%|██▋                                    | 214/3047 [00:00<00:06, 466.55it/s]\u001b[A\n",
      " 10%|███▊                                   | 294/3047 [00:00<00:06, 440.14it/s]\u001b[A\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 681, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 638, in main\n",
      "    metrics = trainer.evaluate()\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 57, in evaluate\n",
      "    eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
      "  File \"run_qa.py\", line 584, in post_processing_function\n",
      "    prefix=stage,\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/utils_qa.py\", line 204, in postprocess_qa_predictions\n",
      "    while predictions[i][\"text\"] == \"\":\n",
      "IndexError: list index out of range\n",
      "100%|█████████████████████████████████████████| 404/404 [00:38<00:00, 10.57it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 100 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train06/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "5d509392-38f3-44ae-a7fb-9b346b4b00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train07/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "e1f4aff1-ffc1-4eb6-b68a-48b46e47224d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/15/2022 14:54:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/15/2022 14:54:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train07/runs/Nov15_14-54-53_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=100.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train07/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train07/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/15/2022 14:54:54 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/15/2022 14:54:54 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/15/2022 14:54:54 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/15/2022 14:54:54 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/15/2022 14:54:54 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/15/2022 14:54:54 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 665.18it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-15 14:54:54,539 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-15 14:54:54,545 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-15 14:54:54,708 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-15 14:54:54,708 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-15 14:54:54,708 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-15 14:54:54,708 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-15 14:54:55,425 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-15 14:55:08,494 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-15 14:55:08,494 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/15/2022 14:55:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/15/2022 14:55:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-15 14:55:12,267 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-15 14:55:12,279 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-15 14:55:12,280 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-15 14:55:12,280 >>   Num Epochs = 100\n",
      "[INFO|trainer.py:1610] 2022-11-15 14:55:12,280 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-15 14:55:12,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-15 14:55:12,280 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-15 14:55:12,280 >>   Total optimization steps = 125600\n",
      "  0%|                                                | 0/125600 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.6975, 'learning_rate': 2.988296178343949e-05, 'epoch': 0.4}          \n",
      "  0%|▏                                   | 500/125600 [02:10<9:04:09,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 14:57:22,488 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 14:57:22,875 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 14:57:45,262 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 14:57:45,580 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 14:57:45,890 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.413, 'learning_rate': 2.9763535031847134e-05, 'epoch': 0.8}          \n",
      "  1%|▎                                  | 1000/125600 [05:52<9:05:46,  3.81it/s][INFO|trainer.py:2671] 2022-11-15 15:01:05,461 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:01:05,818 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:01:27,813 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:01:28,155 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:01:28,527 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5707, 'learning_rate': 2.964410828025478e-05, 'epoch': 1.19}         \n",
      "  1%|▍                                  | 1500/125600 [09:20<8:57:56,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 15:04:32,585 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:04:33,202 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:04:53,840 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:04:54,254 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:04:54,590 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4966, 'learning_rate': 2.952468152866242e-05, 'epoch': 1.59}         \n",
      "  2%|▌                                  | 2000/125600 [12:51<9:01:10,  3.81it/s][INFO|trainer.py:2671] 2022-11-15 15:08:03,754 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:08:04,097 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:08:24,997 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:08:25,363 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:08:25,657 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.7477, 'learning_rate': 2.9405254777070064e-05, 'epoch': 1.99}        \n",
      "  2%|▋                                  | 2500/125600 [16:15<8:53:43,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 15:11:28,106 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:11:28,444 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:11:48,660 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:11:49,042 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:11:49,377 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4618, 'learning_rate': 2.9285828025477707e-05, 'epoch': 2.39}        \n",
      "  2%|▊                                  | 3000/125600 [19:39<8:51:05,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:14:51,900 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:14:52,277 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:15:13,300 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:15:13,753 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:15:14,097 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4774, 'learning_rate': 2.916640127388535e-05, 'epoch': 2.79}         \n",
      "  3%|▉                                  | 3500/125600 [23:06<8:48:05,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:18:19,403 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:18:19,835 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:18:46,454 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:18:46,790 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:18:47,170 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4214, 'learning_rate': 2.9046974522292994e-05, 'epoch': 3.18}        \n",
      "  3%|█                                  | 4000/125600 [26:38<8:46:22,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:21:50,630 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:21:51,002 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:22:12,763 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:22:13,085 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:22:13,368 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5773, 'learning_rate': 2.8927786624203823e-05, 'epoch': 3.58}        \n",
      "  4%|█▎                                 | 4500/125600 [30:04<8:42:59,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 15:25:16,703 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-4500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:25:17,043 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:25:38,284 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:25:38,679 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:25:39,042 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.2549, 'learning_rate': 2.8808359872611467e-05, 'epoch': 3.98}        \n",
      "  4%|█▍                                 | 5000/125600 [33:26<8:41:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:28:38,712 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-5000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:28:39,033 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:28:59,766 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:29:00,082 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:29:00,517 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4922, 'learning_rate': 2.868893312101911e-05, 'epoch': 4.38}         \n",
      "  4%|█▌                                 | 5500/125600 [37:02<8:40:01,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:32:14,627 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-5500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:32:14,962 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:32:36,057 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:32:36,467 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:32:36,848 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-5500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6262, 'learning_rate': 2.856950636942675e-05, 'epoch': 4.78}         \n",
      "  5%|█▋                                 | 6000/125600 [40:28<8:37:12,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:35:41,310 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-6000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:35:41,667 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:36:02,075 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:36:02,439 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:36:02,952 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5596, 'learning_rate': 2.845031847133758e-05, 'epoch': 5.18}         \n",
      "  5%|█▊                                 | 6500/125600 [43:54<8:38:03,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 15:39:07,302 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-6500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:39:07,659 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:39:32,394 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:39:32,771 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:39:33,129 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-6500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.7913, 'learning_rate': 2.8331130573248408e-05, 'epoch': 5.57}        \n",
      "  6%|█▉                                 | 7000/125600 [47:20<8:32:54,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:42:32,964 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-7000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:42:33,292 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:42:54,215 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:42:54,551 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:42:54,904 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0112, 'learning_rate': 2.821170382165605e-05, 'epoch': 5.97}         \n",
      "  6%|██                                 | 7500/125600 [50:41<8:30:38,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:45:54,035 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-7500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:45:54,314 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:46:15,599 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:46:15,938 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:46:16,314 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-7500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6159, 'learning_rate': 2.8092277070063698e-05, 'epoch': 6.37}        \n",
      "  6%|██▏                                | 8000/125600 [54:06<8:28:11,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 15:49:18,684 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-8000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:49:19,083 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:49:41,538 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:49:41,865 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:49:42,248 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.9984, 'learning_rate': 2.7972850318471338e-05, 'epoch': 6.77}        \n",
      "  7%|██▎                                | 8500/125600 [57:30<8:27:31,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:52:43,230 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-8500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:52:43,632 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:53:04,293 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:53:04,586 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:53:04,920 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-8500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6099, 'learning_rate': 2.785342356687898e-05, 'epoch': 7.17}         \n",
      "  7%|██▎                              | 9000/125600 [1:00:47<8:24:01,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 15:55:59,977 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-9000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:56:00,391 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:56:22,699 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 15:56:23,006 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 15:56:23,435 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5667, 'learning_rate': 2.7733996815286625e-05, 'epoch': 7.56}        \n",
      "  8%|██▍                              | 9500/125600 [1:04:13<8:22:20,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 15:59:25,694 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-9500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 15:59:26,064 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 15:59:59,900 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:00:00,492 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:00:00,876 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-9500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5934, 'learning_rate': 2.7614570063694268e-05, 'epoch': 7.96}        \n",
      "  8%|██▌                             | 10000/125600 [1:07:54<8:23:22,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 16:03:06,996 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-10000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:03:07,326 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:03:29,042 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:03:29,416 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:03:29,734 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.7481, 'learning_rate': 2.7495382165605094e-05, 'epoch': 8.36}        \n",
      "  8%|██▋                             | 10500/125600 [1:11:15<8:17:09,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:06:27,750 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-10500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:06:28,077 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:06:48,709 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:06:49,084 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:06:49,421 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-10500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0303, 'learning_rate': 2.737595541401274e-05, 'epoch': 8.76}         \n",
      "  9%|██▊                             | 11000/125600 [1:14:33<8:14:55,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:09:46,008 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-11000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:09:46,370 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:10:09,409 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:10:09,706 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:10:10,056 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8187, 'learning_rate': 2.7256528662420384e-05, 'epoch': 9.16}        \n",
      "  9%|██▉                             | 11500/125600 [1:17:55<8:13:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:13:08,364 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-11500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:13:08,668 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:13:29,292 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:13:29,688 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:13:30,046 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-11500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8765, 'learning_rate': 2.7137101910828027e-05, 'epoch': 9.55}        \n",
      " 10%|███                             | 12000/125600 [1:21:15<8:10:29,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:16:27,977 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-12000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:16:28,270 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:16:57,651 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:16:57,947 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:16:58,307 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8891, 'learning_rate': 2.7017914012738853e-05, 'epoch': 9.95}        \n",
      " 10%|███▏                            | 12500/125600 [1:24:42<8:08:12,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:19:54,644 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-12500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:19:55,020 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:20:15,769 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:20:16,174 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:20:16,506 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-12500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6341, 'learning_rate': 2.6898487261146496e-05, 'epoch': 10.35}       \n",
      " 10%|███▎                            | 13000/125600 [1:28:04<8:06:20,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:23:16,736 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-13000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:23:17,171 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:23:38,338 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:23:38,672 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:23:39,121 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.9864, 'learning_rate': 2.677906050955414e-05, 'epoch': 10.75}        \n",
      " 11%|███▍                            | 13500/125600 [1:31:24<8:04:37,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:26:36,940 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-13500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:26:37,280 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:26:57,952 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:26:58,259 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:26:58,646 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-13500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3945, 'learning_rate': 2.6659633757961786e-05, 'epoch': 11.15}       \n",
      " 11%|███▌                            | 14000/125600 [1:34:44<8:02:26,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:29:56,831 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-14000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:29:57,172 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:30:19,436 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:30:19,768 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:30:20,087 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4969, 'learning_rate': 2.6540207006369426e-05, 'epoch': 11.54}       \n",
      " 12%|███▋                            | 14500/125600 [1:38:09<8:00:20,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 16:33:22,261 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-14500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:33:22,614 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:33:43,436 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:33:43,775 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:33:44,138 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-14500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4386, 'learning_rate': 2.6421019108280255e-05, 'epoch': 11.94}       \n",
      " 12%|███▊                            | 15000/125600 [1:41:30<7:58:16,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 16:36:42,740 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-15000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:36:43,046 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:37:06,021 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:37:06,353 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:37:06,745 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1529, 'learning_rate': 2.63015923566879e-05, 'epoch': 12.34}         \n",
      " 12%|███▉                            | 15500/125600 [1:44:50<8:03:29,  3.80it/s][INFO|trainer.py:2671] 2022-11-15 16:40:03,021 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-15500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:40:03,376 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:40:26,248 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:40:26,647 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:40:27,024 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-15500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4111, 'learning_rate': 2.6182165605095542e-05, 'epoch': 12.74}       \n",
      " 13%|████                            | 16000/125600 [1:48:13<7:54:09,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 16:43:25,608 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-16000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:43:26,017 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:43:46,881 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:43:47,177 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:43:47,495 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1625, 'learning_rate': 2.6062738853503186e-05, 'epoch': 13.14}       \n",
      " 13%|████▏                           | 16500/125600 [1:51:34<7:51:57,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 16:46:46,622 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-16500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:46:47,052 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:47:09,124 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:47:09,421 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:47:09,824 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-16500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.221, 'learning_rate': 2.594355095541401e-05, 'epoch': 13.54}         \n",
      " 14%|████▎                           | 17000/125600 [1:54:59<7:49:33,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 16:50:11,970 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-17000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:50:12,343 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:50:33,113 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:50:33,460 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:50:34,086 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3075, 'learning_rate': 2.5824124203821655e-05, 'epoch': 13.93}       \n",
      " 14%|████▍                           | 17500/125600 [1:58:21<7:47:00,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 16:53:34,252 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-17500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 16:53:34,596 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 16:53:56,747 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 16:53:57,145 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 16:53:57,529 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-17500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 14%|████▍                           | 17637/125600 [2:00:09<7:46:12,  3.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3121, 'learning_rate': 2.5465843949044585e-05, 'epoch': 15.13}       \n",
      " 15%|████▊                           | 19000/125600 [2:08:30<7:40:13,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:03:43,170 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-19000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:03:43,514 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:04:03,489 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:04:03,882 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:04:04,252 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8979, 'learning_rate': 2.5346417197452228e-05, 'epoch': 15.53}       \n",
      " 16%|████▉                           | 19500/125600 [2:11:50<7:46:31,  3.79it/s][INFO|trainer.py:2671] 2022-11-15 17:07:03,112 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-19500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:07:03,592 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:07:25,164 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:07:25,588 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:07:25,903 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-19500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0793, 'learning_rate': 2.5226990445859875e-05, 'epoch': 15.92}       \n",
      " 16%|█████                           | 20000/125600 [2:15:11<7:37:41,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 17:10:23,789 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-20000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:10:24,109 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:10:44,276 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:10:44,568 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:10:44,884 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4946, 'learning_rate': 2.5107563694267518e-05, 'epoch': 16.32}       \n",
      " 16%|█████▏                          | 20500/125600 [2:18:28<7:34:00,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:13:41,069 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-20500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:13:41,374 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:14:01,225 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:14:01,596 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:14:01,922 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-20500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6039, 'learning_rate': 2.4988136942675158e-05, 'epoch': 16.72}       \n",
      " 17%|█████▎                          | 21000/125600 [2:21:49<7:38:53,  3.80it/s][INFO|trainer.py:2671] 2022-11-15 17:17:01,959 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-21000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:17:02,265 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:17:24,393 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:17:24,728 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:17:25,045 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3325, 'learning_rate': 2.4868949044585987e-05, 'epoch': 17.12}       \n",
      " 17%|█████▍                          | 21500/125600 [2:25:09<7:31:33,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 17:20:22,158 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-21500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:20:22,468 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:20:43,612 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:20:43,926 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:20:44,283 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-21500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5405, 'learning_rate': 2.4749761146496816e-05, 'epoch': 17.52}       \n",
      " 18%|█████▌                          | 22000/125600 [2:28:35<7:27:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:23:47,989 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-22000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:23:48,354 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:24:09,719 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:24:10,091 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:24:10,468 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2666, 'learning_rate': 2.463033439490446e-05, 'epoch': 17.91}        \n",
      " 18%|█████▋                          | 22500/125600 [2:31:56<7:26:55,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 17:27:08,488 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-22500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:27:08,868 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:27:28,692 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:27:29,099 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:27:29,411 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-22500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8855, 'learning_rate': 2.4510907643312103e-05, 'epoch': 18.31}       \n",
      " 18%|█████▊                          | 23000/125600 [2:35:17<7:28:31,  3.81it/s][INFO|trainer.py:2671] 2022-11-15 17:30:29,893 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-23000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:30:30,180 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:30:53,109 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:30:53,569 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:30:53,937 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5188, 'learning_rate': 2.439171974522293e-05, 'epoch': 18.71}        \n",
      " 19%|█████▉                          | 23500/125600 [2:39:01<7:21:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 17:34:13,903 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-23500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:34:14,318 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:34:35,681 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:34:36,116 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:34:36,460 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-23500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0856, 'learning_rate': 2.4272292993630572e-05, 'epoch': 19.11}       \n",
      " 19%|██████                          | 24000/125600 [2:42:24<7:19:05,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:37:36,791 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-24000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:37:37,160 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:37:58,366 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:37:58,710 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:37:59,023 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.04, 'learning_rate': 2.415286624203822e-05, 'epoch': 19.51}          \n",
      " 20%|██████▏                         | 24500/125600 [2:45:44<7:17:28,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 17:40:57,307 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-24500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:40:57,572 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:41:19,607 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:41:19,912 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:41:20,292 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-24500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0085, 'learning_rate': 2.4033439490445862e-05, 'epoch': 19.9}        \n",
      " 20%|██████▎                         | 25000/125600 [2:49:11<7:14:32,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:44:23,495 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-25000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:44:23,799 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:44:45,465 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:44:45,785 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:44:46,102 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2144, 'learning_rate': 2.3914012738853502e-05, 'epoch': 20.3}        \n",
      " 20%|██████▍                         | 25500/125600 [2:52:32<7:11:58,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:47:44,644 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-25500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:47:44,959 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:48:06,129 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:48:06,576 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:48:06,925 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-25500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1031, 'learning_rate': 2.3794585987261145e-05, 'epoch': 20.7}        \n",
      " 21%|██████▌                         | 26000/125600 [2:55:53<7:17:40,  3.79it/s][INFO|trainer.py:2671] 2022-11-15 17:51:05,437 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-26000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:51:05,754 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:51:28,595 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:51:28,946 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:51:29,274 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1336, 'learning_rate': 2.3675159235668792e-05, 'epoch': 21.1}        \n",
      " 21%|██████▊                         | 26500/125600 [2:59:16<7:09:07,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 17:54:28,606 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-26500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:54:28,899 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:54:49,539 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:54:49,943 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:54:50,348 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-26500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3206, 'learning_rate': 2.3555732484076436e-05, 'epoch': 21.5}        \n",
      " 21%|██████▉                         | 27000/125600 [3:02:40<7:05:47,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 17:57:53,222 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-27000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 17:57:53,582 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 17:58:15,592 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 17:58:15,947 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 17:58:16,272 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0794, 'learning_rate': 2.3436783439490447e-05, 'epoch': 21.89}       \n",
      " 22%|███████                         | 27500/125600 [3:06:01<7:04:36,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 18:01:13,807 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-27500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:01:14,169 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:01:37,306 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:01:37,605 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:01:37,896 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-27500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0694, 'learning_rate': 2.3317356687898087e-05, 'epoch': 22.29}       \n",
      " 22%|███████▏                        | 28000/125600 [3:09:34<7:01:50,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:04:46,630 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-28000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:04:47,075 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:05:08,329 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:05:08,700 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:05:09,041 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0866, 'learning_rate': 2.3197929936305734e-05, 'epoch': 22.69}       \n",
      " 23%|███████▎                        | 28500/125600 [3:12:53<6:59:30,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:08:05,424 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-28500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:08:05,690 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:08:30,081 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:08:30,483 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:08:30,872 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-28500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2699, 'learning_rate': 2.3078503184713377e-05, 'epoch': 23.09}       \n",
      " 23%|███████▍                        | 29000/125600 [3:16:16<6:57:39,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 18:11:28,751 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-29000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:11:29,082 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:11:50,822 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:11:51,115 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:11:51,447 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0621, 'learning_rate': 2.295907643312102e-05, 'epoch': 23.49}        \n",
      " 23%|███████▌                        | 29500/125600 [3:19:37<6:53:53,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 18:14:50,215 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-29500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:14:50,501 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:15:10,760 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:15:11,090 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:15:11,452 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-29500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1662, 'learning_rate': 2.283964968152866e-05, 'epoch': 23.89}        \n",
      " 24%|███████▋                        | 30000/125600 [3:23:09<6:52:15,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:18:21,841 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-30000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:18:22,136 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:18:47,563 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:18:47,935 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:18:48,379 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9482, 'learning_rate': 2.2720222929936307e-05, 'epoch': 24.28}       \n",
      " 24%|███████▊                        | 30500/125600 [3:26:43<6:50:14,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:21:55,831 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-30500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:21:56,168 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:22:23,389 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:22:23,689 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:22:23,992 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-30500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9606, 'learning_rate': 2.260079617834395e-05, 'epoch': 24.68}        \n",
      " 25%|███████▉                        | 31000/125600 [3:30:06<6:48:02,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:25:19,142 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-31000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:25:19,560 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-31000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:25:40,446 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-31000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:25:40,773 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-31000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:25:41,154 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-31000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2075, 'learning_rate': 2.2481369426751594e-05, 'epoch': 25.08}       \n",
      " 25%|████████                        | 31500/125600 [3:33:29<6:45:29,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 18:28:41,857 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-31500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:28:42,274 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-31500/config.json\n",
      "{'loss': 1.9507, 'learning_rate': 2.2361942675159234e-05, 'epoch': 25.48}       \n",
      " 25%|████████▏                       | 32000/125600 [3:36:47<6:43:08,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 18:32:00,122 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-32000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:32:00,512 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:32:21,343 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:32:21,693 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:32:22,122 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.99, 'learning_rate': 2.224251592356688e-05, 'epoch': 25.88}          \n",
      " 26%|████████▎                       | 32500/125600 [3:40:05<6:41:49,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:35:18,154 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-32500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:35:18,503 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:35:40,720 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:35:41,073 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:35:41,372 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-32500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0022, 'learning_rate': 2.2123089171974524e-05, 'epoch': 26.27}       \n",
      " 26%|████████▍                       | 33000/125600 [3:43:28<6:40:05,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:38:40,606 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-33000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:38:40,964 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:39:05,667 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:39:06,023 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:39:06,377 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0491, 'learning_rate': 2.2003662420382167e-05, 'epoch': 26.67}       \n",
      " 27%|████████▌                       | 33500/125600 [3:47:00<6:37:40,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 18:42:13,134 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-33500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:42:13,463 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:42:33,922 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:42:34,292 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:42:34,623 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-33500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1034, 'learning_rate': 2.1884235668789807e-05, 'epoch': 27.07}       \n",
      " 27%|████████▋                       | 34000/125600 [3:50:20<6:36:49,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 18:45:33,233 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-34000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:45:33,537 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-34000/config.json\n",
      "{'loss': 2.0613, 'learning_rate': 2.1765047770700636e-05, 'epoch': 27.47}       \n",
      " 27%|████████▊                       | 34500/125600 [3:53:36<6:30:27,  3.89it/s][INFO|trainer.py:2671] 2022-11-15 18:48:49,117 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-34500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:48:49,539 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-34500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:49:09,394 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-34500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:49:09,693 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-34500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:49:09,997 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-34500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0392, 'learning_rate': 2.164562101910828e-05, 'epoch': 27.87}        \n",
      " 28%|████████▉                       | 35000/125600 [3:56:56<6:32:17,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 18:52:09,299 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-35000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:52:09,668 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:52:29,760 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:52:30,099 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:52:30,378 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9726, 'learning_rate': 2.1526194267515926e-05, 'epoch': 28.26}       \n",
      " 28%|█████████                       | 35500/125600 [4:00:16<6:29:41,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 18:55:29,051 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-35500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:55:29,376 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:55:50,643 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:55:50,927 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:55:51,201 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-35500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6027, 'learning_rate': 2.1406767515923566e-05, 'epoch': 28.66}       \n",
      " 29%|█████████▏                      | 36000/125600 [4:03:36<6:28:06,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 18:58:49,370 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-36000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 18:58:49,706 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 18:59:08,506 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 18:59:08,822 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 18:59:09,118 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5043, 'learning_rate': 2.128734076433121e-05, 'epoch': 29.06}        \n",
      " 29%|█████████▎                      | 36500/125600 [4:06:52<6:25:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 19:02:05,190 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-36500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:02:05,753 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:02:27,409 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:02:27,748 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:02:28,121 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-36500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.111, 'learning_rate': 2.1167914012738853e-05, 'epoch': 29.46}        \n",
      " 29%|█████████▍                      | 37000/125600 [4:10:19<6:22:37,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 19:05:32,111 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-37000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:05:32,568 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:06:03,646 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:06:04,030 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:06:04,349 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0965, 'learning_rate': 2.10484872611465e-05, 'epoch': 29.86}         \n",
      " 30%|█████████▌                      | 37500/125600 [4:13:51<6:23:56,  3.82it/s][INFO|trainer.py:2671] 2022-11-15 19:09:04,266 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-37500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:09:04,612 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:09:25,301 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:09:25,621 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:09:25,906 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-37500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9389, 'learning_rate': 2.092906050955414e-05, 'epoch': 30.25}        \n",
      " 30%|█████████▋                      | 38000/125600 [4:17:16<6:22:36,  3.82it/s][INFO|trainer.py:2671] 2022-11-15 19:12:28,620 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-38000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:12:28,964 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:12:49,604 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:12:49,905 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:12:50,321 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1853, 'learning_rate': 2.0809633757961783e-05, 'epoch': 30.65}       \n",
      " 31%|█████████▊                      | 38500/125600 [4:20:37<6:16:10,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 19:15:50,135 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-38500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:15:50,471 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:16:16,269 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:16:16,591 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:16:16,979 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-38500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.946, 'learning_rate': 2.0690207006369427e-05, 'epoch': 31.05}        \n",
      " 31%|█████████▉                      | 39000/125600 [4:24:00<6:16:17,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 19:19:13,228 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-39000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:19:13,573 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-39000/config.json\n",
      "{'loss': 2.0071, 'learning_rate': 2.0570780254777073e-05, 'epoch': 31.45}       \n",
      " 31%|██████████                      | 39500/125600 [4:27:22<6:12:01,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 19:22:35,370 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-39500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:22:35,694 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-39500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:22:55,772 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-39500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:22:56,150 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-39500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:22:56,497 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-39500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2209, 'learning_rate': 2.0451831210191085e-05, 'epoch': 31.85}       \n",
      " 32%|██████████▏                     | 40000/125600 [4:30:43<6:10:35,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 19:25:55,698 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-40000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:25:56,036 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:26:19,299 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:26:19,715 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:26:20,093 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0641, 'learning_rate': 2.0332404458598725e-05, 'epoch': 32.25}       \n",
      " 32%|██████████▎                     | 40500/125600 [4:34:06<6:09:37,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 19:29:18,523 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-40500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:29:18,829 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:29:38,699 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:29:39,043 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:29:39,365 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-40500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9674, 'learning_rate': 2.021297770700637e-05, 'epoch': 32.64}        \n",
      " 33%|██████████▍                     | 41000/125600 [4:37:23<6:05:07,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 19:32:36,290 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-41000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:32:36,593 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:32:56,881 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:32:57,299 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:32:57,591 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0169, 'learning_rate': 2.0093550955414015e-05, 'epoch': 33.04}       \n",
      " 33%|██████████▌                     | 41500/125600 [4:40:42<6:03:55,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 19:35:54,845 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-41500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:35:55,171 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:36:16,488 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:36:16,825 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:36:17,165 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-41500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0076, 'learning_rate': 1.9974124203821658e-05, 'epoch': 33.44}       \n",
      " 33%|██████████▋                     | 42000/125600 [4:44:01<6:03:07,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 19:39:13,728 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-42000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:39:14,158 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-42000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:39:34,281 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-42000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:39:34,586 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-42000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:39:34,956 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-42000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 33%|██████████▋                     | 42033/125600 [4:45:25<6:00:51,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7799, 'learning_rate': 1.9735509554140127e-05, 'epoch': 34.24}       \n",
      " 34%|██████████▉                     | 43000/125600 [4:50:44<5:57:23,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 19:45:57,327 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-43000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 19:45:57,680 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-43000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 19:46:17,792 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-43000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 19:46:18,186 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-43000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 19:46:18,480 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-43000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 34%|███████████                     | 43230/125600 [4:52:50<5:56:19,  3.85it/s][INFO|modeling_utils.py:1624] 2022-11-15 20:00:09,084 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-45000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:00:09,444 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-45000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:00:09,785 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-45000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9194, 'learning_rate': 1.8661146496815285e-05, 'epoch': 37.82}       \n",
      " 38%|████████████                    | 47500/125600 [5:21:22<5:37:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 20:16:35,128 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-47500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:16:35,461 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-47500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:16:58,271 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-47500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:16:58,599 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-47500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:16:58,914 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-47500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9433, 'learning_rate': 1.8541719745222932e-05, 'epoch': 38.22}       \n",
      " 38%|████████████▏                   | 48000/125600 [5:25:01<5:35:05,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 20:20:13,918 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-48000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:20:14,268 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:20:36,102 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:20:36,463 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:20:36,902 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9242, 'learning_rate': 1.8422292993630575e-05, 'epoch': 38.61}       \n",
      " 39%|████████████▎                   | 48500/125600 [5:28:25<5:35:15,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 20:23:38,218 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-48500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:23:38,577 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:24:03,294 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:24:03,664 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:24:03,985 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-48500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9324, 'learning_rate': 1.8302866242038215e-05, 'epoch': 39.01}       \n",
      " 39%|████████████▍                   | 49000/125600 [5:31:49<5:38:07,  3.78it/s][INFO|trainer.py:2671] 2022-11-15 20:27:02,322 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-49000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:27:02,980 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:27:26,445 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:27:26,814 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:27:27,335 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9326, 'learning_rate': 1.818343949044586e-05, 'epoch': 39.41}        \n",
      " 39%|████████████▌                   | 49500/125600 [5:35:12<5:28:10,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 20:30:24,751 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-49500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:30:25,469 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:30:45,785 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:30:46,123 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:30:46,468 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-49500/special_tokens_map.json\n",
      "{'loss': 1.8975, 'learning_rate': 1.8064012738853506e-05, 'epoch': 39.81}       \n",
      " 40%|████████████▋                   | 50000/125600 [5:38:33<5:26:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 20:33:46,039 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-50000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:33:46,410 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:34:06,991 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:34:07,315 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:34:07,624 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0671, 'learning_rate': 1.794458598726115e-05, 'epoch': 40.21}        \n",
      " 40%|████████████▊                   | 50500/125600 [5:41:53<5:23:33,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 20:37:05,596 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-50500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:37:06,359 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:37:29,553 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:37:29,906 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:37:30,217 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-50500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.209, 'learning_rate': 1.782515923566879e-05, 'epoch': 40.61}         \n",
      " 41%|████████████▉                   | 51000/125600 [5:45:16<5:21:28,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 20:40:28,527 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-51000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:40:28,849 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-51000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:40:50,042 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-51000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:40:50,410 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-51000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:40:50,747 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-51000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 41%|█████████████                   | 51183/125600 [5:47:15<5:20:56,  3.86it/s][INFO|modeling_utils.py:1624] 2022-11-15 20:51:16,259 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-52500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:51:16,596 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-52500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:51:16,915 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-52500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1207, 'learning_rate': 1.7347691082802548e-05, 'epoch': 42.2}        \n",
      " 42%|█████████████▌                  | 53000/125600 [5:59:02<5:14:05,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 20:54:15,153 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-53000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:54:15,504 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:54:41,410 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:54:41,792 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:54:42,129 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8135, 'learning_rate': 1.722826433121019e-05, 'epoch': 42.6}         \n",
      " 43%|█████████████▋                  | 53500/125600 [6:02:31<5:13:48,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 20:57:44,032 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-53500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 20:57:44,353 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 20:58:05,218 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 20:58:05,703 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 20:58:06,007 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-53500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1076, 'learning_rate': 1.7108837579617835e-05, 'epoch': 42.99}       \n",
      " 43%|█████████████▊                  | 54000/125600 [6:05:59<5:11:55,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 21:01:12,353 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-54000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:01:12,662 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:01:37,188 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:01:37,535 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:01:37,885 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8442, 'learning_rate': 1.6989410828025478e-05, 'epoch': 43.39}       \n",
      " 43%|█████████████▉                  | 54500/125600 [6:09:22<5:08:01,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 21:04:34,930 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-54500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:04:35,247 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:04:55,703 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:04:56,053 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:04:56,386 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-54500/special_tokens_map.json\n",
      "{'loss': 1.8598, 'learning_rate': 1.686998407643312e-05, 'epoch': 43.79}        \n",
      " 44%|██████████████                  | 55000/125600 [6:12:40<5:03:56,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 21:07:52,851 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-55000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:07:53,165 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:08:15,838 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:08:16,185 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:08:16,525 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8612, 'learning_rate': 1.6750557324840765e-05, 'epoch': 44.19}       \n",
      " 44%|██████████████▏                 | 55500/125600 [6:16:16<5:03:04,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 21:11:28,598 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-55500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:11:28,882 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:11:50,659 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:11:51,032 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:11:51,375 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-55500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7965, 'learning_rate': 1.6631369426751594e-05, 'epoch': 44.59}       \n",
      " 45%|██████████████▎                 | 56000/125600 [6:19:35<4:59:25,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 21:14:48,144 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-56000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:14:48,474 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:15:09,530 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:15:09,856 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:15:10,173 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8743, 'learning_rate': 1.6511942675159237e-05, 'epoch': 44.98}       \n",
      " 45%|██████████████▍                 | 56500/125600 [6:22:55<4:58:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 21:18:07,718 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-56500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:18:08,029 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:18:32,294 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:18:32,608 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:18:32,953 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-56500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 45%|██████████████▍                 | 56681/125600 [6:24:53<4:57:34,  3.86it/s][INFO|modeling_utils.py:1624] 2022-11-15 21:25:08,895 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-57500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:25:09,187 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-57500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:25:09,504 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-57500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7475, 'learning_rate': 1.6153662420382167e-05, 'epoch': 46.18}       \n",
      " 46%|██████████████▊                 | 58000/125600 [6:32:55<4:51:20,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 21:28:07,965 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-58000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:28:08,266 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:28:28,577 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:28:28,888 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:28:29,160 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5722, 'learning_rate': 1.603423566878981e-05, 'epoch': 46.58}        \n",
      " 47%|██████████████▉                 | 58500/125600 [6:36:18<4:49:21,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 21:31:31,490 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-58500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:31:31,818 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:31:53,046 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:31:53,509 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:31:54,017 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-58500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7929, 'learning_rate': 1.591480891719745e-05, 'epoch': 46.97}        \n",
      " 47%|███████████████                 | 59000/125600 [6:39:38<4:46:48,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 21:34:51,156 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-59000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:34:51,478 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:35:12,315 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:35:12,668 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:35:13,120 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7854, 'learning_rate': 1.5795382165605094e-05, 'epoch': 47.37}       \n",
      " 47%|███████████████▏                | 59500/125600 [6:43:01<4:46:15,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 21:38:14,409 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-59500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:38:14,761 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:38:41,453 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:38:41,782 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:38:42,108 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-59500/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:42:00,126 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:42:00,591 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:42:00,926 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8969, 'learning_rate': 1.5556767515923566e-05, 'epoch': 48.17}       \n",
      " 48%|███████████████▍                | 60500/125600 [6:49:45<4:43:30,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 21:44:58,259 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-60500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 21:44:58,663 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 21:45:19,136 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 21:45:19,458 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 21:45:19,745 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-60500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 49%|███████████████▌                | 61262/125600 [6:55:27<4:39:11,  3.84it/s][INFO|modeling_utils.py:1624] 2022-11-15 22:05:40,855 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-63500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:05:41,166 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-63500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:05:41,460 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-63500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4926, 'learning_rate': 1.472125796178344e-05, 'epoch': 50.96}        \n",
      " 51%|████████████████▎               | 64000/125600 [7:13:25<4:25:24,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 22:08:38,086 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-64000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:08:38,452 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:09:01,284 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:09:01,670 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:09:02,032 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8296, 'learning_rate': 1.4601831210191083e-05, 'epoch': 51.35}       \n",
      " 51%|████████████████▍               | 64500/125600 [7:16:49<4:24:06,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 22:12:01,562 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-64500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:12:01,966 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:12:23,965 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:12:24,324 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:12:24,662 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-64500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.672, 'learning_rate': 1.4482404458598726e-05, 'epoch': 51.75}        \n",
      " 52%|████████████████▌               | 65000/125600 [7:20:12<4:22:54,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 22:15:25,108 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-65000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:15:25,439 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-65000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:15:45,554 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-65000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:15:45,855 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-65000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:15:46,181 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-65000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7793, 'learning_rate': 1.436297770700637e-05, 'epoch': 52.15}        \n",
      " 52%|████████████████▋               | 65500/125600 [7:23:28<4:19:56,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 22:18:40,627 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-65500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:18:41,115 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-65500/config.json\n",
      "{'loss': 1.6569, 'learning_rate': 1.4243789808917199e-05, 'epoch': 52.55}       \n",
      " 53%|████████████████▊               | 66000/125600 [7:26:48<4:18:47,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 22:22:00,942 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-66000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:22:01,267 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:22:31,928 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:22:32,316 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:22:32,627 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2555, 'learning_rate': 1.412436305732484e-05, 'epoch': 52.95}        \n",
      " 53%|████████████████▉               | 66500/125600 [7:30:19<4:15:47,  3.85it/s][INFO|trainer.py:2671] 2022-11-15 22:25:31,770 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-66500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:25:32,067 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:25:54,822 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:25:55,151 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:25:55,535 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-66500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8567, 'learning_rate': 1.3646894904458598e-05, 'epoch': 54.54}       \n",
      " 55%|█████████████████▍              | 68500/125600 [7:43:59<4:07:41,  3.84it/s][INFO|trainer.py:2671] 2022-11-15 22:39:12,026 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-68500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:39:12,391 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-68500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:39:32,508 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-68500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:39:32,829 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-68500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:39:33,208 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-68500/special_tokens_map.json\n",
      "{'loss': 1.8848, 'learning_rate': 1.3527468152866243e-05, 'epoch': 54.94}       \n",
      " 55%|█████████████████▌              | 69000/125600 [7:47:29<4:04:40,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 22:42:41,548 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-69000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 22:42:41,853 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:43:16,853 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:43:17,121 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:43:17,424 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69000/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:46:34,861 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:46:35,204 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:46:35,656 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-69500/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 22:49:56,135 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-70000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 22:49:56,505 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-70000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 22:49:57,458 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-70000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 56%|██████████████████              | 70898/125600 [8:00:38<3:57:07,  3.84it/s][INFO|tokenization_utils_base.py:2125] 2022-11-15 23:06:53,148 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-72500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:06:53,525 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-72500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6947, 'learning_rate': 1.2572531847133758e-05, 'epoch': 58.12}       \n",
      " 58%|██████████████████▌             | 73000/125600 [8:14:46<3:47:11,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 23:09:58,727 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-73000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 23:09:59,027 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 23:10:21,000 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 23:10:21,305 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:10:21,650 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5709, 'learning_rate': 1.2453105095541403e-05, 'epoch': 58.52}       \n",
      " 59%|██████████████████▋             | 73500/125600 [8:18:06<3:46:29,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 23:13:19,347 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-73500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 23:13:19,726 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 23:13:39,991 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 23:13:40,310 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:13:40,673 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-73500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8282, 'learning_rate': 1.2333678343949045e-05, 'epoch': 58.92}       \n",
      " 59%|██████████████████▊             | 74000/125600 [8:21:26<3:44:15,  3.83it/s][INFO|trainer.py:2671] 2022-11-15 23:16:39,261 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-74000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 23:16:39,586 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 23:17:02,245 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 23:17:02,645 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:17:03,001 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6447, 'learning_rate': 1.221425159235669e-05, 'epoch': 59.32}        \n",
      " 59%|██████████████████▉             | 74500/125600 [8:24:52<3:40:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-15 23:20:04,999 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-74500\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 23:20:05,313 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 23:20:25,898 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 23:20:26,461 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:20:26,793 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-74500/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 23:23:45,437 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-75000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 23:23:45,774 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-75000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:23:46,061 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-75000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 62%|███████████████████▋            | 77334/125600 [8:44:41<3:28:19,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6684, 'learning_rate': 1.0901035031847133e-05, 'epoch': 63.69}       \n",
      " 64%|████████████████████▍           | 80000/125600 [9:02:36<3:16:18,  3.87it/s][INFO|trainer.py:2671] 2022-11-15 23:57:49,401 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-80000\n",
      "[INFO|configuration_utils.py:447] 2022-11-15 23:57:49,681 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-15 23:58:11,332 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-15 23:58:11,672 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-15 23:58:11,996 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4427, 'learning_rate': 1.0781847133757962e-05, 'epoch': 64.09}       \n",
      " 64%|████████████████████▌           | 80500/125600 [9:05:57<3:14:32,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 00:01:09,806 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-80500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:01:10,152 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:01:31,820 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:01:32,149 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:01:32,474 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-80500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5719, 'learning_rate': 1.0662420382165605e-05, 'epoch': 64.49}       \n",
      " 64%|████████████████████▋           | 81000/125600 [9:09:19<3:12:30,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 00:04:32,261 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-81000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:04:32,607 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:04:53,014 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:04:53,399 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:04:53,733 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7179, 'learning_rate': 1.0542993630573249e-05, 'epoch': 64.89}       \n",
      " 65%|████████████████████▊           | 81500/125600 [9:12:37<3:10:28,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 00:07:49,919 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-81500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:07:50,237 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:08:11,068 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:08:11,399 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:08:11,797 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-81500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4319, 'learning_rate': 1.0304140127388535e-05, 'epoch': 65.68}       \n",
      " 66%|█████████████████████           | 82500/125600 [9:19:24<3:06:25,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 00:14:37,013 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-82500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:14:37,340 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-82500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:14:57,710 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-82500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:14:58,769 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-82500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:14:59,089 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-82500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.78, 'learning_rate': 1.0185191082802548e-05, 'epoch': 66.08}         \n",
      " 66%|█████████████████████▏          | 83000/125600 [9:22:44<3:03:54,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 00:17:57,135 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-83000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:17:57,524 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:18:17,743 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:18:18,034 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:18:18,341 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5097, 'learning_rate': 1.0065764331210192e-05, 'epoch': 66.48}       \n",
      " 66%|█████████████████████▎          | 83500/125600 [9:26:02<3:01:31,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 00:21:14,602 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-83500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:21:14,967 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:21:35,269 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:21:35,647 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:21:36,009 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-83500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6796, 'learning_rate': 9.946337579617835e-06, 'epoch': 66.88}        \n",
      " 67%|█████████████████████▍          | 84000/125600 [9:29:20<2:59:58,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 00:24:32,699 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-84000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:24:33,002 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:24:53,522 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:24:54,054 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:24:54,394 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7987, 'learning_rate': 9.826910828025479e-06, 'epoch': 67.28}        \n",
      " 67%|█████████████████████▌          | 84500/125600 [9:32:41<2:58:07,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 00:27:53,882 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-84500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:27:54,166 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:28:15,001 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:28:15,361 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:28:15,872 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-84500/special_tokens_map.json\n",
      "{'loss': 1.5529, 'learning_rate': 9.707484076433122e-06, 'epoch': 67.68}        \n",
      " 68%|█████████████████████▋          | 85000/125600 [9:36:00<2:55:09,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 00:31:13,055 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-85000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:31:13,412 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:31:35,561 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:31:35,877 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:31:36,147 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7102, 'learning_rate': 9.588057324840764e-06, 'epoch': 68.07}        \n",
      " 68%|█████████████████████▊          | 85500/125600 [9:39:27<2:53:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 00:34:40,445 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-85500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:34:40,915 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:35:01,586 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:35:01,916 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:35:02,278 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-85500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7089, 'learning_rate': 9.468630573248409e-06, 'epoch': 68.47}        \n",
      " 68%|█████████████████████▉          | 86000/125600 [9:42:46<2:51:40,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 00:37:58,759 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-86000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:37:59,073 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:38:27,208 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:38:27,553 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:38:27,907 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5696, 'learning_rate': 9.34920382165605e-06, 'epoch': 68.87}         \n",
      " 69%|██████████████████████          | 86500/125600 [9:46:16<2:48:35,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 00:41:28,565 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-86500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:41:28,926 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:41:52,664 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:41:52,956 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:41:53,325 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-86500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4092, 'learning_rate': 9.229777070063695e-06, 'epoch': 69.27}        \n",
      " 69%|██████████████████████▏         | 87000/125600 [9:49:40<2:46:23,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 00:44:52,595 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-87000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:44:52,924 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-87000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:45:13,526 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-87000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:45:13,836 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-87000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:45:14,258 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-87000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 69%|██████████████████████▏         | 87057/125600 [9:51:06<2:47:17,  3.84it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6769, 'learning_rate': 8.991162420382166e-06, 'epoch': 70.06}        \n",
      " 70%|██████████████████████▍         | 88000/125600 [9:56:32<2:43:10,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 00:51:44,985 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-88000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 00:51:45,328 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-88000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 00:52:12,585 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-88000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 00:52:12,929 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-88000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 00:52:13,254 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-88000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6018, 'learning_rate': 8.513694267515923e-06, 'epoch': 71.66}        \n",
      " 72%|██████████████████████▏        | 90000/125600 [10:10:14<2:35:21,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 01:05:26,450 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-90000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:05:26,766 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:05:55,470 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:05:55,796 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:05:56,140 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4916, 'learning_rate': 8.394267515923567e-06, 'epoch': 72.05}        \n",
      " 72%|██████████████████████▎        | 90500/125600 [10:13:48<2:31:17,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 01:09:00,856 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-90500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:09:01,164 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:09:22,066 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:09:22,429 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:09:22,762 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-90500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8035, 'learning_rate': 8.27484076433121e-06, 'epoch': 72.45}         \n",
      " 72%|██████████████████████▍        | 91000/125600 [10:17:15<2:29:40,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 01:12:28,289 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-91000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:12:28,736 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:12:58,726 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:12:59,029 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:12:59,349 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3387, 'learning_rate': 8.155414012738854e-06, 'epoch': 72.85}        \n",
      " 73%|██████████████████████▌        | 91500/125600 [10:20:45<2:27:06,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 01:15:58,383 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-91500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:15:58,772 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:16:21,155 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:16:21,495 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:16:21,830 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-91500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5553, 'learning_rate': 8.035987261146497e-06, 'epoch': 73.25}        \n",
      " 73%|██████████████████████▋        | 92000/125600 [10:24:07<2:25:35,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 01:19:20,095 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-92000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:19:20,583 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-92000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:19:40,460 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-92000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:19:40,808 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-92000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:19:41,198 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-92000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 74%|██████████████████████▉        | 92957/125600 [10:30:37<2:21:45,  3.84it/s][INFO|modeling_utils.py:1624] 2022-11-16 01:40:01,864 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:40:02,402 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:40:02,711 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6435, 'learning_rate': 7.200238853503185e-06, 'epoch': 76.04}        \n",
      " 76%|███████████████████████▌       | 95500/125600 [10:47:45<2:09:58,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 01:42:58,197 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-95500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:42:58,571 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:43:18,382 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:43:18,684 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:43:19,010 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-95500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5282, 'learning_rate': 7.080812101910829e-06, 'epoch': 76.43}        \n",
      " 76%|███████████████████████▋       | 96000/125600 [10:51:03<2:07:52,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 01:46:16,156 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-96000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:46:16,443 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:46:35,810 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:46:36,148 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:46:36,479 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6214, 'learning_rate': 6.961385350318472e-06, 'epoch': 76.83}        \n",
      " 77%|███████████████████████▊       | 96500/125600 [10:54:26<2:05:45,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 01:49:38,698 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-96500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:49:39,016 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:49:59,490 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:49:59,810 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:50:00,232 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-96500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.2999, 'learning_rate': 6.842197452229299e-06, 'epoch': 77.23}        \n",
      " 77%|███████████████████████▉       | 97000/125600 [10:57:45<2:03:31,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 01:52:57,882 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-97000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 01:52:58,183 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-97000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 01:53:19,235 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-97000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 01:53:19,584 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-97000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 01:53:19,875 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-97000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6063, 'learning_rate': 6.006449044585987e-06, 'epoch': 80.02}        \n",
      " 80%|████████████████████████      | 100500/125600 [11:21:50<1:48:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 02:17:03,121 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-100500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:17:03,448 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-100500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:17:32,612 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-100500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:17:32,987 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-100500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:17:33,292 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-100500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5471, 'learning_rate': 5.88702229299363e-06, 'epoch': 80.41}         \n",
      " 80%|████████████████████████      | 101000/125600 [11:25:18<1:45:52,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 02:20:30,917 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-101000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:20:31,237 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:20:52,984 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:20:53,291 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:20:53,684 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6526, 'learning_rate': 5.767595541401274e-06, 'epoch': 80.81}        \n",
      " 81%|████████████████████████▏     | 101500/125600 [11:28:40<1:43:55,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 02:23:52,887 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-101500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:23:53,266 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:24:16,534 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:24:16,847 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:24:17,178 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-101500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4817, 'learning_rate': 5.648168789808917e-06, 'epoch': 81.21}        \n",
      " 81%|████████████████████████▎     | 102000/125600 [11:32:00<1:41:42,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 02:27:12,927 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-102000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:27:13,373 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:27:36,610 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:27:37,002 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:27:37,401 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3771, 'learning_rate': 5.528980891719745e-06, 'epoch': 81.61}        \n",
      " 82%|████████████████████████▍     | 102500/125600 [11:35:29<1:39:28,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 02:30:41,666 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-102500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:30:41,995 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:31:03,805 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:31:04,091 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:31:04,436 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-102500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 82%|████████████████████████▌     | 102606/125600 [11:37:07<1:39:15,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 82%|████████████████████████▋     | 103285/125600 [11:41:23<1:36:11,  3.87it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5639, 'learning_rate': 4.6934713375796184e-06, 'epoch': 84.39}       \n",
      " 84%|█████████████████████████▎    | 106000/125600 [11:59:09<1:24:42,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 02:54:21,626 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-106000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:54:21,931 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:54:42,752 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:54:43,086 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:54:43,413 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5466, 'learning_rate': 4.574044585987262e-06, 'epoch': 84.79}        \n",
      " 85%|█████████████████████████▍    | 106500/125600 [12:02:31<1:22:17,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 02:57:43,975 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-106500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 02:57:44,264 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 02:58:13,076 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 02:58:13,432 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 02:58:13,750 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-106500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4726, 'learning_rate': 4.454617834394905e-06, 'epoch': 85.19}        \n",
      " 85%|█████████████████████████▌    | 107000/125600 [12:06:04<1:20:16,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 03:01:17,157 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-107000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:01:17,477 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:01:39,733 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:01:40,068 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:01:40,378 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5633, 'learning_rate': 4.3351910828025485e-06, 'epoch': 85.59}       \n",
      " 86%|█████████████████████████▋    | 107500/125600 [12:09:26<1:18:20,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 03:04:38,534 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-107500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:04:38,885 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:05:00,015 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:05:00,293 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:05:00,609 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-107500/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:08:35,103 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:08:35,431 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:08:35,751 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.2633, 'learning_rate': 4.096337579617834e-06, 'epoch': 86.39}        \n",
      " 86%|█████████████████████████▉    | 108500/125600 [12:16:27<1:14:11,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 03:11:40,158 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-108500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:11:40,545 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:12:03,466 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:12:03,787 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:12:04,096 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-108500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5035, 'learning_rate': 3.976910828025478e-06, 'epoch': 86.78}        \n",
      " 87%|██████████████████████████    | 109000/125600 [12:19:59<1:11:30,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 03:15:12,162 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-109000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:15:12,464 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:15:37,840 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:15:38,179 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:15:38,518 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6182, 'learning_rate': 3.857484076433121e-06, 'epoch': 87.18}        \n",
      " 87%|██████████████████████████▏   | 109500/125600 [12:23:30<1:09:53,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 03:18:42,976 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-109500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:18:43,350 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:19:05,029 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:19:05,324 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:19:05,665 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-109500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5837, 'learning_rate': 3.618630573248408e-06, 'epoch': 87.98}        \n",
      " 88%|██████████████████████████▍   | 110500/125600 [12:30:33<1:05:08,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 03:25:45,634 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-110500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:25:45,936 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-110500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:26:07,465 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-110500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:26:07,827 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-110500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:26:08,227 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-110500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3664, 'learning_rate': 3.4992038216560512e-06, 'epoch': 88.38}       \n",
      " 88%|██████████████████████████▌   | 111000/125600 [12:33:56<1:03:14,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 03:29:09,122 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-111000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:29:09,407 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:29:31,432 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:29:31,732 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:29:32,059 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6751, 'learning_rate': 3.3797770700636946e-06, 'epoch': 88.77}       \n",
      " 89%|██████████████████████████▋   | 111500/125600 [12:37:25<1:00:56,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 03:32:38,161 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-111500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:32:38,492 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:33:05,555 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:33:05,889 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:33:06,256 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-111500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5701, 'learning_rate': 3.2605891719745224e-06, 'epoch': 89.17}       \n",
      " 89%|████████████████████████████▌   | 112000/125600 [12:40:58<58:44,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 03:36:11,438 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-112000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:36:11,711 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-112000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:36:40,041 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-112000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:36:40,470 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-112000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:36:40,800 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-112000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4953, 'learning_rate': 3.0217356687898087e-06, 'epoch': 89.97}       \n",
      " 90%|████████████████████████████▊   | 113000/125600 [12:47:57<54:25,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 03:43:10,354 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-113000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:43:10,702 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:43:30,236 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:43:30,538 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:43:30,904 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6721, 'learning_rate': 2.902308917197452e-06, 'epoch': 90.37}        \n",
      " 90%|████████████████████████████▉   | 113500/125600 [12:51:18<52:19,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 03:46:31,115 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-113500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:46:31,448 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:46:52,632 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:46:52,941 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:46:53,256 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-113500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4171, 'learning_rate': 2.7831210191082802e-06, 'epoch': 90.76}       \n",
      " 91%|█████████████████████████████   | 114000/125600 [12:54:36<50:15,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 03:49:49,469 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-114000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 03:49:49,801 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-114000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 03:50:12,124 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-114000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 03:50:12,506 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-114000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 03:50:12,858 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-114000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5092, 'learning_rate': 2.4248407643312103e-06, 'epoch': 91.96}       \n",
      " 92%|█████████████████████████████▍  | 115500/125600 [13:04:47<43:44,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 04:00:00,607 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-115500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:00:00,955 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-115500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:00:22,636 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-115500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:00:22,958 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-115500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:00:23,244 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-115500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5075, 'learning_rate': 2.305652866242038e-06, 'epoch': 92.36}        \n",
      " 92%|█████████████████████████████▌  | 116000/125600 [13:08:07<41:26,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 04:03:19,663 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-116000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:03:20,116 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:03:41,761 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:03:42,065 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:03:42,393 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5702, 'learning_rate': 2.1864649681528663e-06, 'epoch': 92.75}       \n",
      " 93%|█████████████████████████████▋  | 116500/125600 [13:11:31<39:28,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 04:06:43,543 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-116500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:06:43,889 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:07:08,970 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:07:09,319 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:07:09,717 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-116500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5646, 'learning_rate': 2.0670382165605097e-06, 'epoch': 93.15}       \n",
      " 93%|█████████████████████████████▊  | 117000/125600 [13:14:50<37:30,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 04:10:03,213 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-117000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:10:03,580 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-117000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:10:25,289 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-117000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:10:25,606 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-117000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:10:25,923 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-117000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5299, 'learning_rate': 1.8281847133757964e-06, 'epoch': 93.95}       \n",
      " 94%|██████████████████████████████  | 118000/125600 [13:21:34<32:46,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 04:16:47,069 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-118000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:16:47,420 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:17:09,677 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:17:09,970 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:17:10,309 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5027, 'learning_rate': 1.7087579617834395e-06, 'epoch': 94.35}       \n",
      " 94%|██████████████████████████████▏ | 118500/125600 [13:24:57<30:39,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 04:20:10,319 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-118500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:20:10,665 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:20:33,045 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:20:33,403 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:20:33,724 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-118500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5207, 'learning_rate': 1.5893312101910827e-06, 'epoch': 94.75}       \n",
      " 95%|██████████████████████████████▎ | 119000/125600 [13:28:27<28:30,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 04:23:40,075 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-119000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:23:40,366 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:24:02,901 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:24:03,215 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:24:03,511 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3726, 'learning_rate': 1.469904458598726e-06, 'epoch': 95.14}        \n",
      " 95%|██████████████████████████████▍ | 119500/125600 [13:31:49<26:20,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 04:27:01,726 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-119500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:27:02,112 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:27:24,396 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:27:24,762 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:27:25,069 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-119500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3402, 'learning_rate': 1.3504777070063694e-06, 'epoch': 95.54}       \n",
      " 96%|██████████████████████████████▌ | 120000/125600 [13:35:13<24:16,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 04:30:26,429 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-120000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:30:27,782 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:30:49,377 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:30:49,825 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:30:50,083 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 96%|████████████████████████████▋ | 120008/125600 [13:36:29<3:13:42,  2.08s/it][INFO|modeling_utils.py:1624] 2022-11-16 04:34:11,590 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:34:11,945 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:34:12,255 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-120500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3725, 'learning_rate': 6.343949044585987e-07, 'epoch': 97.93}        \n",
      " 98%|███████████████████████████████▎| 123000/125600 [13:55:43<11:17,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 04:50:56,263 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-123000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:50:56,599 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:51:18,540 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:51:18,859 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:51:19,161 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5944, 'learning_rate': 5.149681528662421e-07, 'epoch': 98.33}        \n",
      " 98%|███████████████████████████████▍| 123500/125600 [13:59:09<09:03,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 04:54:21,791 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-123500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:54:22,109 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:54:51,331 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:54:51,632 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:54:51,973 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-123500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.466, 'learning_rate': 3.9554140127388536e-07, 'epoch': 98.73}        \n",
      " 99%|███████████████████████████████▌| 124000/125600 [14:02:38<06:54,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 04:57:50,451 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-124000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 04:57:50,776 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 04:58:11,997 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 04:58:12,338 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 04:58:12,636 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.421, 'learning_rate': 2.7611464968152867e-07, 'epoch': 99.12}        \n",
      " 99%|███████████████████████████████▋| 124500/125600 [14:05:58<04:44,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 05:01:10,689 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-124500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 05:01:11,032 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 05:01:32,396 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 05:01:32,702 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 05:01:32,990 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-124500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6345, 'learning_rate': 1.569267515923567e-07, 'epoch': 99.52}        \n",
      "100%|███████████████████████████████▊| 125000/125600 [14:09:20<02:35,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 05:04:33,087 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/checkpoint-125000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 05:04:33,380 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 05:04:53,135 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 05:04:53,509 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 05:04:53,846 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125000/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 05:08:17,898 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 05:08:18,306 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 05:08:18,685 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/checkpoint-125500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|████████████████████████████████| 125600/125600 [14:14:30<00:00,  3.84it/s][INFO|trainer.py:1852] 2022-11-16 05:09:42,539 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 51270.2595, 'train_samples_per_second': 14.699, 'train_steps_per_second': 2.45, 'train_loss': 1.9559951608499904, 'epoch': 100.0}\n",
      "100%|████████████████████████████████| 125600/125600 [14:14:30<00:00,  2.45it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-16 05:09:42,679 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train07/\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 05:09:43,056 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train07/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 05:10:05,857 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train07/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 05:10:06,399 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train07/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 05:10:06,702 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train07/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       100.0\n",
      "  train_loss               =       1.956\n",
      "  train_runtime            = 14:14:30.25\n",
      "  train_samples            =        7536\n",
      "  train_samples_per_second =      14.699\n",
      "  train_steps_per_second   =        2.45\n",
      "11/16/2022 05:10:08 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-16 05:10:08,728 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-16 05:10:08,731 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-16 05:10:08,731 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-16 05:10:08,731 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:35<00:00, 11.37it/s]11/16/2022 05:10:47 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 31/3047 [00:00<00:09, 307.34it/s]\u001b[A\n",
      "  3%|█                                       | 77/3047 [00:00<00:07, 395.44it/s]\u001b[A\n",
      "  4%|█▌                                     | 126/3047 [00:00<00:06, 435.10it/s]\u001b[A\n",
      "  6%|██▎                                    | 176/3047 [00:00<00:06, 457.92it/s]\u001b[A\n",
      "  7%|██▉                                    | 226/3047 [00:00<00:06, 469.81it/s]\u001b[A\n",
      " 10%|███▊                                   | 294/3047 [00:00<00:06, 456.68it/s]\u001b[A\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 681, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 638, in main\n",
      "    metrics = trainer.evaluate()\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 57, in evaluate\n",
      "    eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
      "  File \"run_qa.py\", line 584, in post_processing_function\n",
      "    prefix=stage,\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/utils_qa.py\", line 204, in postprocess_qa_predictions\n",
      "    while predictions[i][\"text\"] == \"\":\n",
      "IndexError: list index out of range\n",
      "100%|█████████████████████████████████████████| 404/404 [00:38<00:00, 10.40it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 100 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train07/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31513341-4e18-4bf4-ba23-27159764d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 epochs breaks eval - trying 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e93351d3-8591-4560-93b2-3474ee592a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train08/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "f06fd993-a78f-476a-94fb-1c7484035b7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/16/2022 15:23:57 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/16/2022 15:23:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train08/runs/Nov16_15-23-56_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=50.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train08/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train08/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/16/2022 15:23:57 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/16/2022 15:23:57 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/16/2022 15:23:57 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/16/2022 15:23:57 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/16/2022 15:23:57 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/16/2022 15:23:57 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 127.34it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-16 15:23:58,022 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-16 15:23:58,028 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-16 15:23:58,182 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-16 15:23:58,182 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-16 15:23:58,182 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-16 15:23:58,182 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-16 15:23:58,845 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-16 15:24:11,690 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-16 15:24:11,691 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/16/2022 15:24:11 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/16/2022 15:24:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-16 15:24:15,208 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-16 15:24:15,242 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-16 15:24:15,242 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-16 15:24:15,242 >>   Num Epochs = 50\n",
      "[INFO|trainer.py:1610] 2022-11-16 15:24:15,242 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-16 15:24:15,242 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-16 15:24:15,242 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-16 15:24:15,242 >>   Total optimization steps = 62800\n",
      "  0%|                                                 | 0/62800 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.4622, 'learning_rate': 2.9765445859872613e-05, 'epoch': 0.4}         \n",
      "  1%|▎                                    | 500/62800 [02:10<4:29:55,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:26:25,274 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:26:25,573 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:27:04,232 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:27:04,449 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:27:04,699 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.5031, 'learning_rate': 2.95265923566879e-05, 'epoch': 0.8}           \n",
      "  2%|▌                                   | 1000/62800 [06:11<4:27:41,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:30:26,315 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:30:26,622 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:31:05,211 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:31:05,456 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:31:05,745 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5962, 'learning_rate': 2.9287738853503186e-05, 'epoch': 1.19}        \n",
      "  2%|▊                                   | 1500/62800 [10:15<4:25:42,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:34:30,403 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:34:30,671 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:35:08,544 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:35:08,812 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:35:09,051 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4488, 'learning_rate': 2.9048885350318473e-05, 'epoch': 1.59}        \n",
      "  3%|█▏                                  | 2000/62800 [14:20<4:23:10,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:38:35,625 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:38:35,965 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:39:11,823 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:39:12,079 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:39:12,299 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.7869, 'learning_rate': 2.8810509554140127e-05, 'epoch': 1.99}        \n",
      "  4%|█▍                                  | 2500/62800 [18:30<4:20:57,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:42:45,917 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:42:46,165 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:43:25,510 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:43:25,788 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:43:26,005 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4343, 'learning_rate': 2.8571656050955414e-05, 'epoch': 2.39}        \n",
      "  5%|█▋                                  | 3000/62800 [22:34<4:18:52,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:46:49,697 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:46:49,943 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:47:25,925 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:47:26,151 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:47:26,391 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4331, 'learning_rate': 2.83328025477707e-05, 'epoch': 2.79}          \n",
      "  6%|██                                  | 3500/62800 [26:38<4:16:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 15:50:54,107 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:50:54,350 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:51:31,700 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:51:31,949 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:51:32,175 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.4734, 'learning_rate': 2.8093949044585988e-05, 'epoch': 3.18}        \n",
      "  6%|██▎                                 | 4000/62800 [30:47<4:17:11,  3.81it/s][INFO|trainer.py:2671] 2022-11-16 15:55:03,081 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:55:03,311 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:55:42,028 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:55:42,300 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:55:42,518 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6575, 'learning_rate': 2.7855573248407642e-05, 'epoch': 3.58}        \n",
      "  7%|██▌                                 | 4500/62800 [35:02<4:12:00,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 15:59:17,791 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-4500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 15:59:18,170 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 15:59:53,198 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 15:59:53,559 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 15:59:53,774 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0617, 'learning_rate': 2.7616719745222932e-05, 'epoch': 3.98}        \n",
      "  8%|██▊                                 | 5000/62800 [39:01<4:09:58,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 16:03:17,125 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-5000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:03:17,416 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:03:54,284 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:03:54,580 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:03:54,889 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6273, 'learning_rate': 2.7377866242038216e-05, 'epoch': 4.38}        \n",
      "  9%|███▏                                | 5500/62800 [43:05<4:07:24,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 16:07:20,481 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-5500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:07:20,790 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:07:55,999 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:07:56,300 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:07:56,562 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-5500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5992, 'learning_rate': 2.7139012738853506e-05, 'epoch': 4.78}        \n",
      " 10%|███▍                                | 6000/62800 [47:15<4:06:58,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 16:11:30,817 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-6000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:11:31,065 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:12:08,667 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:12:08,979 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:12:09,307 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.7197, 'learning_rate': 2.6900636942675157e-05, 'epoch': 5.18}        \n",
      " 10%|███▋                                | 6500/62800 [51:37<4:04:03,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 16:15:52,865 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-6500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:15:53,145 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:16:36,574 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:16:36,872 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:16:37,247 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-6500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 11%|███▊                                | 6714/62800 [54:38<4:02:36,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.9545, 'learning_rate': 2.642292993630573e-05, 'epoch': 5.97}         \n",
      " 12%|████                              | 7500/62800 [1:00:00<3:58:43,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 16:24:15,360 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-7500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:24:15,654 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:24:51,052 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:24:51,297 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:24:51,605 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-7500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5306, 'learning_rate': 2.618407643312102e-05, 'epoch': 6.37}         \n",
      " 13%|████▎                             | 8000/62800 [1:04:05<3:57:46,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 16:28:20,321 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-8000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:28:20,603 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:29:08,958 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:29:09,172 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:29:09,423 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6676, 'learning_rate': 2.594570063694268e-05, 'epoch': 6.77}         \n",
      " 14%|████▌                             | 8500/62800 [1:08:23<3:55:16,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 16:32:38,969 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-8500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:32:39,242 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:33:18,072 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:33:18,328 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:33:18,576 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-8500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4218, 'learning_rate': 2.5706847133757962e-05, 'epoch': 7.17}        \n",
      " 14%|████▊                             | 9000/62800 [1:12:29<3:53:41,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 16:36:44,827 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-9000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:36:45,109 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:37:24,673 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:37:24,940 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:37:25,194 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4235, 'learning_rate': 2.546799363057325e-05, 'epoch': 7.56}         \n",
      " 15%|█████▏                            | 9500/62800 [1:16:33<3:51:41,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 16:40:48,717 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-9500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:40:49,040 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:41:27,527 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:41:27,828 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:41:28,102 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-9500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5572, 'learning_rate': 2.5229140127388536e-05, 'epoch': 7.96}        \n",
      " 16%|█████▎                           | 10000/62800 [1:20:37<3:50:59,  3.81it/s][INFO|trainer.py:2671] 2022-11-16 16:44:52,433 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-10000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:44:52,710 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:45:29,949 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:45:30,357 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:45:30,620 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2513, 'learning_rate': 2.4990764331210194e-05, 'epoch': 8.36}        \n",
      " 17%|█████▌                           | 10500/62800 [1:24:52<3:47:25,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 16:49:07,632 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-10500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:49:07,863 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:49:50,043 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:49:50,345 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:49:50,623 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-10500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3421, 'learning_rate': 2.4751910828025477e-05, 'epoch': 8.76}        \n",
      " 18%|█████▊                           | 11000/62800 [1:29:01<3:45:07,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 16:53:16,426 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-11000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:53:16,684 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:53:52,049 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:53:52,399 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:53:52,633 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3596, 'learning_rate': 2.4513057324840767e-05, 'epoch': 9.16}        \n",
      " 18%|██████                           | 11500/62800 [1:33:07<3:43:28,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 16:57:22,453 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-11500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 16:57:22,726 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 16:58:00,380 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 16:58:00,670 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 16:58:00,891 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-11500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3012, 'learning_rate': 2.427420382165605e-05, 'epoch': 9.55}         \n",
      " 19%|██████▎                          | 12000/62800 [1:37:08<3:40:06,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:01:23,330 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-12000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:01:23,592 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:02:03,256 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:02:03,577 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:02:03,854 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2814, 'learning_rate': 2.403582802547771e-05, 'epoch': 9.95}         \n",
      " 20%|██████▌                          | 12500/62800 [1:41:12<3:37:29,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:05:27,659 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-12500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:05:27,921 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:06:06,454 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:06:06,772 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:06:07,073 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-12500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0015, 'learning_rate': 2.3796974522292992e-05, 'epoch': 10.35}       \n",
      " 21%|██████▊                          | 13000/62800 [1:45:11<3:35:41,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:09:27,090 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-13000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:09:27,347 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:10:06,141 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:10:06,467 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:10:06,711 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0177, 'learning_rate': 2.355859872611465e-05, 'epoch': 10.75}        \n",
      " 21%|███████                          | 13500/62800 [1:49:16<3:34:26,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 17:13:31,726 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-13500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:13:31,986 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:14:10,403 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:14:10,776 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:14:11,065 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-13500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3904, 'learning_rate': 2.3319745222929937e-05, 'epoch': 11.15}       \n",
      " 22%|███████▎                         | 14000/62800 [1:53:23<3:32:26,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 17:17:38,979 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-14000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:17:39,270 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:18:21,012 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:18:21,253 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:18:21,560 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2519, 'learning_rate': 2.3080891719745223e-05, 'epoch': 11.54}       \n",
      " 23%|███████▌                         | 14500/62800 [1:57:28<3:29:41,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 17:21:43,833 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-14500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:21:44,455 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:22:22,466 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:22:22,715 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:22:22,966 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-14500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.712, 'learning_rate': 2.284203821656051e-05, 'epoch': 11.94}         \n",
      " 24%|███████▉                         | 15000/62800 [2:01:40<3:26:44,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:25:55,577 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-15000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:25:55,820 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:26:37,007 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:26:37,286 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:26:37,956 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.01, 'learning_rate': 2.2603184713375797e-05, 'epoch': 12.34}         \n",
      " 25%|████████▏                        | 15500/62800 [2:05:42<3:24:50,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:29:57,643 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-15500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:29:57,949 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:30:35,449 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:30:35,781 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:30:36,061 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-15500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3222, 'learning_rate': 2.2364331210191084e-05, 'epoch': 12.74}       \n",
      " 25%|████████▍                        | 16000/62800 [2:09:47<3:23:54,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 17:34:02,747 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-16000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:34:03,025 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:34:39,412 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:34:39,707 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:34:39,985 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 11.9965, 'learning_rate': 2.212595541401274e-05, 'epoch': 13.14}       \n",
      " 26%|████████▋                        | 16500/62800 [2:13:48<3:21:04,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 17:38:04,228 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-16500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:38:04,493 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:38:42,078 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:38:42,337 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:38:42,621 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-16500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3237, 'learning_rate': 2.1887101910828025e-05, 'epoch': 13.54}       \n",
      " 27%|████████▉                        | 17000/62800 [2:17:50<3:18:45,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 17:42:06,170 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-17000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:42:06,500 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:42:46,913 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:42:47,253 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:42:47,537 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2849, 'learning_rate': 2.1648248407643312e-05, 'epoch': 13.93}       \n",
      " 28%|█████████▏                       | 17500/62800 [2:22:01<3:16:42,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 17:46:16,316 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-17500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:46:16,561 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:46:55,619 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:46:55,843 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:46:56,096 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 2.0276, 'learning_rate': 2.14093949044586e-05, 'epoch': 14.33}         \n",
      " 29%|█████████▍                       | 18000/62800 [2:26:19<3:14:02,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:50:35,089 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-18000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:50:35,486 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:51:11,218 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:51:11,475 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:51:11,726 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.8946, 'learning_rate': 2.1171019108280257e-05, 'epoch': 14.73}       \n",
      " 29%|█████████▋                       | 18500/62800 [2:30:27<3:12:57,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 17:54:42,975 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-18500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:54:43,229 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:55:20,323 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:55:20,590 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:55:20,851 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-18500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2719, 'learning_rate': 2.0932165605095543e-05, 'epoch': 15.13}       \n",
      " 30%|█████████▉                       | 19000/62800 [2:34:30<3:09:43,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 17:58:45,857 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-19000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 17:58:46,189 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 17:59:19,600 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 17:59:19,868 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 17:59:20,094 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2253, 'learning_rate': 2.069331210191083e-05, 'epoch': 15.53}        \n",
      " 31%|██████████▏                      | 19500/62800 [2:38:28<3:07:15,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:02:44,064 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-19500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:02:44,324 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:03:18,236 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:03:18,458 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:03:18,755 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-19500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1949, 'learning_rate': 2.0454458598726117e-05, 'epoch': 15.92}       \n",
      " 32%|██████████▌                      | 20000/62800 [2:42:28<3:05:19,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:06:43,406 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-20000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:06:43,687 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:07:21,416 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:07:21,739 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:07:21,986 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5042, 'learning_rate': 2.02156050955414e-05, 'epoch': 16.32}         \n",
      " 33%|██████████▊                      | 20500/62800 [2:46:46<3:06:24,  3.78it/s][INFO|trainer.py:2671] 2022-11-16 18:11:01,906 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-20500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:11:02,252 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:11:36,872 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:11:37,153 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:11:37,434 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-20500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3934, 'learning_rate': 1.9976751592356687e-05, 'epoch': 16.72}       \n",
      " 33%|███████████                      | 21000/62800 [2:50:45<3:00:37,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 18:15:00,631 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-21000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:15:00,915 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:15:38,234 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:15:38,524 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:15:38,809 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-21000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 34%|███████████▏                     | 21264/62800 [2:53:48<3:00:18,  3.84it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4457, 'learning_rate': 1.949904458598726e-05, 'epoch': 17.52}        \n",
      " 35%|███████████▌                     | 22000/62800 [2:58:58<2:56:25,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:23:13,476 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-22000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:23:13,756 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:23:50,778 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:23:51,008 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:23:51,243 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3067, 'learning_rate': 1.926066878980892e-05, 'epoch': 17.91}        \n",
      " 36%|███████████▊                     | 22500/62800 [3:03:01<2:54:37,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:27:17,213 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-22500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:27:17,523 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:27:53,490 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:27:53,780 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:27:54,030 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-22500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8368, 'learning_rate': 1.9021815286624205e-05, 'epoch': 18.31}       \n",
      " 37%|████████████                     | 23000/62800 [3:07:11<2:52:17,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:31:27,161 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-23000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:31:27,455 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:32:02,944 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:32:03,270 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:32:03,566 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-23000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 37%|████████████▏                    | 23194/62800 [3:09:53<2:51:27,  3.85it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3431, 'learning_rate': 1.854410828025478e-05, 'epoch': 19.11}        \n",
      " 38%|████████████▌                    | 24000/62800 [3:14:39<2:47:39,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 18:38:54,655 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-24000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:38:55,231 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:39:15,477 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:39:15,838 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:39:16,145 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8724, 'learning_rate': 1.8305732484076433e-05, 'epoch': 19.51}       \n",
      " 39%|████████████▊                    | 24500/62800 [3:17:56<2:45:17,  3.86it/s][INFO|trainer.py:2671] 2022-11-16 18:42:12,184 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-24500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:42:12,549 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:42:34,751 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:42:35,089 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:42:35,464 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-24500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9137, 'learning_rate': 1.806687898089172e-05, 'epoch': 19.9}         \n",
      " 40%|█████████████▏                   | 25000/62800 [3:21:15<2:43:35,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:45:31,304 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-25000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:45:32,438 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:45:53,050 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:45:53,382 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:45:53,751 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.9934, 'learning_rate': 1.7828503184713375e-05, 'epoch': 20.3}        \n",
      " 41%|█████████████▍                   | 25500/62800 [3:24:35<2:41:39,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 18:48:51,255 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-25500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:48:51,612 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:49:14,334 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:49:14,685 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:49:15,022 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-25500/special_tokens_map.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:53:03,783 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:53:04,101 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:53:04,504 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0249, 'learning_rate': 1.7350796178343948e-05, 'epoch': 21.1}        \n",
      " 42%|█████████████▉                   | 26500/62800 [3:31:48<2:37:40,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 18:56:03,663 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-26500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:56:04,179 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:56:26,960 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:56:27,269 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:56:27,581 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-26500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8993, 'learning_rate': 1.7111942675159238e-05, 'epoch': 21.5}        \n",
      " 43%|██████████████▏                  | 27000/62800 [3:35:08<2:35:12,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 18:59:24,060 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-27000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 18:59:24,404 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 18:59:47,214 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 18:59:47,523 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 18:59:48,092 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9726, 'learning_rate': 1.687356687898089e-05, 'epoch': 21.89}        \n",
      " 44%|██████████████▍                  | 27500/62800 [3:38:30<2:32:40,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 19:02:45,844 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-27500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:02:46,193 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:03:07,675 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:03:08,036 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:03:08,370 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-27500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2816, 'learning_rate': 1.663471337579618e-05, 'epoch': 22.29}        \n",
      " 45%|██████████████▋                  | 28000/62800 [3:42:01<2:31:09,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:06:16,679 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-28000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:06:17,003 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:06:39,138 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:06:39,452 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:06:39,794 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.068, 'learning_rate': 1.6395859872611463e-05, 'epoch': 22.69}        \n",
      " 45%|██████████████▉                  | 28500/62800 [3:45:25<2:29:01,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:09:41,029 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-28500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:09:41,331 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:10:02,605 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:10:02,906 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:10:03,253 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-28500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.474, 'learning_rate': 1.6157006369426753e-05, 'epoch': 23.09}        \n",
      " 46%|███████████████▏                 | 29000/62800 [3:48:46<2:29:00,  3.78it/s][INFO|trainer.py:2671] 2022-11-16 19:13:01,664 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-29000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:13:02,400 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:13:25,083 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:13:25,357 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:13:25,626 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1175, 'learning_rate': 1.5918630573248408e-05, 'epoch': 23.49}       \n",
      " 47%|███████████████▌                 | 29500/62800 [3:52:05<2:24:24,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:16:21,234 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-29500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:16:21,528 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:16:43,474 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:16:43,783 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:16:44,144 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-29500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0701, 'learning_rate': 1.5679777070063695e-05, 'epoch': 23.89}       \n",
      " 48%|███████████████▊                 | 30000/62800 [3:55:27<2:22:39,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 19:19:42,833 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-30000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:19:43,201 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:20:05,943 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:20:06,251 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:20:06,865 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9486, 'learning_rate': 1.5441401273885353e-05, 'epoch': 24.28}       \n",
      " 49%|████████████████                 | 30500/62800 [3:58:50<2:19:57,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 19:23:05,503 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-30500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:23:05,922 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:23:26,613 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:23:26,998 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:23:27,297 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-30500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 49%|████████████████▎                | 30993/62800 [4:02:13<2:18:27,  3.83it/s][INFO|modeling_utils.py:1624] 2022-11-16 19:26:55,334 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:26:55,624 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:26:55,996 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1584, 'learning_rate': 1.4963694267515923e-05, 'epoch': 25.08}       \n",
      " 50%|████████████████▌                | 31500/62800 [4:05:39<2:15:49,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:29:54,553 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-31500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:29:54,862 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:30:17,613 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:30:17,942 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:30:18,288 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-31500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9624, 'learning_rate': 1.472484076433121e-05, 'epoch': 25.48}        \n",
      " 51%|████████████████▊                | 32000/62800 [4:09:01<2:13:46,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:33:17,114 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-32000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:33:17,500 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:33:38,211 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:33:38,580 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:33:38,859 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0049, 'learning_rate': 1.4485987261146496e-05, 'epoch': 25.88}       \n",
      " 52%|█████████████████                | 32500/62800 [4:12:24<2:12:16,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 19:36:40,258 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-32500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:36:40,676 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:37:00,783 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:37:01,090 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:37:01,368 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-32500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9568, 'learning_rate': 1.4247133757961783e-05, 'epoch': 26.27}       \n",
      " 53%|█████████████████▎               | 33000/62800 [4:15:40<2:09:19,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:39:56,306 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-33000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:39:56,653 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:40:19,365 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:40:19,767 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:40:20,089 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0158, 'learning_rate': 1.400828025477707e-05, 'epoch': 26.67}        \n",
      " 53%|█████████████████▌               | 33500/62800 [4:19:12<2:07:14,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:43:27,683 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-33500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:43:27,999 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:43:49,643 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:43:49,968 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:43:50,265 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-33500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1203, 'learning_rate': 1.3769426751592356e-05, 'epoch': 27.07}       \n",
      " 54%|█████████████████▊               | 34000/62800 [4:22:36<2:05:28,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 19:46:52,226 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-34000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:46:52,562 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-34000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:47:13,893 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-34000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:47:14,253 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-34000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:47:14,590 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-34000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 54%|█████████████████▉               | 34092/62800 [4:24:12<2:04:48,  3.83it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 55%|██████████████████▏              | 34573/62800 [4:27:33<2:02:53,  3.83it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8131, 'learning_rate': 1.3052866242038217e-05, 'epoch': 28.26}       \n",
      " 57%|██████████████████▋              | 35500/62800 [4:32:43<1:58:33,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 19:56:59,361 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-35500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 19:56:59,748 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-35500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 19:57:21,956 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-35500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 19:57:22,280 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-35500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 19:57:22,597 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-35500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4491, 'learning_rate': 1.2814012738853503e-05, 'epoch': 28.66}       \n",
      " 57%|██████████████████▉              | 36000/62800 [4:36:04<1:56:24,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 20:00:19,874 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-36000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:00:20,208 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:00:41,631 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:00:41,929 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:00:42,272 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1958, 'learning_rate': 1.257515923566879e-05, 'epoch': 29.06}        \n",
      " 58%|███████████████████▏             | 36500/62800 [4:39:24<1:54:44,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 20:03:39,457 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-36500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:03:39,765 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:04:01,576 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:04:01,914 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:04:02,254 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-36500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6801, 'learning_rate': 1.2336305732484077e-05, 'epoch': 29.46}       \n",
      " 59%|███████████████████▍             | 37000/62800 [4:42:43<1:52:17,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 20:06:58,772 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-37000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:06:59,090 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:07:30,002 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:07:30,355 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:07:30,715 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8671, 'learning_rate': 1.2097452229299364e-05, 'epoch': 29.86}       \n",
      " 60%|███████████████████▋             | 37500/62800 [4:46:17<1:50:08,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 20:10:32,605 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-37500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:10:32,915 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:10:53,621 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:10:53,982 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:10:54,310 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-37500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 60%|███████████████████▉             | 37969/62800 [4:49:31<1:48:08,  3.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6448, 'learning_rate': 1.1620222929936305e-05, 'epoch': 30.65}       \n",
      " 61%|████████████████████▏            | 38500/62800 [4:53:09<1:45:47,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 20:17:24,800 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-38500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:17:25,086 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-38500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:17:46,337 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-38500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:17:46,650 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-38500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:17:47,157 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-38500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7762, 'learning_rate': 1.1381369426751592e-05, 'epoch': 31.05}       \n",
      " 62%|████████████████████▍            | 39000/62800 [4:56:31<1:43:25,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 20:20:46,498 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-39000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:20:46,810 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:21:06,990 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:21:07,303 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:21:07,587 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.948, 'learning_rate': 1.114299363057325e-05, 'epoch': 31.45}         \n",
      " 63%|████████████████████▊            | 39500/62800 [4:59:47<1:40:21,  3.87it/s][INFO|trainer.py:2671] 2022-11-16 20:24:03,326 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-39500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:24:03,751 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:24:23,851 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:24:24,162 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:24:24,545 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-39500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9231, 'learning_rate': 1.0904617834394904e-05, 'epoch': 31.85}       \n",
      " 64%|█████████████████████            | 40000/62800 [5:03:04<1:39:02,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 20:27:20,414 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-40000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:27:20,860 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:27:40,297 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:27:40,720 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:27:41,009 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9105, 'learning_rate': 1.0665764331210191e-05, 'epoch': 32.25}       \n",
      " 64%|█████████████████████▎           | 40500/62800 [5:06:26<1:37:29,  3.81it/s][INFO|trainer.py:2671] 2022-11-16 20:30:42,088 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-40500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:30:42,435 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:31:02,705 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:31:03,000 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:31:03,413 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-40500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5984, 'learning_rate': 1.0426910828025478e-05, 'epoch': 32.64}       \n",
      " 65%|█████████████████████▌           | 41000/62800 [5:09:46<1:36:19,  3.77it/s][INFO|trainer.py:2671] 2022-11-16 20:34:02,226 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-41000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:34:02,556 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:34:26,751 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:34:27,173 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:34:27,540 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9159, 'learning_rate': 1.0188057324840765e-05, 'epoch': 33.04}       \n",
      " 66%|█████████████████████▊           | 41500/62800 [5:13:34<1:32:36,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 20:37:49,678 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-41500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:37:49,969 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:38:10,756 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:38:11,170 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:38:11,521 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-41500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8716, 'learning_rate': 9.949203821656051e-06, 'epoch': 33.44}        \n",
      " 67%|██████████████████████           | 42000/62800 [5:16:53<1:31:17,  3.80it/s][INFO|trainer.py:2671] 2022-11-16 20:41:08,845 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-42000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:41:09,141 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:41:32,337 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:41:32,639 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:41:32,946 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9519, 'learning_rate': 9.710350318471338e-06, 'epoch': 33.84}        \n",
      " 68%|██████████████████████▎          | 42500/62800 [5:20:14<1:28:13,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 20:44:30,356 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-42500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:44:30,657 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:44:52,368 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:44:52,727 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:44:53,071 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-42500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.663, 'learning_rate': 9.471496815286625e-06, 'epoch': 34.24}         \n",
      " 68%|██████████████████████▌          | 43000/62800 [5:23:35<1:26:20,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 20:47:51,281 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-43000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:47:51,627 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:48:11,958 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:48:12,263 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:48:12,675 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7027, 'learning_rate': 9.232643312101912e-06, 'epoch': 34.63}        \n",
      " 69%|██████████████████████▊          | 43500/62800 [5:26:55<1:24:24,  3.81it/s][INFO|trainer.py:2671] 2022-11-16 20:51:11,348 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-43500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:51:11,675 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:51:40,966 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:51:41,267 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:51:41,614 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-43500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6785, 'learning_rate': 8.993789808917198e-06, 'epoch': 35.03}        \n",
      " 70%|███████████████████████          | 44000/62800 [5:30:32<1:21:46,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 20:54:48,256 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-44000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:54:48,584 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:55:11,018 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:55:11,473 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:55:11,769 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4839, 'learning_rate': 8.754936305732485e-06, 'epoch': 35.43}        \n",
      " 71%|███████████████████████▍         | 44500/62800 [5:34:03<1:19:46,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 20:58:19,390 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-44500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 20:58:19,718 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 20:58:40,670 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 20:58:40,976 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 20:58:41,250 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-44500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0352, 'learning_rate': 8.516082802547772e-06, 'epoch': 35.83}        \n",
      " 72%|███████████████████████▋         | 45000/62800 [5:37:26<1:17:50,  3.81it/s][INFO|trainer.py:2671] 2022-11-16 21:01:41,739 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-45000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:01:42,115 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:02:04,715 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:02:05,058 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:02:05,385 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6355, 'learning_rate': 8.277229299363058e-06, 'epoch': 36.23}        \n",
      " 72%|███████████████████████▉         | 45500/62800 [5:40:48<1:15:18,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:05:03,865 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-45500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:05:04,171 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:05:26,956 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:05:27,287 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:05:27,683 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-45500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5381, 'learning_rate': 8.038853503184713e-06, 'epoch': 36.62}        \n",
      " 73%|████████████████████████▏        | 46000/62800 [5:44:14<1:13:05,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:08:30,051 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-46000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:08:30,383 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:08:50,668 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:08:51,114 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:08:51,395 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2415, 'learning_rate': 7.8e-06, 'epoch': 37.02}                      \n",
      " 74%|████████████████████████▍        | 46500/62800 [5:47:35<1:10:54,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:11:51,243 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-46500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:11:51,538 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:12:12,416 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:12:12,731 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:12:13,071 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-46500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9187, 'learning_rate': 7.561624203821656e-06, 'epoch': 37.42}        \n",
      " 75%|████████████████████████▋        | 47000/62800 [5:50:53<1:08:44,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:15:09,919 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-47000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:15:10,228 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:15:38,689 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:15:39,004 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:15:39,363 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7686, 'learning_rate': 7.322770700636943e-06, 'epoch': 37.82}        \n",
      " 76%|████████████████████████▉        | 47500/62800 [5:54:27<1:06:39,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:18:42,657 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-47500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:18:43,009 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:19:11,810 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:19:12,130 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:19:12,452 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-47500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6681, 'learning_rate': 7.08391719745223e-06, 'epoch': 38.22}         \n",
      " 76%|█████████████████████████▏       | 48000/62800 [5:57:57<1:04:37,  3.82it/s][INFO|trainer.py:2671] 2022-11-16 21:22:13,115 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-48000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:22:13,491 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:22:35,502 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:22:35,884 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:22:36,188 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8831, 'learning_rate': 6.845063694267516e-06, 'epoch': 38.61}        \n",
      " 77%|█████████████████████████▍       | 48500/62800 [6:01:20<1:02:02,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 21:25:36,043 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-48500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:25:36,395 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:25:59,253 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:25:59,582 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:25:59,869 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-48500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.852, 'learning_rate': 6.606210191082802e-06, 'epoch': 39.01}         \n",
      " 78%|███████████████████████████▎       | 49000/62800 [6:04:43<59:49,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 21:28:58,894 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-49000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:28:59,262 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:29:21,839 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:29:22,188 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:29:22,482 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7232, 'learning_rate': 6.367356687898089e-06, 'epoch': 39.41}        \n",
      " 79%|███████████████████████████▌       | 49500/62800 [6:08:08<57:43,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 21:32:23,980 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-49500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:32:24,400 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:32:47,268 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:32:47,596 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:32:47,872 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-49500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6486, 'learning_rate': 6.128503184713376e-06, 'epoch': 39.81}        \n",
      " 80%|███████████████████████████▊       | 50000/62800 [6:11:32<55:33,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 21:35:47,905 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-50000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:35:48,241 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:36:10,655 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:36:11,096 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:36:11,430 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8621, 'learning_rate': 5.8896496815286625e-06, 'epoch': 40.21}       \n",
      " 80%|████████████████████████████▏      | 50500/62800 [6:14:59<53:16,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 21:39:15,395 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-50500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:39:15,721 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:39:37,363 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:39:37,881 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:39:38,218 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-50500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9402, 'learning_rate': 5.651273885350319e-06, 'epoch': 40.61}        \n",
      " 81%|████████████████████████████▍      | 51000/62800 [6:18:19<51:17,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:42:34,836 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-51000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:42:35,181 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:42:57,455 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:42:57,758 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:42:58,065 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8257, 'learning_rate': 5.412420382165605e-06, 'epoch': 41.0}         \n",
      " 82%|████████████████████████████▋      | 51500/62800 [6:21:48<49:07,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 21:46:03,676 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-51500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:46:04,089 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:46:24,768 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:46:25,081 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:46:25,392 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-51500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.57, 'learning_rate': 5.1735668789808915e-06, 'epoch': 41.4}          \n",
      " 83%|████████████████████████████▉      | 52000/62800 [6:25:09<46:45,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 21:49:24,626 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-52000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:49:24,892 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:49:44,586 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:49:44,935 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:49:45,299 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.631, 'learning_rate': 4.934713375796178e-06, 'epoch': 41.8}          \n",
      " 84%|█████████████████████████████▎     | 52500/62800 [6:28:28<44:45,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 21:52:43,482 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-52500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:52:43,815 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:53:05,687 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:53:06,019 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:53:06,364 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-52500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7006, 'learning_rate': 4.696337579617835e-06, 'epoch': 42.2}         \n",
      " 84%|█████████████████████████████▌     | 53000/62800 [6:31:51<42:28,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 21:56:07,134 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-53000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:56:07,482 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:56:40,162 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:56:40,518 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:56:40,953 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6006, 'learning_rate': 4.457484076433121e-06, 'epoch': 42.6}         \n",
      " 85%|█████████████████████████████▊     | 53500/62800 [6:35:23<40:17,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 21:59:39,020 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-53500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 21:59:39,347 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 21:59:59,113 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 21:59:59,476 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 21:59:59,870 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-53500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0137, 'learning_rate': 4.218630573248408e-06, 'epoch': 42.99}        \n",
      " 86%|██████████████████████████████     | 54000/62800 [6:38:41<38:11,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 22:02:56,971 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-54000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:02:57,301 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:03:18,203 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:03:18,549 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:03:18,868 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8358, 'learning_rate': 3.979777070063695e-06, 'epoch': 43.39}        \n",
      " 87%|██████████████████████████████▎    | 54500/62800 [6:42:04<36:02,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 22:06:19,783 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-54500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:06:20,175 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:06:40,490 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:06:40,829 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:06:41,151 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-54500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9298, 'learning_rate': 3.7414012738853503e-06, 'epoch': 43.79}       \n",
      " 88%|██████████████████████████████▋    | 55000/62800 [6:45:24<33:46,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 22:09:39,691 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-55000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:09:40,069 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:10:01,256 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:10:01,636 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:10:02,026 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8361, 'learning_rate': 3.502547770700637e-06, 'epoch': 44.19}        \n",
      " 88%|██████████████████████████████▉    | 55500/62800 [6:48:46<31:47,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 22:13:01,782 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-55500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:13:02,211 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:13:25,745 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:13:26,089 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:13:26,431 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-55500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 89%|███████████████████████████████▏   | 55870/62800 [6:51:32<30:05,  3.84it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:27:11,333 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-57500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:27:11,655 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-57500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:27:11,949 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-57500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8263, 'learning_rate': 2.3087579617834395e-06, 'epoch': 46.18}       \n",
      " 92%|████████████████████████████████▎  | 58000/62800 [7:06:02<20:53,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 22:30:18,322 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-58000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:30:18,786 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:30:41,182 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:30:41,570 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:30:41,907 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4891, 'learning_rate': 2.0699044585987262e-06, 'epoch': 46.58}       \n",
      " 93%|████████████████████████████████▌  | 58500/62800 [7:09:27<18:37,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 22:33:42,735 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-58500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:33:43,037 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:34:05,833 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:34:06,150 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:34:06,531 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-58500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5556, 'learning_rate': 1.8315286624203822e-06, 'epoch': 46.97}       \n",
      " 94%|████████████████████████████████▉  | 59000/62800 [7:12:47<16:37,  3.81it/s][INFO|trainer.py:2671] 2022-11-16 22:37:03,317 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-59000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:37:03,610 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:37:25,957 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:37:26,258 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:37:26,990 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.6579, 'learning_rate': 1.592675159235669e-06, 'epoch': 47.37}        \n",
      " 95%|█████████████████████████████████▏ | 59500/62800 [7:16:08<14:19,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 22:40:23,455 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-59500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:40:23,771 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:40:46,245 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:40:46,556 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:40:47,280 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-59500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.4718, 'learning_rate': 1.3538216560509554e-06, 'epoch': 47.77}       \n",
      " 96%|█████████████████████████████████▍ | 60000/62800 [7:19:29<12:11,  3.83it/s][INFO|trainer.py:2671] 2022-11-16 22:43:44,875 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-60000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:43:45,168 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:44:05,400 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:44:05,718 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:44:06,011 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7222, 'learning_rate': 1.114968152866242e-06, 'epoch': 48.17}        \n",
      " 96%|█████████████████████████████████▋ | 60500/62800 [7:22:49<09:58,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 22:47:04,630 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-60500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:47:04,965 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:47:25,820 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:47:26,172 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:47:26,493 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-60500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5795, 'learning_rate': 8.765923566878981e-07, 'epoch': 48.57}        \n",
      " 97%|█████████████████████████████████▉ | 61000/62800 [7:26:09<07:48,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 22:50:24,816 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-61000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:50:25,205 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:50:49,122 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:50:49,411 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:50:49,712 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7848, 'learning_rate': 6.377388535031846e-07, 'epoch': 48.96}        \n",
      " 98%|██████████████████████████████████▎| 61500/62800 [7:29:30<05:37,  3.85it/s][INFO|trainer.py:2671] 2022-11-16 22:53:45,873 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-61500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:53:46,229 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:54:15,787 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:54:16,061 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:54:16,337 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-61500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5907, 'learning_rate': 3.988853503184714e-07, 'epoch': 49.36}        \n",
      " 99%|██████████████████████████████████▌| 62000/62800 [7:32:58<03:28,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 22:57:13,888 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-62000\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 22:57:14,235 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 22:57:35,013 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 22:57:35,359 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 22:57:35,818 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5547, 'learning_rate': 1.6003184713375795e-07, 'epoch': 49.76}       \n",
      "100%|██████████████████████████████████▊| 62500/62800 [7:36:16<01:18,  3.84it/s][INFO|trainer.py:2671] 2022-11-16 23:00:32,231 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/checkpoint-62500\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 23:00:32,567 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 23:00:52,913 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 23:00:53,264 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 23:00:53,590 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/checkpoint-62500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████| 62800/62800 [7:38:44<00:00,  3.83it/s][INFO|trainer.py:1852] 2022-11-16 23:02:59,967 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 27524.7247, 'train_samples_per_second': 13.69, 'train_steps_per_second': 2.282, 'train_loss': 2.2548915085519194, 'epoch': 50.0}\n",
      "100%|███████████████████████████████████| 62800/62800 [7:38:44<00:00,  2.28it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-16 23:03:00,032 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train08/\n",
      "[INFO|configuration_utils.py:447] 2022-11-16 23:03:00,347 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train08/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-16 23:03:21,654 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train08/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-16 23:03:21,980 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train08/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-16 23:03:22,293 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train08/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       50.0\n",
      "  train_loss               =     2.2549\n",
      "  train_runtime            = 7:38:44.72\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =      13.69\n",
      "  train_steps_per_second   =      2.282\n",
      "11/16/2022 23:03:24 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-16 23:03:24,129 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-16 23:03:24,131 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-16 23:03:24,132 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-16 23:03:24,132 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:34<00:00, 11.44it/s]11/16/2022 23:04:02 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 31/3047 [00:00<00:09, 306.97it/s]\u001b[A\n",
      "  3%|█                                       | 78/3047 [00:00<00:07, 398.93it/s]\u001b[A\n",
      "  4%|█▋                                     | 127/3047 [00:00<00:06, 436.91it/s]\u001b[A\n",
      "  6%|██▎                                    | 176/3047 [00:00<00:06, 457.16it/s]\u001b[A\n",
      "  7%|██▉                                    | 225/3047 [00:00<00:06, 465.75it/s]\u001b[A\n",
      "  9%|███▌                                   | 274/3047 [00:00<00:05, 471.90it/s]\u001b[A\n",
      " 11%|████                                   | 322/3047 [00:00<00:05, 468.17it/s]\u001b[A\n",
      " 12%|████▋                                  | 369/3047 [00:00<00:05, 460.96it/s]\u001b[A\n",
      " 14%|█████▎                                 | 418/3047 [00:00<00:05, 468.32it/s]\u001b[A\n",
      " 15%|█████▉                                 | 465/3047 [00:01<00:06, 426.61it/s]\u001b[A\n",
      " 18%|██████▉                                | 542/3047 [00:01<00:05, 433.56it/s]\u001b[A\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 681, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 638, in main\n",
      "    metrics = trainer.evaluate()\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 57, in evaluate\n",
      "    eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
      "  File \"run_qa.py\", line 584, in post_processing_function\n",
      "    prefix=stage,\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/utils_qa.py\", line 204, in postprocess_qa_predictions\n",
      "    while predictions[i][\"text\"] == \"\":\n",
      "IndexError: list index out of range\n",
      "100%|█████████████████████████████████████████| 404/404 [00:39<00:00, 10.25it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 50 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train08/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390cdc7-0f7a-4601-8479-edc91f72b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "ab492626-1eb4-4df3-9fda-7d96cd99a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir /home/jupyter/gcs/cdcmodel_train11/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "ec2cbbac-8c52-4ca5-ba39-a8ea0048f935",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/21/2022 01:51:09 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "11/21/2022 01:51:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=1,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/jupyter/gcs/cdcmodel_train11/runs/Nov21_01-51-08_first-gpu-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=20.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=/home/jupyter/gcs/cdcmodel_train11/,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/jupyter/gcs/cdcmodel_train11/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "11/21/2022 01:51:09 - INFO - datasets.builder - No config specified, defaulting to the single config: dataset/plain_text\n",
      "11/21/2022 01:51:09 - INFO - datasets.info - Loading Dataset Infos from /home/jupyter/.cache/huggingface/modules/datasets_modules/datasets/dataset/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/21/2022 01:51:09 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "11/21/2022 01:51:09 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "11/21/2022 01:51:09 - WARNING - datasets.builder - Found cached dataset dataset (/home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb)\n",
      "11/21/2022 01:51:09 - INFO - datasets.info - Loading Dataset info from /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 49.79it/s]\n",
      "[INFO|configuration_utils.py:653] 2022-11-21 01:51:09,915 >> loading configuration file config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/config.json\n",
      "[INFO|configuration_utils.py:705] 2022-11-21 01:51:09,921 >> Model config BloomConfig {\n",
      "  \"_name_or_path\": \"bigscience/bloom-560m\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"transformers_version\": \"4.24.0.dev0\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-21 01:51:10,079 >> loading file tokenizer.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-21 01:51:10,079 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-21 01:51:10,079 >> loading file special_tokens_map.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1775] 2022-11-21 01:51:10,079 >> loading file tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:2156] 2022-11-21 01:51:10,897 >> loading weights file pytorch_model.bin from cache at /home/jupyter/.cache/huggingface/hub/models--bigscience--bloom-560m/snapshots/afe2e6f33eb135d254df849c74bb83322b53641c/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2022-11-21 01:51:24,390 >> All model checkpoint weights were used when initializing BloomForQuestionAnswering.\n",
      "\n",
      "[WARNING|modeling_utils.py:2609] 2022-11-21 01:51:24,390 >> Some weights of BloomForQuestionAnswering were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/21/2022 01:51:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-a98e25a74cf0c2f6.arrow\n",
      "11/21/2022 01:51:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/dataset/plain_text/1.0.0/cb09324d802e9c68ea628d81253aca6f30dca08142b40269a52e9fbb856273eb/cache-15990e383ba56028.arrow\n",
      "[INFO|trainer.py:557] 2022-11-21 01:51:28,085 >> Using cuda_amp half precision backend\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "[INFO|trainer.py:1607] 2022-11-21 01:51:28,137 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-11-21 01:51:28,137 >>   Num examples = 7536\n",
      "[INFO|trainer.py:1609] 2022-11-21 01:51:28,137 >>   Num Epochs = 20\n",
      "[INFO|trainer.py:1610] 2022-11-21 01:51:28,137 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:1611] 2022-11-21 01:51:28,137 >>   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "[INFO|trainer.py:1612] 2022-11-21 01:51:28,137 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-11-21 01:51:28,137 >>   Total optimization steps = 25120\n",
      "  0%|                                                 | 0/25120 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 7.5315, 'learning_rate': 2.9414808917197454e-05, 'epoch': 0.4}         \n",
      "  2%|▋                                    | 500/25120 [02:10<1:47:12,  3.83it/s][INFO|trainer.py:2671] 2022-11-21 01:53:38,830 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 01:53:39,288 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 01:53:59,149 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 01:53:59,502 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 01:53:59,921 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 4.5468, 'learning_rate': 2.881767515923567e-05, 'epoch': 0.8}          \n",
      "  4%|█▍                                  | 1000/25120 [05:50<1:44:44,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 01:57:19,151 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-1000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 01:57:19,421 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 01:57:49,152 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 01:57:49,442 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 01:57:49,774 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5593, 'learning_rate': 2.8220541401273887e-05, 'epoch': 1.19}        \n",
      "  6%|██▏                                 | 1500/25120 [09:21<1:42:01,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:00:49,814 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-1500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:00:50,121 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:01:09,490 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:01:09,867 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:01:10,207 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-1500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.3951, 'learning_rate': 2.76234076433121e-05, 'epoch': 1.59}          \n",
      "  8%|██▊                                 | 2000/25120 [12:39<1:39:40,  3.87it/s][INFO|trainer.py:2671] 2022-11-21 02:04:07,690 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-2000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:04:07,996 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:04:30,126 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:04:30,432 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:04:30,743 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.6746, 'learning_rate': 2.7026273885350318e-05, 'epoch': 1.99}        \n",
      " 10%|███▌                                | 2500/25120 [16:04<1:37:48,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 02:07:33,148 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-2500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:07:33,466 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:07:53,240 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:07:53,534 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:07:53,872 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-2500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.5373, 'learning_rate': 2.6429140127388538e-05, 'epoch': 2.39}        \n",
      " 12%|████▎                               | 3000/25120 [19:22<1:35:15,  3.87it/s][INFO|trainer.py:2671] 2022-11-21 02:10:50,862 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-3000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:10:51,652 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:11:13,790 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:11:14,206 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:11:14,502 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.3421, 'learning_rate': 2.583200636942675e-05, 'epoch': 2.79}         \n",
      " 14%|█████                               | 3500/25120 [22:44<1:33:17,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:14:12,956 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-3500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:14:13,253 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:14:37,488 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:14:37,759 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:14:38,038 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-3500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.3664, 'learning_rate': 2.5234872611464968e-05, 'epoch': 3.18}        \n",
      " 16%|█████▋                              | 4000/25120 [26:08<1:31:18,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:17:36,777 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-4000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:17:37,081 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:17:58,940 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:17:59,224 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:17:59,493 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5266, 'learning_rate': 2.463893312101911e-05, 'epoch': 3.58}         \n",
      " 18%|██████▍                             | 4500/25120 [29:24<1:29:26,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 02:20:52,977 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-4500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:20:53,312 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:21:13,933 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:21:14,960 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:21:15,299 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-4500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 3.0146, 'learning_rate': 2.4041799363057323e-05, 'epoch': 3.98}        \n",
      " 20%|███████▏                            | 5000/25120 [32:43<1:26:42,  3.87it/s][INFO|trainer.py:2671] 2022-11-21 02:24:11,870 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-5000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:24:12,181 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:24:33,379 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:24:33,735 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:24:34,075 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4394, 'learning_rate': 2.3444665605095543e-05, 'epoch': 4.38}        \n",
      " 22%|███████▉                            | 5500/25120 [36:06<1:24:47,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:27:34,909 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-5500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:27:35,219 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:27:58,642 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:27:58,964 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:27:59,226 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-5500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.6542, 'learning_rate': 2.284753184713376e-05, 'epoch': 4.78}         \n",
      " 24%|████████▌                           | 6000/25120 [39:28<1:22:35,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:30:57,243 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-6000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:30:57,604 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:31:18,895 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:31:19,245 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:31:19,606 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.5128, 'learning_rate': 2.22515923566879e-05, 'epoch': 5.18}          \n",
      " 26%|█████████▎                          | 6500/25120 [42:54<1:20:15,  3.87it/s][INFO|trainer.py:2671] 2022-11-21 02:34:22,880 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-6500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:34:23,180 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:34:43,344 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:34:43,667 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:34:43,990 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-6500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4535, 'learning_rate': 2.1654458598726115e-05, 'epoch': 5.57}        \n",
      " 28%|██████████                          | 7000/25120 [46:15<1:19:05,  3.82it/s][INFO|trainer.py:2671] 2022-11-21 02:37:43,897 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-7000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:37:44,216 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:38:06,755 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:38:07,065 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:38:07,339 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.7765, 'learning_rate': 2.105732484076433e-05, 'epoch': 5.97}         \n",
      " 30%|██████████▋                         | 7500/25120 [49:44<1:16:16,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 02:41:13,304 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-7500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:41:13,643 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:41:33,331 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:41:33,624 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:41:33,934 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-7500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2506, 'learning_rate': 2.046019108280255e-05, 'epoch': 6.37}         \n",
      " 32%|███████████▍                        | 8000/25120 [53:04<1:14:03,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 02:44:33,017 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-8000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:44:33,332 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:45:00,973 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:45:01,417 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:45:01,789 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.3873, 'learning_rate': 1.9864251592356688e-05, 'epoch': 6.77}        \n",
      " 34%|████████████▏                       | 8500/25120 [56:32<1:11:44,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:48:01,111 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-8500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:48:01,404 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:48:22,070 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:48:22,373 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:48:22,652 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-8500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 35%|████████████▍                       | 8667/25120 [58:27<1:11:07,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.2593, 'learning_rate': 1.866998407643312e-05, 'epoch': 7.56}         \n",
      " 38%|████████████▊                     | 9500/25120 [1:03:16<1:07:40,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 02:54:45,278 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-9500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:54:45,630 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:55:05,230 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:55:05,527 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:55:05,798 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-9500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4399, 'learning_rate': 1.8072850318471338e-05, 'epoch': 7.96}        \n",
      " 40%|█████████████▏                   | 10000/25120 [1:06:40<1:05:20,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 02:58:08,726 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-10000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 02:58:09,044 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 02:58:28,908 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 02:58:29,185 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 02:58:29,562 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1113, 'learning_rate': 1.747691082802548e-05, 'epoch': 8.36}         \n",
      " 42%|█████████████▊                   | 10500/25120 [1:10:11<1:03:24,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 03:01:39,793 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-10500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:01:40,112 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:02:03,669 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:02:04,085 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:02:04,367 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-10500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.293, 'learning_rate': 1.6879777070063697e-05, 'epoch': 8.76}         \n",
      " 44%|██████████████▍                  | 11000/25120 [1:13:33<1:01:17,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 03:05:02,058 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-11000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:05:02,446 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:05:22,082 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:05:22,438 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:05:22,783 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-11000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 44%|██████████████▌                  | 11043/25120 [1:15:06<1:00:46,  3.86it/s]/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7239, 'learning_rate': 1.4492436305732484e-05, 'epoch': 10.35}       \n",
      " 52%|██████████████████                 | 13000/25120 [1:27:09<52:36,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 03:18:37,330 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-13000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:18:37,684 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:19:00,202 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:19:00,644 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:19:01,001 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.4635, 'learning_rate': 1.38953025477707e-05, 'epoch': 10.75}         \n",
      " 54%|██████████████████▊                | 13500/25120 [1:30:29<50:29,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 03:21:57,491 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-13500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:21:57,821 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:22:19,142 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:22:19,445 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:22:19,761 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-13500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0266, 'learning_rate': 1.3298168789808917e-05, 'epoch': 11.15}       \n",
      " 56%|███████████████████▌               | 14000/25120 [1:33:48<48:11,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 03:25:16,558 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-14000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:25:16,843 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:25:38,368 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:25:38,731 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:25:39,016 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.0241, 'learning_rate': 1.2702229299363058e-05, 'epoch': 11.54}       \n",
      " 58%|████████████████████▏              | 14500/25120 [1:37:07<45:54,  3.86it/s][INFO|trainer.py:2671] 2022-11-21 03:28:35,662 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-14500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:28:35,930 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:29:00,472 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:29:00,745 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:29:01,152 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-14500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.1812, 'learning_rate': 1.2105095541401274e-05, 'epoch': 11.94}       \n",
      " 60%|████████████████████▉              | 15000/25120 [1:40:44<43:52,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 03:32:12,547 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-15000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:32:12,891 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:32:39,470 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:32:39,845 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:32:40,297 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-15000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      " 62%|█████████████████████▌             | 15469/25120 [1:44:01<41:40,  3.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8845, 'learning_rate': 7.927547770700637e-06, 'epoch': 14.73}        \n",
      " 74%|█████████████████████████▊         | 18500/25120 [2:04:13<28:44,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 03:55:41,867 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-18500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:55:42,227 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:56:02,495 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:56:02,776 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:56:03,097 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-18500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8036, 'learning_rate': 7.330414012738853e-06, 'epoch': 15.13}        \n",
      " 76%|██████████████████████████▍        | 19000/25120 [2:07:34<26:39,  3.83it/s][INFO|trainer.py:2671] 2022-11-21 03:59:02,493 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-19000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 03:59:02,803 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 03:59:23,672 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 03:59:24,018 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 03:59:24,269 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8352, 'learning_rate': 6.73328025477707e-06, 'epoch': 15.53}         \n",
      " 78%|███████████████████████████▏       | 19500/25120 [2:10:53<24:21,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 04:02:21,910 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-19500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:02:22,240 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:02:42,078 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:02:42,389 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:02:42,706 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-19500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7494, 'learning_rate': 6.136146496815287e-06, 'epoch': 15.92}        \n",
      " 80%|███████████████████████████▊       | 20000/25120 [2:14:12<22:15,  3.83it/s][INFO|trainer.py:2671] 2022-11-21 04:05:41,207 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-20000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:05:41,555 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:06:02,884 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:06:03,210 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:06:03,518 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9054, 'learning_rate': 5.540207006369427e-06, 'epoch': 16.32}        \n",
      " 82%|████████████████████████████▌      | 20500/25120 [2:17:43<20:00,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 04:09:11,639 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-20500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:09:11,931 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:09:33,056 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:09:33,364 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:09:33,688 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-20500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.9424, 'learning_rate': 4.943073248407644e-06, 'epoch': 16.72}        \n",
      " 84%|█████████████████████████████▎     | 21000/25120 [2:21:06<17:50,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 04:12:34,701 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-21000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:12:34,978 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:13:00,600 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:13:00,929 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:13:01,289 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8151, 'learning_rate': 4.34593949044586e-06, 'epoch': 17.12}         \n",
      " 86%|█████████████████████████████▉     | 21500/25120 [2:24:31<15:41,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 04:15:59,463 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-21500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:15:59,773 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:16:20,596 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:16:20,895 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:16:21,184 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-21500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.8903, 'learning_rate': 3.7488057324840764e-06, 'epoch': 17.52}       \n",
      " 88%|██████████████████████████████▋    | 22000/25120 [2:27:53<13:36,  3.82it/s][INFO|trainer.py:2671] 2022-11-21 04:19:22,108 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-22000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:19:22,396 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:19:44,407 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:19:44,762 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:19:45,041 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7528, 'learning_rate': 3.152866242038217e-06, 'epoch': 17.91}        \n",
      " 90%|███████████████████████████████▎   | 22500/25120 [2:31:16<11:24,  3.83it/s][INFO|trainer.py:2671] 2022-11-21 04:22:45,245 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-22500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:22:45,545 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:23:05,334 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:23:05,612 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:23:05,866 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-22500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.5568, 'learning_rate': 2.555732484076433e-06, 'epoch': 18.31}        \n",
      " 92%|████████████████████████████████   | 23000/25120 [2:34:37<09:10,  3.85it/s][INFO|trainer.py:2671] 2022-11-21 04:26:05,807 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-23000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:26:06,099 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:26:27,316 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:26:27,630 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:26:27,947 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 2.004, 'learning_rate': 1.9585987261146497e-06, 'epoch': 18.71}        \n",
      " 94%|████████████████████████████████▋  | 23500/25120 [2:37:58<07:01,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 04:29:27,040 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-23500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:29:27,345 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:29:50,773 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:29:51,057 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:29:51,385 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-23500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7858, 'learning_rate': 1.3614649681528661e-06, 'epoch': 19.11}       \n",
      " 96%|█████████████████████████████████▍ | 24000/25120 [2:41:30<04:51,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 04:32:58,745 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-24000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:32:59,027 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:33:22,338 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:33:22,623 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:33:22,904 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.7296, 'learning_rate': 7.655254777070064e-07, 'epoch': 19.51}        \n",
      " 98%|██████████████████████████████████▏| 24500/25120 [2:44:51<02:41,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 04:36:19,869 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-24500\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:36:20,212 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:36:41,888 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:36:42,174 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:36:42,457 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-24500/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "{'loss': 1.3656, 'learning_rate': 1.6839171974522293e-07, 'epoch': 19.9}        \n",
      "100%|██████████████████████████████████▊| 25000/25120 [2:48:11<00:31,  3.84it/s][INFO|trainer.py:2671] 2022-11-21 04:39:40,286 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/checkpoint-25000\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:39:40,638 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:40:03,869 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:40:04,156 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:40:04,483 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/checkpoint-25000/special_tokens_map.json\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n",
      "100%|███████████████████████████████████| 25120/25120 [2:49:56<00:00,  3.83it/s][INFO|trainer.py:1852] 2022-11-21 04:41:25,089 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10196.9516, 'train_samples_per_second': 14.781, 'train_steps_per_second': 2.463, 'train_loss': 2.4144237244964404, 'epoch': 20.0}\n",
      "100%|███████████████████████████████████| 25120/25120 [2:49:56<00:00,  2.46it/s]\n",
      "[INFO|trainer.py:2671] 2022-11-21 04:41:25,134 >> Saving model checkpoint to /home/jupyter/gcs/cdcmodel_train11/\n",
      "[INFO|configuration_utils.py:447] 2022-11-21 04:41:25,480 >> Configuration saved in /home/jupyter/gcs/cdcmodel_train11/config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-11-21 04:41:54,773 >> Model weights saved in /home/jupyter/gcs/cdcmodel_train11/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2125] 2022-11-21 04:41:55,060 >> tokenizer config file saved in /home/jupyter/gcs/cdcmodel_train11/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2132] 2022-11-21 04:41:55,358 >> Special tokens file saved in /home/jupyter/gcs/cdcmodel_train11/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  train_loss               =     2.4144\n",
      "  train_runtime            = 2:49:56.95\n",
      "  train_samples            =       7536\n",
      "  train_samples_per_second =     14.781\n",
      "  train_steps_per_second   =      2.463\n",
      "11/21/2022 04:41:57 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:726] 2022-11-21 04:41:57,319 >> The following columns in the evaluation set don't have a corresponding argument in `BloomForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BloomForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2922] 2022-11-21 04:41:57,321 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2924] 2022-11-21 04:41:57,321 >>   Num examples = 3229\n",
      "[INFO|trainer.py:2927] 2022-11-21 04:41:57,321 >>   Batch size = 8\n",
      "100%|████████████████████████████████████████▉| 403/404 [00:35<00:00, 11.16it/s]11/21/2022 04:42:35 - INFO - utils_qa - Post-processing 3047 example predictions split into 3229 features.\n",
      "\n",
      "  0%|                                                  | 0/3047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 33/3047 [00:00<00:09, 325.50it/s]\u001b[A\n",
      "  2%|▉                                       | 75/3047 [00:00<00:07, 378.22it/s]\u001b[A\n",
      "  4%|█▌                                     | 119/3047 [00:00<00:07, 402.89it/s]\u001b[A\n",
      "  5%|██                                     | 164/3047 [00:00<00:06, 419.75it/s]\u001b[A\n",
      "  7%|██▋                                    | 208/3047 [00:00<00:06, 426.05it/s]\u001b[A\n",
      " 10%|███▊                                   | 294/3047 [00:00<00:06, 421.64it/s]\u001b[A\n",
      "Traceback (most recent call last):\n",
      "  File \"run_qa.py\", line 681, in <module>\n",
      "    main()\n",
      "  File \"run_qa.py\", line 638, in main\n",
      "    metrics = trainer.evaluate()\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 57, in evaluate\n",
      "    eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
      "  File \"run_qa.py\", line 584, in post_processing_function\n",
      "    prefix=stage,\n",
      "  File \"/home/jupyter/QA_tune/transformers/examples/pytorch/question-answering/utils_qa.py\", line 204, in postprocess_qa_predictions\n",
      "    while predictions[i][\"text\"] == \"\":\n",
      "IndexError: list index out of range\n",
      "100%|█████████████████████████████████████████| 404/404 [00:39<00:00, 10.33it/s]\n"
     ]
    }
   ],
   "source": [
    "!python run_qa.py \\\n",
    "  --model_name_or_path bigscience/bloom-560m \\\n",
    "  --dataset_name dataset \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 6 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 20 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir /home/jupyter/gcs/cdcmodel_train11/ \\\n",
    "  --eval_accumulation_steps 1 \\\n",
    "  --version_2_with_negative \\\n",
    "  --overwrite_output_dir \\\n",
    "  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646c30a-71ac-4ac9-bd00-ec73bdc529b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09c1e7-6e59-49a1-9104-50bf1907c78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9f01b-3f6c-424d-93a2-a9787e1a5fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c187e-ddd5-4a3b-bfd0-54f0544f8301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b6834-fd31-4ac2-bfa5-eba16625989c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c990cd-e25a-48a8-b1c1-364a70eaa61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca8f65-96a2-4856-9d17-bf347b755dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d12386-b73e-4054-acb7-01433ed2fb15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a389d2b1-efb6-4e4a-abe1-8101acfff3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39118266-a8d2-48bf-ad69-d0f59f446ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce912c-7067-4dda-953f-66572a8137ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c357c770-a078-45df-8db2-dd926b7902ab",
   "metadata": {},
   "source": [
    "### Previous output from SQuAD training POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b268463-c6a9-419e-93bd-699f7f742ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUCCESS!! Final output and details below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0408b7a9-b9da-408a-a960-45ee46cfc563",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\\n{'train_runtime': 15323.5402, 'train_samples_per_second': 17.209, 'train_steps_per_second': 2.868, 'train_loss': 3.5304283500368654, 'epoch': 2.0}\\n100%|███████████████████████████████████| 43952/43952 [4:15:23<00:00,  2.87it/s]\\n[INFO|trainer.py:2671] 2022-10-22 22:33:05,661 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/\\n[INFO|configuration_utils.py:447] 2022-10-22 22:33:05,662 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/config.json\\n[INFO|modeling_utils.py:1624] 2022-10-22 22:33:11,183 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/pytorch_model.bin\\n[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:33:11,183 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/tokenizer_config.json\\n[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:33:11,184 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/special_tokens_map.json\\n***** train metrics *****\\n  epoch                    =        2.0\\n  train_loss               =     3.5304\\n  train_runtime            = 4:15:23.54\\n  train_samples            =     131854\\n  train_samples_per_second =     17.209\\n  train_steps_per_second   =      2.868\\n[INFO|modelcard.py:444] 2022-10-22 22:33:13,465 >> Dropping the following result as it does not have all the necessary fields:\\n{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'config': 'squad_v2', 'split': 'train', 'args': 'squad_v2'}}\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
    "\n",
    "\n",
    "{'train_runtime': 15323.5402, 'train_samples_per_second': 17.209, 'train_steps_per_second': 2.868, 'train_loss': 3.5304283500368654, 'epoch': 2.0}\n",
    "100%|███████████████████████████████████| 43952/43952 [4:15:23<00:00,  2.87it/s]\n",
    "[INFO|trainer.py:2671] 2022-10-22 22:33:05,661 >> Saving model checkpoint to /home/jupyter/tmp/debug_bloom_squad/\n",
    "[INFO|configuration_utils.py:447] 2022-10-22 22:33:05,662 >> Configuration saved in /home/jupyter/tmp/debug_bloom_squad/config.json\n",
    "[INFO|modeling_utils.py:1624] 2022-10-22 22:33:11,183 >> Model weights saved in /home/jupyter/tmp/debug_bloom_squad/pytorch_model.bin\n",
    "[INFO|tokenization_utils_base.py:2125] 2022-10-22 22:33:11,183 >> tokenizer config file saved in /home/jupyter/tmp/debug_bloom_squad/tokenizer_config.json\n",
    "[INFO|tokenization_utils_base.py:2132] 2022-10-22 22:33:11,184 >> Special tokens file saved in /home/jupyter/tmp/debug_bloom_squad/special_tokens_map.json\n",
    "***** train metrics *****\n",
    "  epoch                    =        2.0\n",
    "  train_loss               =     3.5304\n",
    "  train_runtime            = 4:15:23.54\n",
    "  train_samples            =     131854\n",
    "  train_samples_per_second =     17.209\n",
    "  train_steps_per_second   =      2.868\n",
    "[INFO|modelcard.py:444] 2022-10-22 22:33:13,465 >> Dropping the following result as it does not have all the necessary fields:\n",
    "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad_v2', 'type': 'squad_v2', 'config': 'squad_v2', 'split': 'train', 'args': 'squad_v2'}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2126558-465f-415e-98c8-7cd5338b15b4",
   "metadata": {},
   "source": [
    "## Model is now built. Let's see if we can figure out how to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "332e184d-11fb-4e52-8917-191a7960483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6737953-e921-4dc2-a341-88a41c7d5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3280ea08-f87c-47a7-ba74-0dbe73024487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354061d4-2145-4d4a-bae0-61f52debc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BloomForQuestionAnswering.from_pretrained(\"/home/jupyter/tmp/debug_bloom_squad/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d9a1d0-e564-458d-851c-b647becd345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BloomTokenizerFast.from_pretrained(\"/home/jupyter/tmp/debug_bloom_squad/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0a1ac7-3b59-4086-899f-e24cd160342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 10560,    632,    368,   9213,    461,  17527, 129602]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "prompt = [\"What is the capital of North Dakota\"] \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\") \n",
    "#inIDs = torch.LongTensor(inputs[\"input_ids\"])\n",
    "#attnIDs = torch.LongTensor(inputs[\"attention_mask\"])\n",
    "#print(inIDs, attnIDs)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b507f11-71af-4e95-b4d7-2d81ebc1492a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of North Dakota'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "963b5fc6-2aa2-4bd7-b2c6-19fe6eab343b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><s><s><s><s><s><s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae6e1c0a-359b-4a5e-972b-d068749a5b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/models/bloom/modeling_bloom.py:659: FutureWarning: `position_ids` have no functionality in BLOOM and will be removed in v5.0.0. You can safely ignore passing `position_ids`.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[110.5513, 107.8777, 111.6836, 111.9656, 107.3340, 111.9346, 110.6411]],\n",
       "       grad_fn=<CloneBackward0>), end_logits=tensor([[51.6394, 47.1113, 48.6247, 51.8243, 46.2082, 51.1148, 51.0818]],\n",
       "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85631cf3-e3cb-4e37-982c-cfb92d5e5c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b4479c2-d144-42c4-9290-e185ff17034a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[110.5513, 107.8777, 111.6836, 111.9656, 107.3340, 111.9346, 110.6411]],\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['start_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51353b18-a15a-41ff-a42c-f22de6d341b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[51.6394, 47.1113, 48.6247, 51.8243, 46.2082, 51.1148, 51.0818]],\n",
       "       grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['end_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0337994-c2a5-4db6-8cc3-9fa687fc6330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Dakota'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([129602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7b679de-09b4-46c6-a23a-e67ae088c16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 7]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits.shape, outputs.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "893ce8ef-6c77-47ad-a244-1f8eb19ae1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3]), tensor([3]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.start_logits.argmax(dim=-1), outputs.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031c63e-442e-4389-b0bc-ddded2c2411e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41a7ea3-7f30-4797-890e-e8cc69b53fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62b8f47-5d2d-4dbe-906a-24023ce62e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e6a66-d442-440d-bb28-61b24b039c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba210e39-d1cf-41ad-b4c2-b17a66e5bf89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915473c0-3709-4bbb-bf97-60c51f8ca8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
